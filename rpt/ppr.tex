\documentclass[12pt]{article}

\usepackage{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=20mm, bottom=20mm}

\setlength{\parindent}{2em}
\setlength{\parskip}{1em}

\usepackage{hyperref}

\usepackage{syntonly}
%\syntaxonly
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[utf8]{inputenc}
\usepackage[backend=biber,style=numeric,sorting=none,]{biblatex}
\addbibresource{ref.bib}

\usepackage{graphicx}

% aliasis
% FreeSurfer from Havord Unv.
\newcommand{\FS}{\href{http://surfer.nmr.mgh.harvard.edu}{\textbf{FreeSurfer}}} 

% encoders
% vector or matrix
\newcommand{\vecEC}[1]{\boldsymbol{#1}}

% decoders
\newcommand{\vecDC}[1]{\boldsymbol{\tilde{#1}}}

\newcommand{\xVO}{\boldsymbol{x}}         % the x vector, original
\newcommand{\xVR}{\boldsymbol{\tilde{x}}} % the x vector, recovered
\newcommand{\xSO}{x}                      % the x scaler, original
\newcommand{\xSR}{\tilde{x}}              % the x scaler, recovered

% the eta vector
\newcommand{\etaEC}{\vecEC{\eta}}                % generic encoder
\newcommand{\etaEi}{\WEC_i^{d_{i+1} \times d_i}} % encoder layer i
\newcommand{\etaDC}{\vecDC{\eta}}                % generic decoder
\newcommand{\etaDi}{\WDC_i^{d_i \times d_{i+1}}} % decoder layer i

% the W matrix
\newcommand{\WEC}{\vecEC{W}}                   % generic encoder
\newcommand{\WEi}{\WEC_i^{d_{i+1} \times d_i}} % encoder layer i
\newcommand{\WEI}[3]{\WEC_{#1}^{d_{#2} \times d_{#3}}} % decoder layer #i
\newcommand{\WEIt}[3]{\WEC_{#1}^{d_{#2} \times d_{#3}\prime}} % decoder layer #i, transposed
\newcommand{\WDC}{\vecDC{W}}                   % generic decoder
\newcommand{\WDi}{\WDC_i^{d_i \times d_{i+1}}} % decoder layer #i
\newcommand{\WDI}[3]{\WDC_{#1}^{d_{#2} \times d_{#3}}} % decoder layer #i
\newcommand{\WDIt}[3]{\WDC_{#1}^{d_{#2} \times d_{#3}\prime}} % decoder layer #i

% the w vector
\newcommand{\wEC}{\vecEC{w}}    % generic encoder
\newcommand{\wEI}[2]{{\wEC_{#1}^{1 \times d_{#2}}}}
\newcommand{\wDC}{\vecDC{w}}    % generic decoder
\newcommand{\wDI}[2]{{\wDC_{#1}^{1 \times d_{#2}}}}
\newcommand{\wDIt}[2]{{\wDC_{#1}^{1 \times d_{#2}\prime}}}

% the b vector
\newcommand{\bEC}{\vecEC{b}}    % generic encoder
\newcommand{\bEi}{\bEC_i^{d_i}} % encoder layer i
\newcommand{\bEI}[2]{\bEC_{#1}^{d_{#2}}} % encoder layer i
\newcommand{\bDC}{\vecDC{b}}    % generic decoder
\newcommand{\bDi}{\bDC_i^{d_i}} % encoder layer i
\newcommand{\bDI}[2]{\bDC_{#1}^{d_{#2}}} % encoder layer i

% the x vector
\newcommand{\xEC}{\vecEC{x}}    % generic encoder
\newcommand{\xDC}{\vecDC{x}}    % generic decoder
% the X matrix
\newcommand{\XEC}{\vecEC{X}}    % generic encoder
\newcommand{\XDC}{\vecDC{X}}    % generic decoder

% the y_hat vector
\newcommand{\yHT}{\boldsymbol{\hat{y}}}
\newcommand{\YHT}{\boldsymbol{\hat{Y}}}

% the z vector
\newcommand{\zEC}{\vecEC{z}}    % generic encoder
\newcommand{\zDC}{\vecDC{z}}    % generic decoder

% I/O for decoder layer
\newcommand{\iDi}{\zDC_{i+1}^{d_{i+1}}}
\newcommand{\zEI}[2]{{\zEC_{#1}^{d_{#2}}}}
\newcommand{\zEIt}[2]{{\zEC_{#1}^{d_{#2}\prime}}}
\newcommand{\oDi}{\zDC_i^{d_i}}
\newcommand{\zDI}[2]{{\zDC_{#1}^{d_{#2}}}}
\newcommand{\zDIt}[2]{{\zDC_{#1}^{d_{#2}\prime}}}

% the vector of ones
\newcommand{\one}{\boldsymbol{1}}         % the z vector in encoders
% the diagnal matrix
\newcommand{\I}[1]{\boldsymbol{I}^{#1}}

% parameters in the neural network
\newcommand{\Par}{\boldsymbol{\Theta}} % the parameters
\newcommand{\pEC}{\boldsymbol{\theta}} % the parameters in the stacked autoencoder
\newcommand{\pDC}{\boldsymbol{\tilde{\theta}}} % the parameters in the decoder

% Loss function in Cross Entropy form
\newcommand{\LCE}[2]{#1\log{#2} + (1 - #1)\log{(1 - #2)}}

% derivative
\newcommand{\DRV}[2]{\frac{d #1}{d #2}}        % derivative
\newcommand{\DRC}[3]{\DRV{#1}{#2}\DRV{#2}{#3}} % chained derivative
\newcommand{\PDV}[2]{\frac{\partial #1}{\partial #2}} % paritial derivative
\newcommand{\PDC}[3]{\PDV{#1}{#2}\PDV{#2}{#3}}        % chained

% invers logit, aka. sigmoid function
\newcommand{\SGM}[1]{\frac{1}{1+e^{-#1}}}

% assign to diagnoral
\newcommand{\diag}[1]{\text{diag}(#1)}

\pagestyle{headings}

\author{Xiaoran Tong}

\begin{document}
\title{An Joint Association Analysis Method for Genomic Sequencing and Neuroimaging Data}
\maketitle

\begin{abstract}
The next generation genome sequencing and neuroimaging technology give rise to large, multiple-site cohort with growing wealth of next generation sequencing (NGS) data and magnetic resonance imaging (MRI) data. These growing databases, while offering new opportunities to detect genetic association with complex diseases, also highlights the ``curse of dimensionality'' and the issue of low statistical power, due to the large number of variants in both types of data, and particularly the low MAF of the NGS data. In this work, we tackle the dimensionality issue of the neuroimaging profile by first training a stacked autoencoder, the use to generating compact, abstracted representation of the raw image data. To solve the power issue, we adopt the grouping and aggregation technique, coupled with an U statistic composed of similarity based kernel functions to detect the joint association among the genomic, neuroimaging and phenotype profiles. The simulation study showed the method can withstand misspecified model and a wide variety of profile distributions. It also achieved a higher power by replacing raw image data with abstracted features, and by the grouping and aggregation of variants. Lastly, the method is applied to the real clinical diagnosis together with the genomic and neuroimaging data from the Alzheimer's disease Neuroimaging Initiative (ADNI), which demonstrated its capability to detect strong interaction effect between the two high dimensional profiles while the genomic profile itself only weakly associated with the phenotype.
\end{abstract}

\input{tex/sec_intro}
\input{tex/sec_method}
\input{tex/sec_result}
\input{tex/sec_dissc}

%\printbibheading
\printbibliography

\section{Appendix}
\input{tex/app_grad}
\input{tex/app_simu_bin}

\end{document}
