\documentclass[12pt]{article}

\usepackage{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=20mm, bottom=20mm}

\setlength{\parindent}{2em}
\setlength{\parskip}{1em}

\usepackage{hyperref}

\usepackage{syntonly}
%\syntaxonly
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[utf8]{inputenc}
\usepackage[backend=biber,style=numeric,sorting=none,]{biblatex}
\addbibresource{ref.bib}

\usepackage{graphicx}

% aliasis
% FreeSurfer from Havord Unv.
\newcommand{\FS}{\href{http://surfer.nmr.mgh.harvard.edu}{\textbf{FreeSurfer}}} 

% encoders
% vector or matrix
\newcommand{\vecEC}[1]{\boldsymbol{#1}}

% decoders
\newcommand{\vecDC}[1]{\boldsymbol{\tilde{#1}}}

\newcommand{\xVO}{\boldsymbol{x}}         % the x vector, original
\newcommand{\xVR}{\boldsymbol{\tilde{x}}} % the x vector, recovered
\newcommand{\xSO}{x}                      % the x scaler, original
\newcommand{\xSR}{\tilde{x}}              % the x scaler, recovered

% the eta vector
\newcommand{\etaEC}{\vecEC{\eta}}                % generic encoder
\newcommand{\etaEi}{\WEC_i^{d_{i+1} \times d_i}} % encoder layer i
\newcommand{\etaDC}{\vecDC{\eta}}                % generic decoder
\newcommand{\etaDi}{\WDC_i^{d_i \times d_{i+1}}} % decoder layer i

% the W matrix
\newcommand{\WEC}{\vecEC{W}}                   % generic encoder
\newcommand{\WEi}{\WEC_i^{d_{i+1} \times d_i}} % encoder layer i
\newcommand{\WEI}[3]{\WEC_{#1}^{d_{#2} \times d_{#3}}} % decoder layer #i
\newcommand{\WEIt}[3]{\WEC_{#1}^{d_{#2} \times d_{#3}\prime}} % decoder layer #i, transposed
\newcommand{\WDC}{\vecDC{W}}                   % generic decoder
\newcommand{\WDi}{\WDC_i^{d_i \times d_{i+1}}} % decoder layer #i
\newcommand{\WDI}[3]{\WDC_{#1}^{d_{#2} \times d_{#3}}} % decoder layer #i
\newcommand{\WDIt}[3]{\WDC_{#1}^{d_{#2} \times d_{#3}\prime}} % decoder layer #i

% the w vector
\newcommand{\wEC}{\vecEC{w}}    % generic encoder
\newcommand{\wEI}[2]{{\wEC_{#1}^{1 \times d_{#2}}}}
\newcommand{\wDC}{\vecDC{w}}    % generic decoder
\newcommand{\wDI}[2]{{\wDC_{#1}^{1 \times d_{#2}}}}
\newcommand{\wDIt}[2]{{\wDC_{#1}^{1 \times d_{#2}\prime}}}

% the b vector
\newcommand{\bEC}{\vecEC{b}}    % generic encoder
\newcommand{\bEi}{\bEC_i^{d_i}} % encoder layer i
\newcommand{\bEI}[2]{\bEC_{#1}^{d_{#2}}} % encoder layer i
\newcommand{\bDC}{\vecDC{b}}    % generic decoder
\newcommand{\bDi}{\bDC_i^{d_i}} % encoder layer i
\newcommand{\bDI}[2]{\bDC_{#1}^{d_{#2}}} % encoder layer i

% the x vector
\newcommand{\xEC}{\vecEC{x}}    % generic encoder
\newcommand{\xDC}{\vecDC{x}}    % generic decoder
% the X matrix
\newcommand{\XEC}{\vecEC{X}}    % generic encoder
\newcommand{\XDC}{\vecDC{X}}    % generic decoder

% the y_hat vector
\newcommand{\yHT}{\boldsymbol{\hat{y}}}
\newcommand{\YHT}{\boldsymbol{\hat{Y}}}

% the z vector
\newcommand{\zEC}{\vecEC{z}}    % generic encoder
\newcommand{\zDC}{\vecDC{z}}    % generic decoder

% I/O for decoder layer
\newcommand{\iDi}{\zDC_{i+1}^{d_{i+1}}}
\newcommand{\zEI}[2]{{\zEC_{#1}^{d_{#2}}}}
\newcommand{\zEIt}[2]{{\zEC_{#1}^{d_{#2}\prime}}}
\newcommand{\oDi}{\zDC_i^{d_i}}
\newcommand{\zDI}[2]{{\zDC_{#1}^{d_{#2}}}}
\newcommand{\zDIt}[2]{{\zDC_{#1}^{d_{#2}\prime}}}

% the vector of ones
\newcommand{\one}{\boldsymbol{1}}         % the z vector in encoders
% the diagnal matrix
\newcommand{\I}[1]{\boldsymbol{I}^{#1}}

% parameters in the neural network
\newcommand{\Par}{\boldsymbol{\Theta}} % the parameters
\newcommand{\pEC}{\boldsymbol{\theta}} % the parameters in the stacked autoencoder
\newcommand{\pDC}{\boldsymbol{\tilde{\theta}}} % the parameters in the decoder

% Loss function in Cross Entropy form
\newcommand{\LCE}[2]{#1\log{#2} + (1 - #1)\log{(1 - #2)}}

% derivative
\newcommand{\DRV}[2]{\frac{d #1}{d #2}}        % derivative
\newcommand{\DRC}[3]{\DRV{#1}{#2}\DRV{#2}{#3}} % chained derivative
\newcommand{\PDV}[2]{\frac{\partial #1}{\partial #2}} % paritial derivative
\newcommand{\PDC}[3]{\PDV{#1}{#2}\PDV{#2}{#3}}        % chained

% invers logit, aka. sigmoid function
\newcommand{\SGM}[1]{\frac{1}{1+e^{-#1}}}

% assign to diagnoral
\newcommand{\diag}[1]{\text{diag}(#1)}

\pagestyle{headings}

\author{Xiaoran Tong}

\begin{document}
\title{An Joint Association Analysis Method for Genomic Sequencing and Neuroimaging Data}
\maketitle

\begin{abstract}
The next generation genome sequencing and medical imaging technology give rise to large, mult-site cohort with growing wealth of next generation sequencing (NGS) and imaging profiles, which mandates the development of analytical methods capable of utilizing both type of information to identify predictive biomarkers of disorders. However, we are met with “the curse of dimensionality and multiple testing”, due to the large number of variants in these profiles. In this study, we try to tackled the dimensionality issue by sending the 3 dimensional cortical surfaces through stacked audoencoders constructed by unsupervised machine learning technique, which produce compact high order feature of the original surface, following which a similarity based U statistic were used to evaluate the joint association of the high order image features, the genome profile, and the phenotype of interest.
We conducted simulation studies and real data analysis using the deep genomic sequencing and neuroimage data provided by the Alzheimer’s Disease Neuroimaging Initiative (ADNI). The study demonstrated higher power with the joint analysis than either genome or image data alone, also demonstrated is the power improvement achieved by replace raw images with high order features abstracted from them. 
\end{abstract}

\input{tex/sec_intro}
\input{tex/sec_method}
\input{tex/sec_result}
\input{tex/sec_dissc}

%\printbibheading
\printbibliography

\section{Appendix}
\input{tex/app_grad}
\input{tex/app_simu_bin}

\end{document}
