\documentclass[12pt]{article}

\usepackage{geometry}
\geometry{a4paper, left=20mm, right=20mm, top=20mm, bottom=20mm}

\setlength{\parindent}{2em}
\setlength{\parskip}{1em}

\usepackage{hyperref}

\usepackage{syntonly}
%\syntaxonly
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[utf8]{inputenc}
\usepackage[backend=biber,style=numeric,sorting=none,]{biblatex}
\addbibresource{ref.bib}

\usepackage{graphicx}
\graphicspath{{img/}}

% aliasis
% FreeSurfer from Havord Unv.
\newcommand{\FS}{\href{http://surfer.nmr.mgh.harvard.edu}{\textbf{FreeSurfer}}} 

% encoders
% vector or matrix
\newcommand{\vecEC}[1]{\pmb{#1}}

% decoders
\newcommand{\vecDC}[1]{\pmb{\tilde{#1}}}

\newcommand{\xVO}{\pmb{x}}         % the x vector, original
\newcommand{\xVR}{\pmb{\tilde{x}}} % the x vector, recovered
\newcommand{\xSO}{x}               % the x scaler, original
\newcommand{\xSR}{\tilde{x}}       % the x scaler, recovered

% the eta vector
\newcommand{\etaEC}{\vecEC{\eta}}                % generic encoder
\newcommand{\etaEi}{\WEC_i^{d_{i+1} \times d_i}} % encoder layer i
\newcommand{\etaDC}{\vecDC{\eta}}                % generic decoder
\newcommand{\etaDi}{\WDC_i^{d_i \times d_{i+1}}} % decoder layer #i

% the W matrix
\newcommand{\WEC}{\vecEC{W}}                   % generic encoder
\newcommand{\WEi}{\WEC_i^{d_{i+1} \times d_i}} % encoder layer i
\newcommand{\WEI}[3]{\WEC_{#1}^{d_{#2} \times d_{#3}}} % decoder layer #i
\newcommand{\WEIt}[3]{\WEC_{#1}^{d_{#2} \times d_{#3}\prime}} % decoder layer #i, transposed
\newcommand{\WDC}{\vecDC{W}}                   % generic decoder
\newcommand{\WDi}{\WDC_i^{d_i \times d_{i+1}}} % decoder layer #i
\newcommand{\WDI}[3]{\WDC_{#1}^{d_{#2} \times d_{#3}}} % decoder layer #i
\newcommand{\WDIt}[3]{\WDC_{#1}^{d_{#2} \times d_{#3}\prime}} % decoder layer #i

% the w vector
\newcommand{\wEC}{\vecEC{w}}    % generic encoder
\newcommand{\wEI}[2]{{\wEC_{#1}^{1 \times d_{#2}}}}
\newcommand{\wDC}{\vecDC{w}}    % generic decoder
\newcommand{\wDI}[2]{{\wDC_{#1}^{1 \times d_{#2}}}}
\newcommand{\wDIt}[2]{{\wDC_{#1}^{1 \times d_{#2}\prime}}}

% the b vector
\newcommand{\bEC}{\vecEC{b}}    % generic encoder
\newcommand{\bEi}{\bEC_i^{d_i}} % encoder layer i
\newcommand{\bEI}[2]{\bEC_{#1}^{d_{#2}}} % encoder layer i
\newcommand{\bDC}{\vecDC{b}}    % generic decoder
\newcommand{\bDi}{\bDC_i^{d_i}} % encoder layer i
\newcommand{\bDI}[2]{\bDC_{#1}^{d_{#2}}} % encoder layer i

% the x vector
\newcommand{\xEC}{\vecEC{x}}    % generic encoder
\newcommand{\xDC}{\vecDC{x}}    % generic decoder
% the X matrix
\newcommand{\XEC}{\vecEC{X}}    % generic encoder
\newcommand{\XDC}{\vecDC{X}}    % generic decoder

% the y_hat vector
\newcommand{\yHT}{\pmb{\hat{y}}}
\newcommand{\YHT}{\pmb{\hat{Y}}}

% the z vector
\newcommand{\zEC}{\vecEC{z}}    % generic encoder
\newcommand{\zDC}{\vecDC{z}}    % generic decoder

% I/O for decoder layer
\newcommand{\iDi}{\zDC_{i+1}^{d_{i+1}}}
\newcommand{\zEI}[2]{{\zEC_{#1}^{d_{#2}}}}
\newcommand{\zEIt}[2]{{\zEC_{#1}^{d_{#2}\prime}}}
\newcommand{\oDi}{\zDC_i^{d_i}}
\newcommand{\zDI}[2]{{\zDC_{#1}^{d_{#2}}}}
\newcommand{\zDIt}[2]{{\zDC_{#1}^{d_{#2}\prime}}}

% the vector of ones
\newcommand{\one}{\pmb{1}}         % the z vector in encoders
% the diagnal matrix
\newcommand{\I}[1]{\pmb{I}^{#1}}

% parameters in the neural network
\newcommand{\Par}{\pmb{\Theta}} % the parameters
\newcommand{\pEC}{\pmb{\theta}} % the parameters in the stacked autoencoder
\newcommand{\pDC}{\pmb{\tilde{\theta}}} % the parameters in the decoder

% Loss function in Cross Entropy form
\newcommand{\LCE}[2]{#1\log{#2} + (1 - #1)\log{(1 - #2)}}

% derivative
\newcommand{\DRV}[2]{\frac{d #1}{d #2}}        % derivative
\newcommand{\DRC}[3]{\DRV{#1}{#2}\DRV{#2}{#3}} % chained derivative
\newcommand{\PDV}[2]{\frac{\partial #1}{\partial #2}} % paritial derivative
\newcommand{\PDC}[3]{\PDV{#1}{#2}\PDV{#2}{#3}}        % chained

% invers logit, aka. sigmoid function
\newcommand{\SGM}[1]{\frac{1}{1+e^{-#1}}}

% assign to diagnoral
\newcommand{\diag}[1]{\text{diag}(#1)}

\pagestyle{headings}

\author{Xiaoran Tong}

\begin{document}
\title{A non-parametric method for joint association analysis of
  sequencing and imaging data}
\maketitle

\begin{abstract}
  The next generation genome sequencing and neroimaging technology
  give rise to large, mult-site cohort with growing wealth of next
  generation sequencing (NGS) data and magnetic resonance imaging
  (MRI) data, which mandates the corresponding analytical methodology
  capable of utilizing both type of information to identify predictive
  biomarkers associated with complex disorders. Such attemp, however,
  are met with "the curse of dimensionality", due to the large number
  of variants in the genome and images. In this work, we tackled the
  dimensionality issue of 3D cortical surface vertices by training a
  stacked audoencoder with deep learning algorithm, and generating a
  compact, abstracted representation of the original surface with the
  encoder per se. An U statistic with profile similarity based weight
  term were then adopted to evaluate the joint association of encoded
  surface and genome data with the phenotype. We showed by simulation
  that the method maintains the correct type 1 error rate, and
  achieved a statistical power higher then using either genome or
  image data alone, or using the original surface, or methods relying
  on large number of per-variant test. To illustrate our approach, we
  apply the proposed method to the genomic sequencing and neuroimage
  data from the Alzheimer's disease Neuroimaging Initiative (ADNI).
\end{abstract}

\input{tex/sec_intro}
\input{tex/sec_mater}
\input{tex/sec_method}
\input{tex/sec_result}
\input{tex/sec_dissc}

%\printbibheading
\printbibliography

\section{Appendix}
\input{tex/app_grad}

\end{document}
