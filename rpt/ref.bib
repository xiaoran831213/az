@article{GWA1,
author = {Manolio, Teri A.},
title = {Genomewide Association Studies and Assessment of the Risk of Disease},
journal = {New England Journal of Medicine},
volume = {363},
number = {2},
pages = {166-176},
year = {2010},
doi = {10.1056/NEJMra0905980},
note = {PMID: 20647212},
URL = {http://dx.doi.org/10.1056/NEJMra0905980},
eprint = {http://dx.doi.org/10.1056/NEJMra0905980}
}

@article{GWA2,
author = {Janardan P. Pandey},
title = {Comment on Genomewide Association Studies and Assessment of Risk of Disease},
journal = {New England Journal of Medicine},
volume = 363,
number = 21,
pages = {2076-2077},
year = 2010,
doi = {10.1056/NEJMc1010310},
note = {PMID: 21083406},
URL = {http://dx.doi.org/10.1056/NEJMc1010310},
eprint = {http://dx.doi.org/10.1056/NEJMc1010310}
}

@article {MR1,
author = {Lawlor, Debbie A. and Harbord, Roger M. and Sterne, Jonathan A. C. and Timpson, Nic and Davey Smith, George},
title = {Mendelian randomization: Using genes as instruments for making causal inferences in epidemiology},
journal = {Statistics in Medicine},
volume = {27},
number = {8},
publisher = {John Wiley & Sons, Ltd.},
issn = {1097-0258},
url = {http://dx.doi.org/10.1002/sim.3034},
doi = {10.1002/sim.3034},
pages = {1133--1163},
keywords = {Mendelian randomization, instrumental variables, genetics, causal models, econometrics, epidemiology, confounding},
year = {2008},
}

@article{MR2,
  title={‘Mendelian randomization’: can genetic epidemiology contribute to understanding environmental determinants of disease?},
  author={Smith, George Davey and Ebrahim, Shah},
  journal={International journal of epidemiology},
  volume={32},
  number={1},
  pages={1--22},
  year={2003},
  publisher={IEA}
}

@article{RVCD1,
  title={Uncovering the roles of rare variants in common disease through whole-genome sequencing},
  author={Cirulli, Elizabeth T and Goldstein, David B},
  journal={Nature Reviews Genetics},
  volume={11},
  number={6},
  pages={415--425},
  year={2010},
  publisher={Nature Publishing Group}
}

@article{NGS1,
  title={The impact of next-generation sequencing technology on genetics},
  author={Mardis, Elaine R},
  journal={Trends in genetics},
  volume={24},
  number={3},
  pages={133--141},
  year={2008},
  publisher={Elsevier}
}

@article{plink1,
  title={PLINK: a tool set for whole-genome association and population-based linkage analyses},
  author={Purcell, Shaun and Neale, Benjamin and Todd-Brown, Kathe and Thomas, Lori and Ferreira, Manuel AR and Bender, David and Maller, Julian and Sklar, Pamela and De Bakker, Paul IW and Daly, Mark J and others},
  journal={The American Journal of Human Genetics},
  volume={81},
  number={3},
  pages={559--575},
  year={2007},
  publisher={Elsevier}
}

@article{Burden1,
    author = {Madsen, Bo Eskerod AND Browning, Sharon R.},
    journal = {PLoS Genet},
    publisher = {Public Library of Science},
    title = {A Groupwise Association Test for Rare Mutations Using a Weighted Sum Statistic},
    year = {2009},
    month = {02},
    volume = {5},
    url = {http://dx.doi.org/10.1371%2Fjournal.pgen.1000384},
    pages = {e1000384},
    abstract = {<title>Author Summary</title><p>Resequencing is an emerging tool for the identification of rare disease-associated mutations. Recent studies have shown that groups of multiple rare mutations together can explain a large proportion of the genetic basis for some diseases. Therefore, we propose a new statistical method for analysing a group of mutations in order to test for groupwise association with disease status. We compare the proposed weighted-sum method to alternative methods and show that it is powerful for identifying disease-associated groups of mutations, both on computer-simulated and real data. By using computer simulations, we further show that resequencing a few thousand individuals is sufficient to perform a genome-wide study of all human genes, if the proposed method is used. This study thus demonstrates that resequencing studies can identify important genetic associations, provided that specialised analysis methods, such as the proposed weighted-sum method, are used.</p>
},
    number = {2},
    doi = {10.1371/journal.pgen.1000384}
}

@article{HWU,
  title={A weighted U statistic for association analysis considering genetic heterogeneity},
  author={Wei, Changshuai and Elston, Robert C and Lu, Qing},
  journal={arXiv preprint arXiv:1504.08319},
  year={2015}
}

@article{SKT,
title = "Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test ",
journal = "The American Journal of Human Genetics ",
volume = "89",
number = "1",
pages = "82 - 93",
year = "2011",
issn = "0002-9297",
doi = "http://dx.doi.org/10.1016/j.ajhg.2011.05.029",
url = "http://www.sciencedirect.com/science/article/pii/S0002929711002229",
author = "Michael C. Wu and Seunggeun Lee and Tianxi Cai and Yun Li and Michael Boehnke and Xihong Lin",
abstract = "Sequencing studies are increasingly being conducted to identify rare variants associated with complex traits. The limited power of classical single-marker association analysis for rare variants poses a central challenge in such studies. We propose the sequence kernel association test (SKAT), a supervised, flexible, computationally efficient regression method to test for association between genetic variants (common and rare) in a region and a continuous or dichotomous trait while easily adjusting for covariates. As a score-based variance-component test, \{SKAT\} can quickly calculate p values analytically by fitting the null model containing only the covariates, and so can easily be applied to genome-wide data. Using \{SKAT\} to analyze a genome-wide sequencing study of 1000 individuals, by segmenting the whole genome into 30 kb regions, requires only 7 hr on a laptop. Through analysis of simulated data across a wide range of practical scenarios and triglyceride data from the Dallas Heart Study, we show that \{SKAT\} can substantially outperform several alternative rare-variant association tests. We also provide analytic power and sample-size calculations to help design candidate-gene, whole-exome, and whole-genome sequence association studies. "
}

@article{META1,
  title={Multivariate meta-analysis: a robust approach based on the theory of U-statistic},
  author={Ma, Yan and Mazumdar, Madhu},
  journal={Statistics in medicine},
  volume={30},
  number={24},
  pages={2911--2929},
  year={2011},
  publisher={Wiley Online Library}
}

@ARTICLE{Dai:2015,
AUTHOR={Dai, Hongying (Daisy)  and  Leeder, J Steven  and  Cui, Yuehua},
TITLE={A Modified Generalized Fisher Method for Combining Probabilities from Dependent Tests},
JOURNAL={Frontiers in Genetics},
VOLUME={5},
YEAR={2014},
NUMBER={32},
URL={http://www.frontiersin.org/evolutionary_and_population_genetics/10.3389/fgene.2014.00032/abstract},
DOI={10.3389/fgene.2014.00032},
ISSN={1664-8021},
ABSTRACT={Rapid developments in molecular technology have yielded a large amount of high throughput genetic data to understand the mechanism for complex traits. The increase of genetic variants requires hundreds and thousands of statistical tests to be performed simultaneously in analysis, which poses a challenge to control the overall Type I error rate. Combining p-values from multiple hypothesis testing has shown promise for aggregating effects in high-dimensional genetic data analysis. Several p-value combining methods have been developed and applied to genetic data; see Dai et al. (2012b) for a comprehensive review. However, there is a lack of investigations conducted for dependent genetic data, especially for weighted p-value combining methods. Single nucleotide polymorphisms (SNPs) are often correlated due to linkage disequilibrium (LD). Other genetic data, including variants from next generation sequencing, gene expression levels measured by microarray, protein and DNA methylation data, etc. also contain complex correlation structures. Ignoring correlation structures among genetic variants may lead to severe inflation of Type I error rates for omnibus testing of p-values. In this work, we propose modifications to the Lancaster procedure by taking the correlation structure among p-values into account. The weight function in the Lancaster procedure allows meaningful biological information to be incorporated into the statistical analysis, which can increase the power of the statistical testing and/or remove the bias in the process. Extensive empirical assessments demonstrate that the modified Lancaster procedure largely reduces the Type I error rates due to correlation among p-values, and retains considerable power to detect signals among p-values. We applied our method to reassess published renal transplant data, and identified a novel association between B cell pathways and allograft tolerance.}
}

@article{eQTL1,
title = "Revealing the architecture of gene regulation: the promise of eQTL studies ",
journal = "Trends in Genetics ",
volume = "24",
number = "8",
pages = "408 - 415",
year = "2008",
issn = "0168-9525",
doi = "http://dx.doi.org/10.1016/j.tig.2008.06.001",
url = "http://www.sciencedirect.com/science/article/pii/S0168952508001777",
author = "Yoav Gilad and Scott A. Rifkin and Jonathan K. Pritchard",
abstract = "Expression quantitative trait loci (eQTL) mapping studies have become a widely used tool for identifying genetic variants that affect gene regulation. In these studies, expression levels are viewed as quantitative traits, and gene expression phenotypes are mapped to particular genomic loci by combining studies of variation in gene expression patterns with genome-wide genotyping. Results from recent eQTL mapping studies have revealed substantial heritable variation in gene expression within and between populations. In many cases, genetic factors that influence gene expression levels can be mapped to proximal (putatively cis) eQTLs and, less often, to distal (putatively trans) eQTLs. Beyond providing great insight into the biology of gene regulation, a combination of eQTL studies with results from traditional linkage or association studies of human disease may help predict a specific regulatory role for polymorphic sites previously associated with disease. "

}
@article{eQTL2,
  title={Integrative eQTL-based analyses reveal the biology of breast cancer risk loci},
  author={Li, Qiyuan and Seo, Ji-Heui and Stranger, Barbara and McKenna, Aaron and Pe’er, Itsik and LaFramboise, Thomas and Brown, Myles and Tyekucheva, Svitlana and Freedman, Matthew L},
  journal={Cell},
  volume={152},
  number={3},
  pages={633--641},
  year={2013},
  publisher={Elsevier}
}

@article{cytokine1,
author = {Ashraf, Saeed and Bhattacharya, Kausik and Tian, Yi and Watterson, Kevin}, 
title = {Cytokine and S100B levels in paediatric patients undergoing corrective cardiac surgery with or without total circulatory arrest},
volume = {16}, 
number = {1}, 
pages = {32-37}, 
year = {1999}, 
doi = {10.1016/S1010-7940(99)00136-0}, 
abstract ={Objectives: Neurological damage following cardiopulmonary bypass (CPB) is difficult to objectively evaluate in infants. In adults, serum elevations of astroglial S100B correlate with proven brain injury independent of operative temperature. The deleterious effects of inflammatory cytokines, generated during CPB, on the brain have not been studied in infants using S100B as a marker for cerebral injury. Methods: Twelve neonates, weighing 3.3±0.2 kg (total circulatory arrest group (TCA)) and 12 infants weighing 7.0±1.0 kg (cardiopulmonary bypass group (CPB)) underwent corrective cardiac surgery for various pathologies. Serial blood samples on induction, at the end of CPB, 30 min, 2 h and 24 h after the administration of protamine, were taken. The resultant plasma was frozen to −80°C and stored for batch analysis. Cytokines were measured using ELISAs and S100B using a luminometric assay. Results: The TCA group were younger and experienced a longer perfusion time than the CPB group (137±8 vs. 113±7, P=0.04). The mean TCA time was 23±4 min. The TCA group had significantly higher levels of IL-6 (P=0.001), IL-8 (P=0.005)and S100B (P=0.002) at 24 h. C5b-9 levels were significantly lower in the TCA group: end of CPB (P=0.001), 30 min (P<0.001), 2 h (P=0.002). There was a weak, but significant correlation between IL-6 levels at the end of CPB and S100B levels 2 h later (r=0.55, P=0.03). Long extubation times were associated with high 24-h S100B levels (r=0.52, P=0.01). Conclusions: (1) The TCA group have prolonged rises of IL-6, IL-8 and S100B. (2) The TCA group generates significantly lower complement. (3) Astroglial injury, seen after surgery, may, in part, be cytokine mediated.}, 
URL = {http://ejcts.oxfordjournals.org/content/16/1/32.abstract}, 
eprint = {http://ejcts.oxfordjournals.org/content/16/1/32.full.pdf+html}, 
journal = {European Journal of Cardio-Thoracic Surgery} 
}

@article{CRP1,
author = {Ridker, Paul M. and Hennekens, Charles H. and Buring, Julie E. and Rifai, Nader},
title = {C-Reactive Protein and Other Markers of Inflammation in the Prediction of Cardiovascular Disease in Women},
journal = {New England Journal of Medicine},
volume = {342},
number = {12},
pages = {836-843},
year = {2000},
doi = {10.1056/NEJM200003233421202},
note ={PMID: 10733371},
URL = {http://dx.doi.org/10.1056/NEJM200003233421202},
eprint = {http://dx.doi.org/10.1056/NEJM200003233421202}
}

@article{VWA1,
title = "Voxel-Based Morphometry—The Methods ",
journal = "NeuroImage ",
volume = "11",
number = "6",
pages = "805 - 821",
year = "2000",
issn = "1053-8119",
doi = "http://dx.doi.org/10.1006/nimg.2000.0582",
url = "http://www.sciencedirect.com/science/article/pii/S1053811900905822",
author = "John Ashburner and Karl J. Friston",
abstract = "At its simplest, voxel-based morphometry (VBM) involves a voxel-wise comparison of the local concentration of gray matter between two groups of subjects. The procedure is relatively straightforward and involves spatially normalizing high-resolution images from all the subjects in the study into the same stereotactic space. This is followed by segmenting the gray matter from the spatially normalized images and smoothing the gray-matter segments. Voxel-wise parametric statistical tests which compare the smoothed gray-matter images from the two groups are performed. Corrections for multiple comparisons are made using the theory of Gaussian random fields. This paper describes the steps involved in VBM, with particular emphasis on segmenting gray matter from \{MR\} images with nonuniformity artifact. We provide evaluations of the assumptions that underpin the method, including the accuracy of the segmentation and the assumptions made about the statistical distribution of the data. "
}

@article{VWA2,
  title={Tract-based spatial statistics: voxelwise analysis of multi-subject diffusion data},
  author={Smith, Stephen M and Jenkinson, Mark and Johansen-Berg, Heidi and Rueckert, Daniel and Nichols, Thomas E and Mackay, Clare E and Watkins, Kate E and Ciccarelli, Olga and Cader, M Zaheer and Matthews, Paul M and others},
  journal={Neuroimage},
  volume={31},
  number={4},
  pages={1487--1505},
  year={2006},
  publisher={Elsevier}
}

@article{VWA3,
title = "In Vivo Mapping of Gray Matter Loss with Voxel-Based Morphometry in Mild Alzheimer's Disease ",
journal = "NeuroImage ",
volume = "14",
number = "2",
pages = "298 - 309",
year = "2001",
issn = "1053-8119",
doi = "http://dx.doi.org/10.1006/nimg.2001.0848",
url = "http://www.sciencedirect.com/science/article/pii/S1053811901908481",
author = "J.C. Baron and G. Chételat and B. Desgranges and G. Perchey and B. Landeau and V. de la Sayette and F. Eustache",
abstract = "Uptill now, the study of regional gray matter atrophy in Alzheimer's disease (AD) has been assessed with regions of interest, but this method is time-consuming, observer dependent, and poorly reproducible (especially in terms of cortical regions boundaries) and in addition is not suited to provide a comprehensive assessment of the brain. In this study, we have mapped gray matter density by means of voxel-based morphometry on T1-weighted \{MRI\} volume sets in 19 patients with mild \{AD\} and 16 healthy subjects of similar age and gender ratio and report highly significant clusters of gray matter loss with almost symmetrical distribution, affecting mainly and in decreasing order of significance the medial temporal structures, the posterior cingulate gyrus and adjacent precuneus, and the temporoparietal association and perisylvian neocortex, with only little atrophy in the frontal lobe. The findings are discussed in light of previous studies of gray matter atrophy in \{AD\} based either on postmortem or neuroimaging data and in relation to \{PET\} studies of resting glucose consumption. The limitations of the method are also discussed in some detail, especially with respect to the segmentation and spatial normalization procedures as they apply to pathological brains. Some potential applications of voxel-based morphometry in the study of \{AD\} are also mentioned. "
}

@article{VWA4,
title = "Using voxel-based morphometry to map the structural changes associated with rapid conversion in MCI: A longitudinal \{MRI\} study ",
journal = "NeuroImage ",
volume = 27,
number = 4,
pages = "934 - 946",
year = 2005,
issn = "1053-8119",
doi = "http://dx.doi.org/10.1016/j.neuroimage.2005.05.015",
url = "http://www.sciencedirect.com/science/article/pii/S1053811905003277",
author = "G. Chételat and B. Landeau and F. Eustache and F. Mézenge and F. Viader and V. de la Sayette and B. Desgranges and J.-C. Baron",
keywords = "Alzheimer's disease",
keywords = "Mild cognitive impairment",
keywords = "Longitudinal MRI",
keywords = "Brain mapping",
keywords = "Gray matter loss ",
abstract = "Capturing the dynamics of gray matter (GM) atrophy in relation to the conversion from mild cognitive impairment (MCI) to clinically probable Alzheimer's disease (AD) would be of considerable interest. In this prospective study we have used a novel longitudinal voxel-based method to map the progression of \{GM\} loss in \{MCI\} patients over time and compared converters to non-converters. Eighteen amnestic \{MCI\} patients were followed-up for a predefined fixed period of 18 months and conversion was judged according to NINCDS-ADRDA criteria for probable AD. Each patient underwent a high-resolution T1-weighted volume \{MRI\} scan both at entry in the study and 18 months later. We used an optimal \{VBM\} protocol to compare baseline imaging data of converters to those of non-converters. Moreover, to map \{GM\} loss from baseline to follow-up assessment, we used a modified voxel-based morphometry (VBM) procedure specially designed for longitudinal studies. At the end of the follow-up period, seven patients had converted to probable AD. Areas of lower baseline \{GM\} value in converters mainly included the hippocampus, parahippocampal cortex, and lingual and fusiform gyri. Regions of significant \{GM\} loss over the 18-month follow-up period common to both converters and non-converters included the temporal neocortex, parahippocampal cortex, orbitofrontal and inferior parietal areas, and the left thalamus. However, there was significantly greater \{GM\} loss in converters relative to non-converters in the hippocampal area, inferior and middle temporal gyrus, posterior cingulate, and precuneus. This accelerated atrophy may result from both neurofibrillary tangles accumulation and parallel pathological processes such as functional alteration in the posterior cingulate. The ability to longitudinally assess \{GM\} changes in \{MCI\} offers new perspectives to better understand the pathological processes underlying \{AD\} and to monitor the effects of treatment on brain structure. "
}

@article{FS:Intro,
title = "FreeSurfer ",
journal = "NeuroImage ",
volume = "62",
number = "2",
pages = "774 - 781",
year = "2012",
note = "20 \{YEARS\} \{OF\} fMRI20 \{YEARS\} \{OF\} fMRI ",
issn = "1053-8119",
doi = "http://dx.doi.org/10.1016/j.neuroimage.2012.01.021",
url = "http://www.sciencedirect.com/science/article/pii/S1053811912000389",
author = "Bruce Fischl",
keywords = "Morphometry",
keywords = "Registration",
keywords = "Segmentation",
keywords = "MRI ",
abstract = "FreeSurfer is a suite of tools for the analysis of neuroimaging data that provides an array of algorithms to quantify the functional, connectional and structural properties of the human brain. It has evolved from a package primarily aimed at generating surface representations of the cerebral cortex into one that automatically creates models of most macroscopically visible structures in the human brain given any reasonable T1-weighted input image. It is freely available, runs on a wide variety of hardware and software platforms, and is open source. "
}

@article{FS:Tck1,
author = {Fischl, Bruce and Dale, Anders M.}, 
title = {Measuring the thickness of the human cerebral cortex from magnetic resonance images},
volume = {97}, 
number = {20}, 
pages = {11050-11055}, 
year = {2000}, 
doi = {10.1073/pnas.200033797}, 
abstract = {Accurate and automated methods for measuring the thickness of human cerebral cortex could provide powerful tools for diagnosing and studying a variety of neurodegenerative and psychiatric disorders. Manual methods for estimating cortical thickness from neuroimaging data are labor intensive, requiring several days of effort by a trained anatomist. Furthermore, the highly folded nature of the cortex is problematic for manual techniques, frequently resulting in measurement errors in regions in which the cortical surface is not perpendicular to any of the cardinal axes. As a consequence, it has been impractical to obtain accurate thickness estimates for the entire cortex in individual subjects, or group statistics for patient or control populations. Here, we present an automated method for accurately measuring the thickness of the cerebral cortex across the entire brain and for generating cross-subject statistics in a coordinate system based on cortical anatomy. The intersubject standard deviation of the thickness measures is shown to be less than 0.5 mm, implying the ability to detect focal atrophy in small populations or even individual subjects. The reliability and accuracy of this new method are assessed by within-subject test–retest studies, as well as by comparison of cross-subject regional thickness measures with published values.}, 
URL = {http://www.pnas.org/content/97/20/11050.abstract}, 
eprint = {http://www.pnas.org/content/97/20/11050.full.pdf}, 
journal = {Proceedings of the National Academy of Sciences} 
}

@article{FS:Tck2,
title = "Reliability of MRI-derived measurements of human cerebral cortical thickness: The effects of field strength, scanner upgrade and manufacturer ",
journal = "NeuroImage ",
volume = "32",
number = "1",
pages = "180 - 194",
year = "2006",
note = "",
issn = "1053-8119",
doi = "http://dx.doi.org/10.1016/j.neuroimage.2006.02.051",
url = "http://www.sciencedirect.com/science/article/pii/S1053811906001601",
author = "Xiao Han and Jorge Jovicich and David Salat and Andre van der Kouwe and Brian Quinn and Silvester Czanner and Evelina Busa and Jenni Pacheco and Marilyn Albert and Ronald Killiany and Paul Maguire and Diana Rosas and Nikos Makris and Anders Dale and Bradford Dickerson and Bruce Fischl",
keywords = "Cortical thickness",
keywords = "Structural MRI",
keywords = "Cerebral cortex",
keywords = "Morphology ",
abstract = "In vivo MRI-derived measurements of human cerebral cortex thickness are providing novel insights into normal and abnormal neuroanatomy, but little is known about their reliability. We investigated how the reliability of cortical thickness measurements is affected by \{MRI\} instrument-related factors, including scanner field strength, manufacturer, upgrade and pulse sequence. Several data processing factors were also studied. Two test–retest data sets were analyzed: 1) 15 healthy older subjects scanned four times at 2-week intervals on three scanners; 2) 5 subjects scanned before and after a major scanner upgrade. Within-scanner variability of global cortical thickness measurements was &lt;0.03 mm, and the point-wise standard deviation of measurement error was approximately 0.12 mm. Variability was 0.15 mm and 0.17 mm in average, respectively, for cross-scanner (Siemens/GE) and cross-field strength (1.5 T/3 T) comparisons. Scanner upgrade did not increase variability nor introduce bias. Measurements across field strength, however, were slightly biased (thicker at 3 T). The number of (single vs. multiple averaged) acquisitions had a negligible effect on reliability, but the use of a different pulse sequence had a larger impact, as did different parameters employed in data processing. Sample size estimates indicate that regional cortical thickness difference of 0.2 mm between two different groups could be identified with as few as 7 subjects per group, and a difference of 0.1 mm could be detected with 26 subjects per group. These results demonstrate that MRI-derived cortical thickness measures are highly reliable when \{MRI\} instrument and data processing factors are controlled but that it is important to consider these factors in the design of multi-site or longitudinal studies, such as clinical drug trials. "
}

@article{FS:Anl1,
title = "Statistical analysis of longitudinal neuroimage data with Linear Mixed Effects models ",
journal = "NeuroImage ",
volume = "66",
number = "",
pages = "249 - 260",
year = "2013",
note = "",
issn = "1053-8119",
doi = "http://dx.doi.org/10.1016/j.neuroimage.2012.10.065",
url = "http://www.sciencedirect.com/science/article/pii/S1053811912010683",
author = "Jorge L. Bernal-Rusiel and Douglas N. Greve and Martin Reuter and Bruce Fischl and Mert R. Sabuncu",
keywords = "Longitudinal studies",
keywords = "Linear Mixed Effects models",
keywords = "Statistical analysis ",
abstract = "Longitudinal neuroimaging (LNI) studies are rapidly becoming more prevalent and growing in size. Today, no standardized computational tools exist for the analysis of \{LNI\} data and widely used methods are sub-optimal for the types of data encountered in real-life studies. Linear Mixed Effects (LME) modeling, a mature approach well known in the statistics community, offers a powerful and versatile framework for analyzing real-life \{LNI\} data. This article presents the theory behind \{LME\} models, contrasts it with other popular approaches in the context of LNI, and is accompanied with an array of computational tools that will be made freely available through FreeSurfer — a popular Magnetic Resonance Image (MRI) analysis software package. Our core contribution is to provide a quantitative empirical evaluation of the performance of \{LME\} and competing alternatives popularly used in prior longitudinal structural \{MRI\} studies, namely repeated measures \{ANOVA\} and the analysis of annualized longitudinal change measures (e.g. atrophy rate). In our experiments, we analyzed MRI-derived longitudinal hippocampal volume and entorhinal cortex thickness measurements from a public dataset consisting of Alzheimer's patients, subjects with mild cognitive impairment and healthy controls. Our results suggest that the \{LME\} approach offers superior statistical power in detecting longitudinal group differences. "
}

@article{FS:Anl2,
author = {Salat, David H. and Buckner, Randy L. and Snyder, Abraham Z. and Greve, Douglas N. and Desikan, Rahul S.R. and Busa, Evelina and Morris, John C. and Dale, Anders M. and Fischl, Bruce}, 
title = {Thinning of the Cerebral Cortex in Aging},
volume = {14}, 
number = {7}, 
pages = {721-730}, 
year = {2004}, 
doi = {10.1093/cercor/bhh032}, 
abstract ={The thickness of the cerebral cortex was measured in 106 non-demented participants ranging in age from 18 to 93 years. For each participant, multiple acquisitions of structural T1-weighted magnetic resonance imaging (MRI) scans were averaged to yield high-resolution, high-contrast data sets. Cortical thickness was estimated as the distance between the gray/white boundary and the outer cortical surface, resulting in a continuous estimate across the cortical mantle. Global thinning was apparent by middle age. Men and women showed a similar degree of global thinning, and did not differ in mean thickness in the younger or older groups. Age-associated differences were widespread but demonstrated a patchwork of regional atrophy and sparing. Examination of subsets of the data from independent samples produced highly similar age-associated patterns of atrophy, suggesting that the specific anatomic patterns within the maps were reliable. Certain results, including prominent atrophy of prefrontal cortex and relative sparing of temporal and parahippocampal cortex, converged with previous findings. Other results were unexpected, such as the finding of prominent atrophy in frontal cortex near primary motor cortex and calcarine cortex near primary visual cortex. These findings demonstrate that cortical thinning occurs by middle age and spans widespread cortical regions that include primary as well as association cortex.}, 
URL = {http://cercor.oxfordjournals.org/content/14/7/721.abstract}, 
eprint = {http://cercor.oxfordjournals.org/content/14/7/721.full.pdf+html}, 
journal = {Cerebral Cortex} 
}

@article {DL:Greedy,
author = {Hinton, G. E. and Salakhutdinov, R. R.},
title = {Reducing the Dimensionality of Data with Neural Networks},
volume = {313},
number = {5786},
pages = {504--507},
year = {2006},
doi = {10.1126/science.1127647},
publisher = {American Association for the Advancement of Science},
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
issn = {0036-8075},
URL = {http://science.sciencemag.org/content/313/5786/504},
eprint = {http://science.sciencemag.org/content/313/5786/504.full.pdf},
journal = {Science}
}
