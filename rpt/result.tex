\documentclass[twocolumn]{article}

\usepackage{syntonly}
%\syntaxonly
\usepackage{amsmath}

\pagestyle{headings}

\author{Xiaoran Tong}

\begin{document}

\section{Simulation}
Simulations are based on 806 participants of ADNI study with both WGS and MRI data avaliable. Each iteration run choose a pair of testing units for genomc and image profiles. The genomic testing unit is a randomly gene randomly picked from the genomic features list supplied by GRC38. For neuroimage profile, the testing unit is regions of 512 vertices randomly located in the white matter (WM) surface reconstructed from MRI data. The genomic effect $\beta_G$ and vertex effect $\beta_G$ are then generated by randomly selecting 5\% of the variants (e.g. SNPs in a gene, vertices in a WM region) in both testing units and assigning a value drawn from $N(0,1)$. Given the row major genomic profile $X_G$ and image profile $X_V$ of all subjects, two basic continuous response $Y_G$ and $Y_V$ were calculated by adding up product of variants and their corresponding effect across the testing unit; two additional continuous responses $Y_+$ and $Y_*$ were then created by adding up the two basics with or without an extra product term, representing the additive effect and the interactive effect of the genomic and image profile. Four binary responds $D_G, D_V, D_+, D_*$ were also generated by putting the four continuous ones through inverse logit and draw cases from the resulting probabilities. The 8 responses can be written as:
\begin{equation*} \label{eq:SIM}
\begin{split}
  \boldsymbol{\beta_G} &= [N(0,1) B(0.05)]_{1:|G|} \\
  \boldsymbol{\beta_V} &= [N(0,1) B(0.05)]_{1:|V|} \\
  \boldsymbol{Y_G}     &= \boldsymbol{X_G \beta_G} \\
  \boldsymbol{Y_V}     &= \boldsymbol{X_V \beta_V} \\
  \boldsymbol{Y_+}     &= \boldsymbol{Y_G} + \boldsymbol{Y_V} \\
  \boldsymbol{Y_*}     &= \boldsymbol{Y_G} + \boldsymbol{Y_V} + \boldsymbol{Y_G Y_V} \\
  \boldsymbol{Y_G}     &= B(logit^{-1}(\boldsymbol{Y_G})) \\
  \boldsymbol{Y_V}     &= B(logit^{-1}(\boldsymbol{Y_V})) \\
  \boldsymbol{Y_+}     &= B(logit^{-1}(\boldsymbol{Y_+})) \\
  \boldsymbol{Y_*}     &= B(logit^{-1}(\boldsymbol{Y_*}))
\end{split}
\end{equation*}
where $k$ and $l$ indices variants within the choson genomic and image units, and $|G|$ and $|V|$ is the variant count. For now $|V|$ is fixed to 512 -- the number of vertices in a WM sample region.

To access the power performance of the the potential benifit of joint U versus single profile based statistics, and the encoded vertices versus the original ones, two factors are considered simutaniously in the simulation study. First is the construct of U statistics which may result in correct, partially or completely mis-specification when faced with responses of various underlying effect. The available consitutions are genomic only ($U_G$), image only ($U_V$), or joint ($U_J$), corresponding to the 3 aforementioned hypothesis. The second factor to be considered is the evaluation of image similarity. With the same evaluator $g(v_i, v_j)$, one could plugin either the original or the encoded vertices. Alternatively, as a reference method, the widely used vertex-wise analysis (VWA) is also implemented, which is an analogy of GWAS if one see a vertices as a SNP. In brief, VWA first smoothes the attributes of vertices (e.g. gray matter thickness, average curvature) via a gaussian blur process to attenuate the noises within the image profile, then evaluate the pairwise similarity with respect to each vertex and performs a large number of tests, after which the most significant one is chosen as the representative of the whole testing unit. All scensiable combinations of the two factors were tested against all 8 responses across a sample sized spectrium from 100 to 800.

\begin{tabular}{|c|c|c|c|}
  \hline
  *   & Encoded Vertex, regional & Raw Vertex, regional & Raw vertex, VWA  \\ \hline
  V   & Y              & Y          & Y            \\ \hline
  G   & N              & N          & N            \\ \hline
  G+V & Y              & Y          & Y            \\ \hline
\end{tabular}

For continuous outcomes, the genome based $U_G$ had the best performace out of the 3 statistics when the response was $Y_G$ which is also of purely genomic origin, but no power at all could be gained when the image originated response $Y_V$ was being tested against. Conversely, the vertex based $U_V$ yielded better power then the other two when $Y_V$ was selected as phenotype, and no power when $Y_G$ was used. The joint statistics $U_J$ outperformed the two basic statistics $U_G$ and $U_V$ when the response was the additive $Y_+$ or the higher ordered $Y_*$. Between the two last responses, all 3 statistics displayed slightly higher power with the addtive $Y_*$ then with the ineractive $Y_*$. As expected, the consitution of an U statistics that better matches the underlying effect of the response should asertaine heigher power across all sample sizes, while under a scenario of complete mis-specification no statistical power should be seen. Partially mis-specified U statistics maintained a performance close to that of the perfect match, thus making the joint statistic $U_J$ an overall better choice [figure ?].
By limiting the comparesion among tests involving raw vertices, one could see that regional U statistics outperformed VWA under all circumstances except the total mis-specification where both showed no power. By only looking at regional U tests involving vertices, those employing encoded vertices would be equal or slightly underpowered at smallest sample sizes (N=100) but enjoy larger power boost alongside an constant increament in sample size. When the U statistic is correctly specified, the power is always higher if encoded vertices were used to construct $U_G$ or $U_J$. When a partial mis-specification was inccured by testing the vertex effect$Y_V$ with a joint U statistics $U_J$, where the encoded vertices outperformed the raw vertices at a higher sample size (>500), suggesting that the benefit brought by image feature abstraction could "back fire" when the statistical model is partially or fully wrong and a small sample is given. 
For binary responses, the statistical power shared similar patten under every combination of U-statistic and vertex term formulation [table ?], albeit universally lower then its continuous counterpart. Another noteworthy fact is that the U statistics are indifferent to the preprocessing of the binary response thanks to the rank normal quantile standardization. The same U score are obtained regardless the U kernel being based on the deviance residual, the least squre resisual, or the binary response itself.
Nonetheless, the simulation demonstrated that the joint U statistics is an omnibus test which is more robust under most circumstances, especially when the prior knowledge of effect composition is unknown to the investigator. The feature abstration and dimension reduction is generally benificial, in that the resulting vertex code offered an accelerated power boost when sample sizes is linearly increased.

\section{real data analysis}
The baseline data of the same 806 ANDI study participants were used for the real data analysis. The genomic testing units are still genes, and thus the calculation of the corresponding similarity component $S^G_ij$ of the U statistics is the same with the simulation study. The image testing units are 68 standard anatomy regions in the cortical surface, and only the encoded vertices are used. To train the 68 encoders for these region, all 806 subjects are utilized since they all have MRI image profile. The subsequenct U statistic test only included 327 subjects comprised of 47 definite Alzheimer's disease (AD) cases and 280 healthy controls (CN). The rest 479 participants diagnosed with pre-AD or stage such as minor congitive impairment (MCI) or probable AD (PAD) were excluded from the analysis. The dichotomous outcome was first regressed on 7 known risk factors of AD, namely age, gender, race, ethnicity, years of education, marriage status, ever smoking, and APOE $\epsilon$4 allele count. The regression residuals were then taken as an continuous phenotype to construct the U kernel and weight terms. The comprehensive resuls of all 3 types of U statistics are show in Figure [?], the top 10 most significant tests are listed in table [?]. 
The results showed that the vertex based test are statistically more significance then the genome based ones, which is coherent with the fact that the neuron loss and thinning of gray matter is an proxmate indicator of the progression of Alzheimer's , while genomic profile is a remote predictor of a greater uncertainty. A noteworthy phenominium is how the joint U statistic ($U_J$) could "borrow" power from the two simpler genomic ($U_G$) and vertex ($U_V$) statistics, such that for most combinations of gene and @M regions the p-value of the joint test aligns closer to the more significant one of the two simpler U statistics, moreover, when both $U_G$ and $U_V$ are moderately significant, $U_J$ will be more so then either of them. In fact, the top 10 tests in table[?] were all from the joint U statistics $U_G$, which is the combination of the most significant WM region - left superiortemporal and the 10 most significant genes from tests $U_V$ and $U_G$, respectively. Decection of left superiortemporal by either vertex U or joint U test is consistant with known fact that thinning of this region is a strong indicator for AD diagnosis, which is backed by ample evidences from imaging and atopsy studies [?]. The top genes so far detected couldn't remain statistically significant after multiple-testing correction and didn't contain or close to the top 20 SNPs [ref: AD SNP database] reported so far by GWAS and meta-analysis, they are likely to be due to chance. 

\section{discussion}
Using the similarity U statistics, the study method managed to combined the information from cortical surface and genomic profiles to detect their joint association with the phenotype of interests, and by abstracting the low dimensional encoding of image features with deep-learning prior to the joint U test, the method also received a growing power boost over increased sample size. 

The strengthes of the method are its robustness, versertility and computational capacity. The robustness is demenstrated by its capability of maintaining close to optimal power allowed by the weighted U framework when confronted with unknown effect constitution, and a huge number of rare genomic variants which alone offers little to no power. Being a non-parametric method, the joint U is versertile enough to tolerate a diverse profile distributions and can be easly incoperated with components more than 3 in this study. As for the computation, the joint U test of all combinations of 68 cortical regions and 40000 genes can be done within 24 hour on HPCC of MSU, with a peak load of 400 cpu. Though the deep-learning and vertex encoding will take another 6 hours in the same cluster, caching the encoded vertices could save future computation.

As a trade-off, favoring regional test for a less severe multiple testing, an enhanced heterogeneity of the testing unit, and a reduced computation time also raises the issue of deciding the testing units, plus the added noise when the size of a unit grows. For now the genes and anatomy regions are conveniently chosen since they are functional elements in the genome and cortical surface, respectively, which facilitate the check of biological plausibility. Aside from the the low sample size (N=327), taking genes as test units may contribute to the failure of replicating the single variant based GWAS findings. Figuring out the optimal genome and cortical segregation that balances the need of meaningful casual inference and statistically power is beyond this work.

By increase the initial layer size and total number of layers, The deep-learning could go deeper, creating a more compact, more abstract vertex code which may boost the statistical power further. However, training the encoder require an exponential time with respect the network depth, a GPU cluster maybe a temporary computational solution. To fully utilize deep-learning, one could also build encoder for genomic units which is also of high dimensional, but again the computation can be intense because the gene is much more numerous than cortical regions, even if we choose only around 20,000 protein coding genes.

The feature abstraction may actually better dementrated its power in the real data analysis because the real life disease of central neural system usually morphs the cortical surface. However, the current simulated randomly assigned effect to vertices across entire unit, instead of assigning to vertices clusterd at visible cortical featuers -- most commonly a few dozen named sulci and gyri. It is likely our simulation study only treated the deep encoder as an noise reduction tool resembling the Gaussian blur process employed by traditional vertex-wise analysis (VWA), or a generalized principal component analysis in terms of dimension reduction. A visible feature based simulation study is needed to provide counter-factual evidence of the full capacity of deep-learning.

%\begin{bibliography}
%\end{bibliography}

\end{document}
