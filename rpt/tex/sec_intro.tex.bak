\section{Introduction}
The decade long search of casual variant by genome wide associatio analysis (GWA) hasn't been satisfying. So far GWA hardly find any single nucleotite variant (SNV) with an large enough effect to act as a stand along necessary cause of any complex diseases. Although a large number of statistically significant common variants were indeed identified by GWA, only a moderate fraction of heritability have been explained by the totality of these finding\cite{GWA1, GWA2}. Despite the setback, human genome is still an intriging source of curiosity owning to its intrisic advantages. When viewed as an exposure, genetic polymophisim is consistant throughout an individual's life course and all types of organism, saving the complication of study design. Also, as one of the fundermental causes of all biological processes, genomic polymophisim is not suseptable to reverse causality. From a population perspective, the occurance of genetic variation mimic a random assignment of treatment in an quasi-experiment, which in turn can be exploited to infer non-genetic effect through an intrumental variable approach, such as Mandilian Randomization \cite{MR1, MR2}. These features keeps genomic analysis a promising tool for casual inference.

\subsection{Analysis of Rare Genomic Variants}
The "rare variant, common disease (RVCD)" hypothesis aims to to explain of the "missing heritability" which GWA failed to capture. RVCD states the gap could be attributed to rare variants of moderate to large effect not covered by GWA \cite{RVCD1}. The Next Generation Sequencing (NGS) projects, growing in both number and scale over the last decade, offered numerous data sources for the analysis of rare variants. However, the stockpiling data also raise a number of methodological challenges. For one, the variants in a NGS profile is much denser than a GWA profile, which poses intense computation and multiple testing should the traditional per-variant based screening procedures is applied. Also, as the name suggested, the newly detected rare variants came with their minor allele frequencies (MAF) close to 0. As a consequence, the lack of heterogeneity in genotype threatens the statistical power for studies of moderate and small sample size. So far the most popular remedy is signal aggregation, that is, instead of screening the whole profile one variant after another, we first group the variants accroding to certain criteria, and subsequently all the variants in one group are tested together as an unit, during which the signal of the members are aggregated. The aggregation can be achieved by either collapsing the grouped variants into a single variant \cite{Burden1} before the statistical test, or by testing all the variants together with a multivariant approach \cite{UST1, UST2, SKT}. The aggregation can also be done after a per-variant screening, by summarizing the test statistics (e.g. p-values) of group members into one statistics \cite{Dai:2015, plink1}. Grouping and aggregation drastically reduce the number of hypothesis testing, and improve the heterogeniety or the testing unit. A dilemma comes along though, is the choice of grouping criteria. The most common criteria refer prior knowledge of biological function, resulting in gene or pathway based grouping. One could also group the variants by every few or by a threshold of linkage disequilibrium (LD) \cite{plink1}. Since the focus of this study is hypothesis testing, for now, we adopt a gene based grouping scheme for both simulation and real data analysis.

\subsection{Incorporating Imaging Data}
A hard truth contributing to the unsatisfactory performance of GWA is the genetic effect being intrinsically weak over a complex disease, owning to the large ``black box'' between the upstream genomic variants and the health outcome at the downstream far end. It is always desirable to probe the ``black box'' by incorporating other intermidiate biological profiles. The added information should increase the chance of detecting strong association, especially when the new profiles is in the casual pathway from the genomic profile to the disease. Our method borrows additional information from neuroimaging data, knowning that the cerebral structure is a powerful indicator of neurological disorder. Much like the GWA, similar, per-variant screening procedures can be applied to image profiles if proper definition of variants and their values are given. Taking the structured MRI as an example, it is natural to view voxels in the MRI slices as variants, and the normalized brightness of those voxels as their values. Following such definition, a per-voxel screening can be immediately done to the entire profile to search for signifiant loci in the brain, which is typically called voxel-wise analysis (VWA) \cite{VWA1, VWA2, VWA3, VWA4}. In our study, the neuroimaging profiles are preprocessed by \FS, a freely distributed neuroimaging processing software. Instead of the orignal structure MRI slices, the cortex reconstructed from these slices is taken as image profile. Instead of the voxels, the variants are redefined as the vertices spanning the 3D cortex, while the values are defined as the gray matter thinkness at these vertices. \FS already comes with its own vertex-wise analysis (the abbreviation happens to be ``VWA'' as well), which in essential is applying generalized mixed linear model to very vertex \cite{FS:Anl1, FS:Anl2}. The per-variant screening approach may work better for the cortex than for the genomic profile, because the value of an vertex is continuous, which mean it does not suffer low variability like a rare genomic variant does. However, the multiple testing issue might be worsen because the neighboering image variants cover tightly connected cerebral tissue, their value (thickness of gray matter) are highly coorelated. Thus, the grouping and aggregation strategy could also benefit analysis involving cortex profiles. As an analogy of gene based grouping and aggregation of genomic variants, the cortical vertices can be conveniently grouped into 68 recognized functional anatomy regions (34 in each hemisphere) and have each of them analyzed as an unit. In the simulation study, we will take out small regions from the cortex and compare the power performance of an aggregated unit against the per-variant based VWA.

\subsection{Dimension reduction using Deep Artifical Neural Network}
The typical testing unit of the image profile is also high dimensional, oftentimes even higher then most of the genomic testing units. For our study data, after segregating cortex into 68 anatomy regions with \FS, the chance of a randomly picked cortex region holding more variants than a randomly picked gene is 96.8\%. Although the multivariate algorithms that works with genomic testing unit can be applied to the cortex with little to no rework, it would be desirable to first reduce the dimensionality of a image based test unit before the analysis. One popular trend is using deep artificial neural network to extract high order features from the raw profile, which not only lowers the dimensionality but also raises the signal - noise ratio \cite{DL:Intro1}. The potent of deep network lies in its capability to accumulatively encrypt incoming knowledge with unsupervised training technique. In other words, as long as the future collection of image data follows protocols compatible to the current, the existing deep network can be incrementally refined to achieve more precise and concise feature abstraction. For now, included in the method is a series of stacked autoencoders for each the cortical regions, which are light weighted deep network whose capability is comparable to the more powerful but much slower deep belief networks \cite{DL:SDA1, DL:Intro2}. The formulation and calibration of the stacked autoencoder is detailed in the method section and appendix, simulation studies are conducted to compare the performance of original cortical vertices versus the high order features abstracted from these vertices.

\subsection{Association Analysis of multiple high-dim Component}
The introduction of additional high dimensional profile also complicated the constitution of an association. In most cases, an investigator does not know the effect composition in advance, that is, the variation of phenotypes can be attributed to either genomic polymorphism or cortical vertex alone, or both, either with or without unknown type of interaction, mediation or even feedback loops among these profiles. Thus, the method in mind must be sensitive to the association but at the same time robust enough to maintain statistical power when the putative mode is unavoidably, partially mis-representing the reality. Also, the value of genomic and cortical variants could be drawn from distinct non-normal distriubtions, while the phenotype profile could also be multivariate, with each element following unknown distribution. Considering these uncertainties, the test statistic in mide should also be versatile enough to counter an admixture of possibally skewed, non-normally distributed data component. More ever, even with the grouping and aggregation that reduces the number of tests, the method has to be reasonablly fast and powerful, because the combination of two or more profile can still be a huge number (in our case, there are approximately 41,000 genes and 68 cortex regions), which is already a computational and multiple testing challange, and not to forget is the dimensionality of each testing unit. The core of the propsed method is the generalized multivariate similarity U (GMSU) postulated by \cite{UST1, UST2}, a fast, distribution insensitive non-parametric statistics. GMSU is capable of capturing associations among multiple high dimensional profiles without assuming the effect composition.

In summary, seeing the increasingly denser genomic profile with a growing number of rare variants, and the avalability of other high dimensional biological profiles motivated this study to design and test a befitting method capable of exploring these new data to search for the heritability. We propose the similarity U statistic to involve both genomic and cortex profile. Specifically to the cortex profile, we adopt variant grouping and aggregation from genomic analysis to bootst statistical power, and the used stacked autoencoders to reduce dimensionalality and noise of the testing units.