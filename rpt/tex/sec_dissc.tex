\section{discussion}
The joint U statistics can effectively combine information from multiple sources and types to achieve better statistical power, which, in our case, is the joint U with both genomic and vertex components. The major strength of joint U test is robustness, demenstrated by its capability of retaining the power performance close to an optimal model specification, when the true constitution of effect was unknown a prior. Being a non-parametric method, the joint U is versertile enough to tolerate a diverse type of profiles. In our case, the the original vertex data may be generated from a multivariant normal distribution, while the encoded vertex could come from a unknown distribution type, yet either of them can be seemlessly included in the final statistics as a component term. As for the computation, the joint U test finished all combinations of 68 cortical regions and more than 20,000 genes within 24 hour on HPCC of MSU, with a peak load of 400 cpu. 

The study also showed the higher order feature abstracted by the SA not only has much a lower dimension than the original profile, but also help the later U test involving the vertex component. The usefulness of the SA offered some intriguing prospects. First, it should grow deeper, by increasing the number of layers. A deeper SA is capable of creating more compact and more meaningful abstraction of the original vertex data, and therefore boost the statistical power even further. Second, the way we learned the SA, that is, in an unsupervised manner, implies greater use of data not allocated for any specific study, as long as the image aquisition follows compatible protocols. For example, the 489 subjects who didn't enter the real data analysis due to uncertainly in diagnosis, can still contribute thire cortical surface profiles to the knowledge encrypted in the SA. If we go even further in the future, besides the 806 subjects with both high dimensional profiles available, the rest, over one thousand ADNI1 and ADNI2 participants not included in this study due to the lack of WGS profiles, could also help building the SA since with their MRI scan at baseline. Aside from cortical surface profile, the learning and encoding can be applied to the genomic profile as well, which is also high dimensional. The advantages of unsupervised learning could be even more pronounced with genomic data, given the abundance of publicly available databanks of human genome such as the 1000 genome project.

The study is not without issues however. Although the information from the cortical surface profile did help the joint U statistics to achieve whole genome, whole surface significance, but the genomic component was still under powered, which could be the limitation of our small sample (N=327). As we just mentioned, without additional sample, the power could still be hopefully boosted by constructing SA for genomic regions that encrypt general knowledge of human genome from large amount of public data.

We also believe the feature abstraction better demonstrated its potent in the real data analysis than in the simulation studies, because a real life diseases in the CNS usually morphe the cortical surface, making visible changes of features, but the simulation only assigned effective vertices randomly across the testing unit, instead of having the them clusterd at visible surface featuers such as the few dozen named sulci and gyri. As a consequence, our simulation treated the deep encoder more like an uniform noise reduction tool resembling the Gaussian filter employed by many traditional vertex-wise analysis (VWA), or at best, a generalized principal component analysis aimming to reduce the dimensionality. A simulation driven by visible feature is required to provide evidence of the full capacity of an SA.

The machine learning is still a intense operation even with the aid of greedy layer-wise pre-training. Creating 68 SA for all cortical surface regions took another 6 hours in the same HPCC cluster. A GPU cluster maybe a temporary solution, yet, when much deeper and wider SA is desired, the computation will eventually overrun the expedience brough by the pre-training. Also, as we want to the features to be abstracted from the genomic profile as well, the sheer number of genomic testing unit (more than 20000 genes) is already a serious concern. We are now closely following the machine learning literature, any learning scheme of better performace will be potentially helpful for both the deepening of an SA and its application on the numerous genomic features.