\section{discussion}
The proposed method can effectively combine information from multiple high dimensional data sources of distinct type to achieve higher statistical power, which in our case are the joint signal of genomic and cortex profiles. The major strength of the metod is its robustness and versatility. The robustness is demenstrated by its ability of retaining power that is close to the optimal model specification, when the true effect constitution was not known $a prior$. The versatility is shown by its acceptance of a wide variety of profiles regardless of their distribution. The proposed method also applies variant grouping and signal aggregation strategy to the cortex profiles, which not only considerably boosted the statistcial power over the per-varaint screening procedure, but also save the computation time. With properly assigned kernel functions, the method can also incorporate additional profiles into the analysis, such as other ``omics'' data closer to the upstream genomics, or inflammatory bio-markers closer to the down-stream health outocme, without worrying about many possible high order interactions among more than two or three types of profile.

The method also build one of its components with the high order features abstracted from the cortex profile of eligible samples ($n=327$) using the stacked autoencoders (SA) tained with the whole dataset ($n=806$). The abstracted features not only has lower dimensionality, but also help the methods to achieve slightly higher statistical power. We feel the potent of the deep artifical neural network is only explored at its surface, and the usefulness of the SA opened up intriguing prospects. First, the encoders can go deeper, by increasing the number of layers. Though a deeper SA is harder to train, it is capable of creating more compact yet more meaningful abstraction from the orignal input, subsequently boost the statistical power even further yet lower the computation load. Second, as mentioned before, the SA was trained in an unsupervised manner, which means a bank of data collected not for any particular study can be used to increamentally refine the existing SA, as long as the data collection follows compatible protocols. In our case, the 489 subjects who couldn't enter the real data analysis due to uncertainly in diagnosis, still contributed their cortex profile to train the SAs. Further, in the near future, the rest of the ADNI participants who were not included in this study due to the lack of next generation sequencing genomic profile, can still help to refine the SAs we already have, because every ADNI participants has structure MRI data. Lastly, it is very tempting to use the same training and abstraction approach on the genomic profile, which is also high dimentional and growing in size, and again, with unsupervised training technique, we can utilize a huge wealth of NGS data from collaborators and public database, such as the freely available 1000 genome project data.

These prospects, are not coming without challenges. First regarding the construction of the joint U statistic, it is easy to bring in more high dimensional components, but when an overall association is detected, it is very hard to tease out exactly how each component contributed to that association, and it is also hard to tell the  (other than a simple product form in the simulation) and effect size of that interaction between the components when more than two kernel functions are involved. In the real data analysis, when the joint U statistic $U_J$ turned out to be statistically significant while the two simplified $U_G$ and $_V$ didn't, the interaction between the genomic and cortex profile is guaranteed, but one can not be sure this interaction is associated with the phenotype profile. When all three statistics are significant, we known from the simulation study the interaction between genomic and cortex profiles may, or may not exists. The proposed method is better suited for fast screening of a large number of combinations of multiple high dimensional profiles, but in the end an explicit modeling is still reqiured for the categorization and quantifying of the associations.

Another challenge involves the way variants are grouped, and the way stacked autoencoders are trained and utilized. An autoencoder require constant input dimension. The cortex profile is stable in the number of vertices ($2 \times 1683400$), so are the 67 anatomy regions, if the current and furture samples are registered to the same atlas. We trained 67 stacked autoencoders for each region, but in reality, they are far too coarse to accurately pinpoint the loci in the cortex. For the genomic data, grouping variants by gene is an accepted compromise between accuracy and statistical power, however, the input dimension is not fixedsince the number and location of variants can differ from study to study, also the dimension of a gene can differ from sample to sample due to the indels and copy number variation. Besides, training over $40$ thousands stacked autoencoders is hardly affordable given the intensity of computation even if the greedy layer-wise pretrain techneque is used. So a grouping and training scheme not colluded with any existing functional information is required. Instead of training an SA for any specific gene or cortical region, an SA will be only be trained for an resonable input dimensionality, which is small enough to both satisfy the desired accuracy and ease the computation, but not too small so no meaningful high order feature is contained. Taking the cortex profile as an example, if the manipulation of vertices in the 3D space is not too complicated, one could first realign the vertices to a sphere with uniform spacing without altering the topology of the cortex, after which only one SA will be trained to encrypt the general knowledge of any cortical region mapped to a sphere surface of, for example, 5 degree in both latitude and longitude. For the genomic profile, only one SA will be trained to encrypt the general knowledge of, for example, a 100kb segment any where in the genome except the proximity of centromeres and telomeres. Yet, another complication will surface due to the inclusion of redundant sequences between the nucleotide polymorphisms, making the abstracted feature likely to have even higher dimension than the polymorphism in any 100kb region, unless a very deep stacked autoencoder is successfully trained. What can be guaranteed however, is the abundance of training materials.

There is also rooms to improve the simulation studies. We known the disease in CNS does not alter the genome, but it does so to the cortex, often making visible changes of features. The current simulation only assigned effects to vertices randomly chosen across a testing unit, instead of changing the thickness for a group of connected vertices to creat a visible shrinkage. As an result, the benefit of feature abstraction may be more pronounced in the real data analysis, without the evidence from the simulation study. The desired simulation however, may required intense human input because the change of features must mimic the real life clinical experiences, plus the difficulty to manipulate the vertices in 3D space. A more approachable simulation can be done by assigning the effect to vertices clusterd in existing visible featuers such as the few dozen named sulci and gyri which is already marked by \FS.

Should these challenges be addressed and improvements be realized, we see a greater used of the rich and ever accumulating data, for more powerful inference of the relationship between complex disease and genome.
