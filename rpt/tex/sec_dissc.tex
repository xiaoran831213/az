\section{discussion}
Using the similarity U statistics, the study method managed to combined the information from cortical surface and genomic profiles to detect their joint association with the phenotype of interests, and by abstracting the low dimensional encoding of image features with deep-learning prior to the joint U test, the method also received a growing power boost over increased sample size. 

The strengthes of the method are its robustness, versertility and computational capacity. The robustness is demenstrated by its capability of maintaining close to optimal power allowed by the weighted U framework when confronted with unknown effect constitution, and a huge number of rare genomic variants which alone offers little to no power. Being a non-parametric method, the joint U is versertile enough to tolerate a diverse profile distributions and can be easly incoperated with components more than 3 in this study. As for the computation, the joint U test of all combinations of 68 cortical regions and 40000 genes can be done within 24 hour on HPCC of MSU, with a peak load of 400 cpu. Though the deep-learning and vertex encoding will take another 6 hours in the same cluster, caching the encoded vertices could save future computation.

As a trade-off, favoring regional test for a less severe multiple testing, an enhanced heterogeneity of the testing unit, and a reduced computation time also raises the issue of deciding the testing units, plus the added noise when the size of a unit grows. For now the genes and anatomy regions are conveniently chosen since they are functional elements in the genome and cortical surface, respectively, which facilitate the check of biological plausibility. Aside from the the low sample size (N=327), taking genes as test units may contribute to the failure of replicating the single variant based GWAS findings. Figuring out the optimal genome and cortical segregation that balances the need of meaningful casual inference and statistically power is beyond this work.

By increase the initial layer size and total number of layers, The deep-learning could go deeper, creating a more compact, more abstract vertex code which may boost the statistical power further. However, training the encoder require an exponential time with respect the network depth, a GPU cluster maybe a temporary computational solution. To fully utilize deep-learning, one could also build encoder for genomic units which is also of high dimensional, but again the computation can be intense because the gene is much more numerous than cortical regions, even if we choose only around 20,000 protein coding genes.

The feature abstraction may actually better dementrated its power in the real data analysis because the real life disease of central neural system usually morphs the cortical surface. However, the current simulated randomly assigned effect to vertices across entire unit, instead of assigning to vertices clusterd at visible cortical featuers -- most commonly a few dozen named sulci and gyri. It is likely our simulation study only treated the deep encoder as an noise reduction tool resembling the Gaussian blur process employed by traditional vertex-wise analysis (VWA), or a generalized principal component analysis in terms of dimension reduction. A visible feature based simulation study is needed to provide counter-factual evidence of the full capacity of deep-learning.
