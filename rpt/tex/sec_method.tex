\section{Method}

\subsection{Generalized Multivariate Similarity U Statistic}
The goal of our method is to jointly test the possible association among the genomic, cortical surface and the phnotype profiles. The mothod in mind must address several issues. For one, the inclusion of the vertices into the genomic analysis complicates the modeling of association since in most cases an investigator does not know the effect composition in advance, that is, the variation of phenotypes can be attributed to genomic variants or vertices alone, or both, either with or without some unknown type of interaction, mediation or even feedback loops among all three types of profiles. Thus, the method must be sensitive but at the same time robust enough to maintain statistical power when the putative mode is unavoidably missepecified. Also, the value of both genomic and image variants could be generated by distinct and unknown distinct distriubtion types, while the phenotype profile could also be multivariate (e.g. disease diagnosis plus additional demographics and known risk factors), with each element following distinct and uncertain distribution types. Taking these uncertainty into consideration, the test statistic should be versatile enough to counter an admixture of possibally skewed, non-normally distributed data component. More ever, it has to be reasonablly fast in order to deal with the high dimensional profiles comprised of up to tens of thousands genomic polymophisms or vertices (before being replaced with high order features). In this study, we implement the generalized multivariate similarity U statistic (GMSU) postulated by Changshuai et. al. \cite{HWU}. GMSU is computationally efficient, and highly flexible since no distribution assumption will be imposed on genomic and cortical surface profiles, together with a rank based normalization procedure, the U statistic is also invulnerable to multidimensional phenotype with constituents from a mixture of unknown distributions.
To derive the generalized similarity U statistic, three kernel functions are chosen for each of the profiles accordingly. A kernel function measure the similarity between a pair of samples with respect to one of the profiles. Depending on the charisteristics of that profile, the exact form of kernal function can be flexible, as long as it is symmetric and has finite second moment. Thus, measurement $f$ is a valid U kernal function if $f(x_i,x_j)=f(x_j,x_i)$ and $E(f^2(X_1, X_2))<+\infty$ are satisfied. For the current stuty, the functions are chosen according to common practices.

For genomic variants taking values from discrate minor allele count ${0, 1 \textrm{ and } 2}$, the common choice of similarity mesurement is the identical by state (IBS) kernel function
\label{eq:wSG}
\newcommand{\vg}{\pmb{g}}
\newcommand{\GG}{\pmb{G}}
\[ f_G(\vg_{i.}, \vg_{j.}) = \frac{\sum_{m=1}^{|G|}{w_m(2 - |g_{im} - g_{jm}|)}} {2\sum_{m=1}^{|G|}{w_m}}, \]
where $g_{im}$ and $g_{jm}$ is the value of $m$ th. variant in the testing unit (e.g. a gene) taken from the $i$ th. and $j$ th. samples, respectively, and $|G|$ is the dimensionality of the testing unit (e.g. number of polymorphism in a gene). $w_m$ assigns weight to the $m th.$ variant according to \textit{a prior} hypothesis, one example is the minor allele frequency (MAF) based $w_m=\frac{1}{\sqrt{MAF(g_{.m})(1-MAF(g_{.m}))}}$ which gives more emphasize on rare variants. Without any prior knowledge though, the IBS kernal is simplifed to $\frac{\sum_{m = 1}^{|G|}{(2-|g_{im} - g_{jm}|)}}{2|G|}$ by setting $w_m \equiv 1$.

With respect to cortical surface profiles constituted by connected vertices who took continuous values within $[0,1]$, we would used the euclidian distance based function
\label{eq:wSV}
\[  f_V(v_{i.},v_{j.}) = \exp{ [-\frac{\sum_{m=1}^{|V|}{w_m(u_{im}-u_{jm})^2}} {\sum_{m=1}^{|V|}{w_m}}] } \]
to measure the simiarity between sample $i$ and $j$, which is also called a Gaussian kernel function. Here $v_{im}$ and $v_{jm}$ are values of the $m$ th. vertex in the cortical surface testing unit of the $i$ th. and $j$ th. sample, respectively, and $|V|$ denote the number of vertices in the testing unit. The vertices in the cortical surface profile can also be weighted by vector $\boldsymbol{w}.$, but for now we have no prior knowledge of the relative importance of the vertices, the Gaussian kernal function is simplifed to $\exp{[-\frac{\sum_{m=1}^{|V|}{(v_{im}-v_{jm})^2}} {|V|}]}$.

Lastly, for a multivariate phenotype profile whose elements may be drawn from a variety of unknown distributions, we first normalize its elements with rank normal quantile function
\[ q_{im}=\frac{\Phi^{-1}[rank(y_{im})-0.5)]}{|Y|} \]
where $y_{im}$ is the value of the $m$ th. element of the phenotype profile of the $i$ th. sample, and $|Y|$ is the dimensionality of the phenotype (i.e. number of elements). Doing so not only corrects skewed elements, but also bypass the complication of admixed distribution type commanly introduced by a multivariate phenotype. As a result, the phenotype based similarity can be measured in a manner similar to that of cortical surface:
\[ f_Y(q_{i.},q_{j.}) = \exp{[-\frac{\sum_{m=1}^{|Y|}{w_m(q_{im}-q_{jm})^2}} {\sum_{m=1}^{|Y|}{w_m}}]} \]
where $q_{im}$ is the values of the $m$ th. element of the normalized phenotype profile, again with weight $w_m$ denotes the ralative importance of that elements. In case of a phenotype with only one dimension, that is, $|Y|=1$, the similarity measurement simplifys to $\exp{[-(q_i - q_j)^2]}$.
All three kernel functions must be centralized, which is done by substracting the function value at each pair $(i,j)$ with the two marginal mean of all pairs involving $i$ and $j$, respectively, then adding the overall mean of all pairs to it \cite{HWU}. Taking the kernal function of genomic profile as an example, the centralized measurement is
\[ \tilde{f}_G = f_G(\vg_{i.}, \vg_{j.})-\frac{1}{N} \sum_{k=1}^N{f_G(\vg_{i.}, \vg_{k.})}-\frac{1}{N}\sum_{l=1}^N{f_G(\vg_{l.}, \vg_{j.})}+\frac{2}{N(N-1)}\sum_{1 \le k < l \le N}{f_G(\vg_{l.}, \vg_{k.})}\]
where $N$ is number of samples. Finally, the generalized multivariate similarity U statistics is the mean product of all similarity measurement excluding the self-pairs, which is
\[ U_J = \frac{2}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{f}_G() \tilde{f}_V() \tilde{f}_Y(). \]
The actual implementation calculated three symmetric $N \times N$ similarity matrices to facilitate later manipulation. The U statistic follows a mixture of $\chi_1^2$ distrubtion, a $p$ value can then be calculated using Davis method \cite{HWU}.

For now, the genomic and vertex similarity are treated U-weight terms, while the phenotype similarity acts as the U-kernel term. In general, the assignment of U-kernal and U-weight terms does not alter the limiting distribution of the final U statistics. More specific association test can be perform by dropping one of the U-weight terms. To see if only the phenotype and genomic profiles are associated, construct the U statistics without vertex similarity term:
\begin{displaymath}
  U^{GY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{G} \tilde{S}_{ij}^{Y}.
\end{displaymath}
Likewise, dropping the genomic similarity to test if only the phenotype and genomic profiles are uncorrelated:
\begin{displaymath}
  U^{VY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{V} \tilde{S}_{ij}^{Y}.
\end{displaymath}

\subsection{Stacked Autoencoder}
The stacked autoencoder is an artificial neural network mimicking sentimental visual processing, its purpose is to abstract high order features from the raw image profile. The high order feature not only has lower dimensionality, but is also more relevent  to decision making. Taking our data as an example, being able to see the approximate location and size of the laceration sites in the cortical surface, is far more important than knowning the exact thickness, curvature and coordinates of every vertex in the original profile. Thus, deside the dimension reduction, we also anticipate a power boost for the joint similarity U statistics when the orignal cortical surface profile is replaced with the abstracted features.

An SA is formed by layers of autoencoders stacking on top of each other, hence the name "Stacked". An autoencoder layer performs a linear recombination of the input elements, followed by an element-wise non-linear transformation. Usually, we make sure the output has a lower dimensionality then the input to ensure feature abstraction and dimension reduction. The autoencoder at the $i$ th. layer of the stack is written as:
\begin{equation} \label{eq:AE}
    \zEC_i^{d_i} = s(\WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}) \\
\end{equation},
where $\zEI{i}{i}$ is the layer output and $\zEI{i-1}{i-1}$ is the layer input, which is also the output of the autoencoder from down below, that is, the $i-1$ th. layer in the stack. The cross product between the input vector $\zEI{i}{i}$ and the weight matrix $\WEI{i}{i}{i-1}$ followed by the addition of the offset $\bEI{i}{i}$ achieves the linear recombination of input elements. The superscript $d_{i-1}$ and $d_i$ denote the dimensionality of data and structure parameters of the autoencoder layer. As mentioned before, to ensure feature abstraction and dimension reduction actually happens, $d_i$ is made smaller than $d_{i-1}$. For our method, a autoencoder layer always halve the dimension of its input, that is, $d_i$ = $d_{i-1}/2$. Lastly, the inverse logit is chosen for the elementwise non-linear transformation, thus
\begin{equation} \label{eq:InvLgt}
    s(\etaEC_i^{d_i})     = [\SGM{\eta_{i1}}, \SGM{\eta_{i2}}, \dots, \SGM{\eta_{d_{id_i}}}]^{\prime},
\end{equation}
where $\etaEC_i^{d_i} = \WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}$ is the linear recombination of the input $\zEI{i-1}{i-1}$; the super script $d_i$ denotes its dimensionality and $k = 1, \dots, d_i$ indexes its $d_i$ elements. The "S" shaped inverse logit curve $\SGM{\eta_{ik}}$ resembles the biological activation of the $k$ th. neuron in the $i$ th. layer of visual cortex when the weighted sum of simulations from all $d_{i-1}$ neurons in the previous layer, $\zEI{i-1}{i-1}$, exceeds a threshold. The weight is taken from the $k$ th. row vector of $\WEI{i}{i-1}{i}$,  and the threshold is the negation of $k$ th. elements in the offest vector $\bEI{i}{i}$.

An SA of $M$ layers, of $P$ dimensional raw input $\xEC^P$ and $Q$ dimensional output $\yHT^Q$, is assembled by recursively taking the output of the lower autoencoder layer as the input of the layer above, and ensuring the dimensionality of the output at the top is $Q$.
\begin{equation} \label{eq:ES}
  \begin{split}
    \yHT^Q &= \zEI{M}{M} \\
    \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M  }{M  }) \\
    \zEI{M-1}{M-1} &= s(\WEI{M-1}{M-1}{M-2} \zEI{M-2}{M-2} + \bEI{M-1}{M-1}) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{i  }{i  } &= s(\WEI{i  }{i  }{i-1} \zEI{i-1}{i-1} + \bEI{i  }{i  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{2  }{2  } &= s(\WEI{2  }{2  }{1  } \zEI{1  }{1  } + \bEI{2  }{2  }) \\
    \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1  }{1  }) \\
    \zEI{0  }{0  } &= \xEC^P,
  \end{split}
\end{equation}
where $\xEC^P$ is the $P$ dimensional raw input of one individual, which is viewed as the output of non-existing $0$ th. autoencoder, with $d_0=P$. Reading from bottom to top, the SA gradually abstracts higher order features from the $P$ dimensional raw input $\xEC^P$, until the dimensionality of the output is as low as $d_M=Q$.

The SA thus constructed is worthless without calibration. One must find the set of structure parameters $\pEC=\{\WEC_1, \bEC_1, \WEC_2, \bEC_2, \dots, \WEC_M, \bEC_M\}$ that best represents the body of knowledge regarding the data, which, in our case, is the knowlege of human cortical surface. Only then the SA is truely capable of abstracting meaningful features out of the raw input instead of haphazardly reducing it into a small but irrelevent output (e.g. a vector of $Q$ random numbers). The ``goodness of abstraction'' can be inferred from the disagreement between the raw input, $\xEC^P$, and its mirrored self, $\xDC^P$, which is the input reconstructed from $yHT^Q$, the abstracted high order features. The rationale is that, the disagreement between $\xEC^P$ and $\xDC^P$ measures how badly the restoration resemble the true original, which indirectly tells us how poorly the encoder had performed, because, a superior abstraction should be less likely to obstruct the recovery effort. Thus, the set of parameter that minimize the difference between the orignal $\xEC^P$ and the reconstructed $xDC^P$ will be considered the optimal configuration of the SA. The calibration guilded by such criteria is called unsupervised training, or unsupervised machine learning. The term ``unsupervised'' states the fact that no external knowledge other than the raw input $xEC^P$ is needed. Instead of tuning the paramters to appeal a certain problem (e.g. logistic regression aims to maximize the classification accuray), unsuersvied learning encourage the SA to manifest itself into an encrypted knowlege of the data of interest, which, in our case, is the knowledge of the human cortial surface. Not requiring labeled data is the greatest strength of unsurpervised learning (e.g. logistic regression requires not just $x$, but pairs of $(x,y)$ to fit $\beta$), which in turn enables a much larger number of sample to contribute to the calibration, and, when new sample become avaible, the training can continue on without reset, mimicing the acceptance of new knowlege. In particular to our method, unsupervised learning make sure all 806 samples could contribute their cortical surface profiles to construct the SA, even if 427 of them cannot enter the joint U statistical analysis because their uncertain disease diagnosis at the baseline.

The new issue on the table is the reconstruction of input, that is, a decoder counterpart of the SA is needed. The most nature way to build the decoder do so is to mirror the encoder structure, so the decoder will also be a stack of unit layers, the number of layers is the same with the SA, each layer also performs linear recombination of its input, followed by a non-linear, element-wise transformation, but, the dimensionality change is in exactly reverse order of the SA. By mirrowing the $i$ th. encoder in the SA, the $i$ th. layer in the decoder stack is
\begin{equation*}
  \zDI{i-1}{i-1} = s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}).
\end{equation*}
 With the layer definition being done, the decoder stack can be assembled in the same way of the SA. Continue with the $M$ layered encoder in \ref{eq:ES}, its decoder counterpart is
\begin{equation} \label{eq:DS}
\begin{split}
  \xDC^P &= \zDI{0}{0} \\
  \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1  }{0  }) \\
  \zDI{1  }{1  } &= s(\WDI{2  }{1  }{2  } \zDI{2  }{2  } + \bDI{2  }{1  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{i-1}{i-1} &= s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{M-2}{M-2} &= s(\WDI{M-1}{M-2}{M-1} \zDI{M-1}{M-1} + \bDI{M-1}{M-2}) \\
  \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M  }{M-1}) \\
  \zDI{M  }{M  } &= \yHT^Q .
\end{split}
\end{equation}
Reading from bottom to top, the decoder gradually adds details back to the abstracted feature $\yHT^Q$, and eventually produce a retored state of the raw input on its top, denoted by $\xDC^P$. The restoration process is reflected, and driven by the dimenality change from $d_M = Q$ to $d_0 = P$, which is in exact reversed order of the SA. Now with both encoder and decoder stacks ready, the complete cycle of encoding and reconstruction is done by treating the top output of the SA (\ref{eq:ES}), that is, the abstracted code $yHT^P$, as the lowest input of the decoder stack. The combined the structure is
\begin{equation} \label{eq:ED}
\begin{split}
  \xDC^P &= \zDI{0}{0} \\
  \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1}{0  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M}{M-1}) \\
  \zDI{M  }{M  } &= \yHT = \zEI{M}{M} \\
  \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M}{M  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1}{1  }) \\
  \zEI{0  }{0  } &= \xEC^P
\end{split}.
\end{equation}
In addition to structure mirroring, a common strategy to train an stacked  autoencoder is to constrain the weight matrix in a decoder layer to be the transpose of its encoder counterpart, that is, by forcing $\WDI{i}{i-1}{i} \equiv \WEIt{i}{i}{i-1}$, the $i$ th. decoder layer become
\begin{equation} \label{eq:CW}
  \zDI{i-1}{i-1} = s(\WEIt{i}{i}{i-1} \zDI{i  }{i  } + \bDI{i  }{i-1}).
\end{equation}
Our method adopts this behavior, because doing so almost halve the number of parameter to be tuned, which is a great boost to the computation, besides, the encoder -- decoder tuples follow the common sense that they are essentially symmetric operations. Most importantly, the constraint encourages the calibration of an optimal SA, instead of a inferior SA coupled with a superior decoder partner. Afterall, our best interest is the abstracted high order features, not the reconstructed input.

The next thing to do is to measure the total disagreement between original profiles and reconstructed profiles for all the samples, which is called the reconstruction loss $L$. For now, the most popular form is cross entrophy
\newcommand{\XECD}{\XEC^{N \times P}}   % encoder input, with dimensions
\newcommand{\XDCD}{\XDC^{N \times P}}   % decoder output, with dimensions
\newcommand{\YHTD}{\YHT^{N \times P}}   % encoder output, with dimensions
\begin{equation} \label{eq:CE}
  L(\XECD, \XDCD) = -\sum_{j=1}^{N}{\sum_{k=1}^P[x_{jk}\log{\tilde{x}_{jk}}+(1-x_{jk})\log(1-\tilde{x}_{jk})]}.
\end{equation}
The two $N \times P$ matrices $\XECD$ and $\XDCD$ store the original and the reconstructed profiles of all $N$ samples, respectively, with each individual sample indexed by $j=(1, \dots, N)$, and the elements in each sample profile indexed by $k = (1, \dots, P)$. One could view the restoration of $\XEC$ from $YHT$ as an array of binary classification problems, with the true probabilities being $\XEC$, the predicted probability being $\XDC$. The reconstruction loss $L$ closely resembles the deviance of a logistic regression analysis which measures of how badly the fitted model reflects the observed reality. With the reconstruction loss $L$ defined, the calibration of SA become a numerical optimization problem
\[ \Par = \min_{\Par} L(\XDCD, \XECD). \]
With the constraint (\ref{eq:CW}) on the weight matrices in the decoder stack, the set of parameters to be tuned is
\[ \Par = \{\WEI{1}{1}{0}, \bEI{1}{1}, \bDI{1}{0}\} \cup \{\WEI{2}{2}{1}, \bEI{2}{2}, \bDI{2}{1}\} \dots \cup \{\WEI{M}{M}{M-1}, \bEI{M}{M}, \bDI{M}{M-1}\}, \]
whose size is $|\Par| = \sum_{i=1}^M{d_i d_{i-1}} + \sum_{j=1}^M{(d_j + d_{j-1})}$, which is $\sum_{i=1}^M{d_i d_{i-1}}$ parameters less then the non-constrained decoder. The optimization procedure is is done by gradient guided iterative algorithm, which is covered in the appendix section.

The optimization is computational intense, because the number of parameters $|\Par|$ is usually large. When the number of layers in the encoder -- decoders stacks is also large, the computation use to be inhibitively hard, because with the same number of allowed parameters, the complexity of the function represented by a network grows exponentially with the number of layers, that is, a deeper SA has more local minimum in the parameter space for the reconstruction loss $L$ to fall into. Yet, complex function also stands for high flexibility, which makes deeper network enormously intrigging, since a exponentially richer funtion space means a much better chance to find a network that could further reduce $L$ and at the same time produce even more compact abstraction. Deep artificial neuro networks have revived its popularity in recent years, thanks to the break through in its training procedure, which is now popularly dubbed ``deep learning''. For our method, we implement the layer-wise greedy pre-training procedure \cite{DL:DBN1, DL:SDA1}. The idea is to first train each encoder layer separately, then fine tune of the entire structure afterwards. To perform the layer-wise pre-training, the output of $i$ th. autoencoder $\zEC_i$ is not sent to the $i+1$ th. autoencoder like (\ref{eq:ED}) does, but is instead redirected to its decoder counterpart immediately to produce the intermidiate reconstruction $zDC_i$, the encoder--decoder tuple can then be easily trained by minimizing the local reconstruction loss $L_i=L(\zEC_{i-1}, \zDC{i-1})$. The training is easy, becase of the small number of parameters $\Par_i=\{\WEI{i}{i}{i-1}, \bEI{i}{i}, \bDI{i}{i-1}\}$. A total of $M$ such tuples is formed and trained separately like
\newcommand{\ED}[2]
{
  \arraycolsep=1.2pt
  \begin{array}{rcl}
    \zDC_{#1} &=& s(\WDC_{#2}\zDC_{#2} + \bDC_{#2}) \\
    \zDC_{#2} &=& \yHT_{#2} = \zEC_{#2} \\
    \zEC_{#2} &=& s(\WEC_{#2}\zEC_{#1} + \bEC_{#2}) \\
    \Par_{#2} &=& \min_{\Par_#2}L(\zEC_{#1},\zDC_{#1})
  \end{array}
}
\begin{equation}\label{eq:Greedy}
\begin{array}{cc}
  \ED{0}{1} \quad \ED{1}{2} \dots \ED{M-1}{M}
\end{array}
\end{equation}
What the greedy layer-wise pre-training has done is non-randomly initialize the entire structure to a state closer to optimum. After pre-training, all the encoders and the decoders are wired together like \ref{eq:ED} and fine tuned together. The comprehensive fine tuning will reach convergence much faster and less likely to fall into a poor local minimum then a direct training scheme without the pre-training. 

\subsection{Implementation}
In this suty, we will first formulate three similarity kernel function based on the three types of profiles available, which is the genotype in NGS format, the reconstructed cortical surface profile in vertex format, and the phenotype composed of diagnostic status and various demographics. The joint similarity U statistics will then be calculated and tested against the null hypotheis that no association exists between any two of the profiles.
Then, we will use all 806 cortical surface profiles to construct a 4 layered the stacked autoencoder (SA), The SA thus created will be used to abstrat the cortical surface profiles of 327 subject with definite diagnossis (either healthy control or Alzeimer's disease) into the condensed code which is 16 times smaller then the original vertex data. We will then use this condensed code to replace the original cortical surface profile and redo the joint similarity U statistic test.
We will use simulation study to compare the the power performance between the test using original vertex versus the one using encoded vertex. A boost in both computation speed and statistical power is expected, since the encoded cortical surface profile is smaller, and the abstracted high order feature is supposedly more informative than the original vertices which is tangled with trivial noises.
