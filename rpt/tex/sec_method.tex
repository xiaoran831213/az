\section{Method}

\subsection{Processing Imaging Profile with Stacked Autoencoder}
A Stacked Autoencoder (SA) is a type of artificial neural network (ANN) mimicking visual processing that abstracts high order features from the raw image. High order feature is more relevant to decision making. Taking our 3D cortex as an example, being able to recognize general location, size and shape of the laceration sites in the cortex, is far more helpful than knowing the exact thickness and curvature of every point in a person's cortex. Thus, by learning the high order features and ditching the trivial details, that is, the redundancies, we expect a power boost for subsequent association analysis if these features are allowed to replace the unprocessed imaing profile.

An stacked autoencoder (SA), as the name suggests, comprises two or more encoders stacked on top of each other. Using supersciptes to denote dimensionality, the the $i$ th.\ encoder in the stack looks like
\begin{equation} \label{eq:AE}
  \zEC_i^{d_i} = \boldsymbol{s}(\WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}),
\end{equation}
where $\zEI{i}{i}$ is the $i$ th.\ encoder's $d_i$ dimensional output, while $\zEI{i-1}{i-1}$ being its $d_{i-1}$ dimensional input, which is also the output from the encoder down below (the $i-1$ th.\ in the stack). The $j$ th.\ output entry is the sum of input entries weighted by the $j$ th.\ row vector of the weight matrix $\WEI{i}{i}{i-1}$, plus the $j$ th.\ element in the offset vector $\bEI{i}{i}$, then put through a entry-wise, non-linear transformation $\boldsymbol{s}(.)$. The popular choice of $\boldsymbol{s}$ is the inverse logit function that mimics neuron activation, so
\[z_{i,j} = \frac{\exp(\wEI{i,j}{i-1} \zEI{i-1}{i-1} + b_{i,j})}{1 + \exp(\wEI{i,j}{i-1} \zEI{i-1}{i-1} + b_{i,j})}, \quad (j=1 \dots, d_i). \]
To ensure that feature abstraction can actually happen (hence the word ``encode''), the output size $d_i$ is made smaller than the input size $d_{i-1}$. In our case, we let the encoders halve their input.

To assemble a stack of $M$ encoders which accepts size $P$ input vector $\xEC^P$ and produces size $Q$ high order feature vector $\hHT^Q$, we recursively wire the output of an encoder to the one above it, and ensure the input dimensionality at the bottom, $d_0$, equals to $P$, and the output dimensionality on the top, $d_M$, equals to $Q$. The encoder stack looks like
\begin{equation} \label{eq:SE}
  \begin{split}
    \hHT^Q &= \zEI{M}{M} \\
    \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M  }{M  }) \\
    \zEI{M-1}{M-1} &= s(\WEI{M-1}{M-1}{M-2} \zEI{M-2}{M-2} + \bEI{M-1}{M-1}) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{i  }{i  } &= s(\WEI{i  }{i  }{i-1} \zEI{i-1}{i-1} + \bEI{i  }{i  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{2  }{2  } &= s(\WEI{2  }{2  }{1  } \zEI{1  }{1  } + \bEI{2  }{2  }) \\
    \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1  }{1  }) \\
    \zEI{0  }{0  } &= \xEC^P.
  \end{split}
\end{equation}
Here, the $P$ dimensional input $\xEC^P$ can be viewed as the output of the non-existing $0$ th.\ encoder. By ensureing $P = d_0 > d_1 > d_2 \dots \ge d_{M-1} > d_M = Q$, the encoder stack abstracts $Q$ dimensional high order features from the $P$ dimensional raw profile.
The parameters in the stack, that is, the weights and offsets $\{\WEC_1, \bEC_1, \WEC_2, \bEC_2, \dots, \WEC_M, \bEC_M\}$ must be tuned to represent the body of knowledge that generated $\xEC^P$, which in our case is the knowledge of human cortex. To do so we first assemble a stacked decoder counterpart which exactly mirrors the topology and weights of the encoders, that is,
\begin{equation} \label{eq:SD}
  \begin{split}
    \xDC^P &= \zDI{0}{0} \\
    \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1  }{0  }) \\
    \zDI{1  }{1  } &= s(\WDI{2  }{1  }{2  } \zDI{2  }{2  } + \bDI{2  }{1  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zDI{i-1}{i-1} &= s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}) \\
    & \quad \quad \quad \quad \vdots \\
    \zDI{M-2}{M-2} &= s(\WDI{M-1}{M-2}{M-1} \zDI{M-1}{M-1} + \bDI{M-1}{M-2}) \\
    \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M  }{M-1}) \\
  \end{split}
\end{equation}
and by mirroring the weights, it means we force
\begin{align}
  \WDI{M-i}{M-i-1}{M-i} \equiv {(\WEI{M-i}{M-i}{M-i-1})}^\prime, \quad (i=0, \dots, M),
\end{align}
which is quiet instinctive since decoding is the opposite of encoding, though the offsets in the decoders, $\bDI{M-i}{M-i-1}, \quad (i=0, \dots, M)$, are still allowed to be flexible. From bottom to top, this stack of decoders gradually restores the details back to the abstracted state $\zDI{M}{M}$, and eventually presents a reconstructed input $\xDC^P$ on its top.

Next thing to do is to wire (\ref{eq:SE}) and (\ref{eq:SD}) together by making $\zDI{M}{M} = \hHT^Q$,
\begin{equation} \label{eq:SA}
  \begin{split}
    \xDC^P &= \zDI{0}{0} \\
    \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1}{0  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M}{M-1}) \\
    \zDI{M  }{M  } &= \hHT^Q = \zEI{M}{M} \\
    \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M}{M  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1}{1  }) \\
    \zEI{0  }{0  } &= \xEC^P,
  \end{split}
\end{equation}
which creates a stacked autoencoder (SA)~\cite{DL:SDA1}. The aforementioned weights and offsets in the encoder stack (lower half of the SA), alongside with $M$ extra offsets in the decoder stack (higher half of the SA), constitute the parameters to be calibrated in order to make the encoder stack best represent the process that generated $\xEC^P$. The calibration is done by minimize the discrepancy between the reconstructed input $\xDC^P$ and the true input $\xEC^P$. The rationale is that, if the compact code $\hHT$ presented by the encoder stack truly captures the major features of $\xEC^P$, the restored input, $\xDC^P$, should very much identical to the original one except some trivial details. Now, tuning the encoder stack is equivalent to solving the optimization problem
\begin{equation} \label{eq:CE}
  \begin{split}
    \Par^* = \min_{\Par} \sum_{k=1}^N{d(\xDC_k^P, \xEC_k^P)}, \quad \Par = \cup_{i=1}^M \{\WEC_i, \bEC_i, \bDC_i\},
  \end{split}
\end{equation}
where $k$ indices the $N$ training samples. The objective function $d$ measures the disagreement between the reconstructed and the original input, a popular form of which is cross-entropy
\begin{align} \label{eq:CE}
  d(\xDC_k^P, \xEC_k^P) = \sum_{j=1}^P[{x_{j,k}\log{\tilde{x}_{j,k}} + (1 - x_{j,k})\log{(1 - \tilde{x}_{j,k})}}],
\end{align}
where $j$ indices the $P$ entries of the input. 

The optimization of large number of parameters ($|\Par| = \sum_{i=1}^M{d_i d_{i-1} + d_i + d_{i-1}}$) is done by stochastic gradient descent (SGD)~\cite{SGD1, SGD2}, which is also called back propagation (BP) by neural network literature concerning the computation of high dimensional gradient $\PDV{d(\xDC_k^P, \xEC_k^P)}{\Par}$~\cite{BP1, BP2, BP3}. For the actual implementation, SGD and BP is programed using Theano~\cite{Theano1}, a computational differentiation library written in Python. 

To bypass shallow local minimum and slow converging caused by a multi-layer ANN, that is, the SA, we follow the recent ``deep learning'' trend by which a layer-wise greedy pre-training procedure is applied prior to fine-tuning the entire network~\cite{DL:DBN1, DL:SDA1}. To do so, output of the $i$ th.\ encoder, $\zEC_i$, is disconnected from the encoder above it and rewired to its decoder counterpart, immediately forming a single layered autoencoder, which is then calibrated by minimizing the intermediate reconstruction loss $d(\zEC_{i-1}, \zDC_{i-1})$, that is,
\begin{equation}\label{eq:Greedy}
  \begin{split}
    \zDC_{i-1} &= s(\WDC_i \zDC_i + \bDC_i) \\
    \zDC_{i  } &= \hHT_i = \zEC_i \quad \qquad \qquad \qquad (i = 0 \dots M) \\
    \zEC_{i  } &= s(\WEC_i\zEC_{i-1} + \bEC_i). \\
    \pEC_i^* &= \min_{\pEC_i}{d(\zEC_{i-1}, \zDC_{i-1})}, \qquad \pEC_i = \{\WEC_i, \bEC_i, \bDC_i\},
  \end{split}
\end{equation}
which is easy given the small number of parameters $|\pEC_i|=d_i d_{i-1} + d_i + d_{i-1}$. After all $M$ single-layer autoencoders are pre-trained, the encoders and decoders are wired back to (\ref{eq:SE}) and fine-tuned together, which should converge fast and much less likely to ``go down the wrong pit'' in the terrian of $d(\xDC_k^P, \xEC_k^P)$.

\subsection{Joint Test with Similarity U}
\newcommand{\vg}{\boldsymbol{g}}
\newcommand{\vv}{\boldsymbol{v}}
We use a similarity U statistic~\cite{UST1, UST2} to jointly test the existence of association among genomic, imaging and phenotype profiles. To derive the statistic, three kernel functions measuring pairwise similarity are chosen for each profile. The measurement $f$ can be flexible to suit the charisteristics of the profile (e.g.\ bounded or not, continuous or discrete), as long as $f$ is symmetric and finite in the second moment, that is, $f(x_i,x_j) \equiv f(x_j,x_i)$ and $E(f^2(x_i, x_j))<+\infty$.

For genomic profile coded by minor allele count 0, 1 or 2, we use the Identical By State (IBS) kernel
\label{eq:wSG}
\[ f_G(\vg_{i.}, \vg_{j.}) = \frac{\sum_{m=1}^{|G|}{w_m(2 - |g_{im} - g_{jm}|)}} {2\sum_{m=1}^{|vG|}{w_m}}, \]
where $(i,j)$ indices a pair of observations, and $m$ indices a genomic variant (i.e.\ a SNP) in the testing unit (i.e.\ a gene); $w_m$ is the weight assigned to the $m$ th.\ variant according to \textit{a prior} hypothesis, such as allele frequency (AF) based $w_m=\sqrt{AF(g_{.m})[1-AF(g_{.m})]}$ that emphasizes the effect of rare variants.

The imaging profile can either be the raw 3D cortical surface vertices or the high order features abstracted from them by an SA, for both we can use the Gaussian kernel which is better suited for continuous value,
\label{eq:wSV}
\[ f_V(v_{i.},v_{j.}) = \exp{ [-\frac{\sum_{m=1}^{|V|}{w_m{(u_{im}-u_{jm})}^2}} {\sum_{m=1}^{|V|}{w_m}}] }. \]
Here $m$ indices a imaging variant (i.e.\ a vertex), other notations means the same.

Lastly, for a phenotype profile generated by unknown distribution, it is first normalized with the rank normal quantile transformation
\[ q = \frac{\Phi^{-1}[rank(y - 0.5)]}{N}, \]
where $N$ is the number of observations. The phenotype similarity is also measured by a Gaussian kernel, simplied because the phenotype is one dimensional,
\[ f_Y(q_i, q_j) = \exp{[-{(q_i - q_j)}^2]}. \]

The similarity measure for a pair $(i, j)$ is centralized by substracting the two marginal mean measure involving either $i$ or $j$, then adding the grand mean~\cite{UST1}:
\begin{align*}
  \tilde{f}(*_i, *_j) = f(*_i, *_j)-\frac{1}{N} \sum_{k=1}^N{f(*_i, *_k)}-\frac{1}{N}\sum_{l=1}^N{f(*_l, *_j)} + \frac{1}{N^2} \sum_{k=1}^{N} \sum_{l=1}^{N} {f(*_l, *_k)}.
\end{align*}
Finally, the joint similarity U statistics $U_J$ is the mean of entry-wise product of centralized measurements excluding the self-pairs, which is
\[ U_J = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_G(\vg_{i.}, \vg_{j.}) \circ \tilde{f}_V(\vv_{i.}, \vv_{j.}) \circ \tilde{f}_Y(y_i, y_j), \]
where $\circ$ means entry-wise product. When there is no association among any of the profiles, $U_J$ should be close to $0$ since all the measurements have mean $0$. Conversely, if $U_J$ deviates from $0$ significantly, it means the similarity regarding one profile is correlated to that regarding other profiles, implying the presence of association. Formally, under $H_0^J$: there is no association between any of the profiles, $U_J$ follows a $\chi_1^2$ mixture weighted by the squared engien values of $\tilde{f}_G(\vg_{i.}, \vg_{j.}) \circ \tilde{f}_V(\vv_{i.}, \vv_{j.}) \circ \tilde{f}_Y(y_i, y_j)$~\cite{UST1, UST2}. The p-value is calculated using Davis method~\cite{davies80}.

Alternatively, two simpler tests can be done by dropping either the imaging or genomic kernal. Thus,
\[ U_G = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_G(\vg_{i.}, \vg_{j.}) \circ \tilde{f}_Y(y_i, y_j) \]
tests $H_0^G$: there is no association between the phenotype and genomic profiles. And,
\[ U_V = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_V(\vv_{i.}, \vv_{j.}) \circ \tilde{f}_Y(y_i, y_j) \]
tests $H_0^V$: there is no association between the phenotype and imaging profiles. These two tests are more specific thus may produce more parsimonious models, but also risk mis-specification, which is to be examed by simulation study.

The imaging similarity by design is an aggregation of signals of all vertices in the testing unit. For comparison purpose, we also implement the vertex-wise analysis (VWA). Briefly speaking, we first smooth the imaging profile with a Gaussian filter of standard deviation 2, which reduces the noise by grinding away trivial details in the 3D cortex. Next, we treat the orginal $|V|$ dimensional testing unit as $|V|$ one dimensional profiles, and perform that many times of $U_J$ or $U_V$ tests. The resulting $|V|$ p-values are corrected by FDR (false discovery rate) against multiple testing. If any of the corrected p-values is below the $0.05$ threshold, the entire testing unit of $|V|$ vertices is declared statistically significant.
