\section{Method}

To simoteniously detect association among genomics, cortical surface and phenotype profiles, a generalized multivariate similarity U statistic was used, postulated by Changshuai et. al. \cite{HWU}. Of all three types profile, the cortical surface has the most numerous variants, with 68 anatomical regions comprised of few hundreds to more than ten thousands of vertices. To deal with the dimensionality issue, a 4 layered Stacked Autoencoder(SA) was trained with a greed layer by layer manner, as suggested by the deep learning literatures. The SA is capable of abstracting high order features from the vertices in the cortical surface, and at the same time reducing its dimenstionality by 16 fold. 
The vertex similarity can then be calculated with the code instead of the orignial. Supposingly, any U statistic involving a vertex similarity weight should enjoy a power boost if the raw verties were to be replaced with the code, because high order features are less suscptable to random noise. to calculate will be cal by piping the encoded vertices can then by derived by summerizing the product of pairwise similarity between subjects, with respect to the aforementioned genetic and phenotipic profiles, and the encoded vertices. 

\subsection{Similarity U Test}
To derive the proposed U statistic, one U-kernel and one or more U-weights must be constructed. A kernal/weight measures the similarity of each pair of samples, with respect to one of the profiles. The measurement funciton can be flexable depending on the distribution of the profile and the hypothesis in mind, as long as it is symmetric and has finite second moment. Thus, function $f$ is a valid U kernal/weight if $f(x_i,x_j)=f(x_j,x_i)$ and $E(f^2(X_1, X_2))<+\infty$ are satisfied. For the three types of profile, three pairwise similarity measurement were chosen according to common practice.

For biallelic genetic variants whose value was taken from minor allele count ${0, 1, 2}$, a common choice is the weighted complement of Manhattan Distance (wMD):
\begin{equation} \label{eq:wSG}
\begin{split}
  S_{ij}^G &= wMD(g_{i.},g_{j.}) \\
  &= \frac{\sum_{m=1}^{|G|}{w_m(2-|g_{im}-g_{jm}|)}} {2\sum_{m=1}^{|G|}{w_m}},
\end{split}
\end{equation}
, which is also called identical by state (IBS) kernel. Here $g_{im}$($g_{jm}$) is the value of the $m th.$ variant (e.g. a SNP, indel, or deletion) in a testing unit (e.g. a gene) for the $i th.$($j th.$) sample; $|G|$ is the dimensionality of the genomic testing unit (e.g. total number of SNPs, indels and deletions in a gene). $w_m$ is the weight assigned to the $m th.$ variant depending on \textit{a prior} hypothesis, one such example is the reversed square root of minor allele frequency which places more emphasize on rarer variants:
\begin{displaymath}w_m=\frac{1}{\sqrt{MAF(g_m)(1-MAF(g_m))}}.\end{displaymath}
Without any \textit{a prior} hypothesis of the relative importance of genomic variants, an unweighted complement of Manhatten Distance (uMD) can be used by forcing $w_m \equiv 1$:
\begin{equation*} \label{eq:uSG}
\begin{split}
  S_{ij}^G &= uMD(g_{i.}, g_{j.}) \\
  &= \frac{\sum_{m = 1}^{|G|}{(2-|g_{im} - g_{jm}|)}}  {2|G|}.
\end{split}
\end{equation*}

For vertices in the reconstructed cortical surface, their values, for now, is taken from white matter thickness rescaled to $[0,1]$, one could base the similarity measurement on weighted Euclidian Distance (wED):
\begin{equation} \label{eq:wSV}
\begin{split}
  S_{ij}^V &= wED(v_{i.},v_{j.}) \\
  &= \exp
  {
    \Big[-\frac{\sum_{m=1}^{|V|}{w_m(u_{im}-u_{jm})^2}} {\sum_{m=1}^{|V|}{w_m}}\Big]
  },
\end{split}
\end{equation}
, which is also called a Gaussian kernel. Here $v_{im}$($v_{jm}$) is the value of the $m$ th. vertex of the cortical surface testing unit in the $i$($j$) th. sample, while $|V|$ is the number of vertices in it that testing unit, that is, the dimensionality. $w_m$ is the weight assigned to the $m$ th. vertex in the testing unit. Without pre-knowledge regarding the relative importance of the vertices, the similarity measurement is simplifed by equal weighting ($w_m \equiv 1$):
\begin{equation*} \label{eq_uSV}
\begin{split}
  S_{ij}^V &= uED(v_{i.},v_{j.}) \\
  &= \exp
  {
    \Big[-\frac{\sum_{m=1}^{|V|}{(u_{im}-u_{jm})^2}} {|V|}\Big]
  }.
\end{split}
\end{equation*}
Lastly, for a multivariate phenotype profile, first a rank normal quantile normalization were a
pplied to all the components according to \cite{HWU}:
\begin{displaymath}
  q_{im}=\Phi^{-1}[(rank(y_{im})-0.5)/|Y|],
\end{displaymath} 
where $y_{im}$ is the value of the $m th.$ component of the phenotype for the $i th.$ sample, and $|Y|$ is the dimensionality of the phenotype (i.e. number of components). Doing so not only correct skewed components, but also remove the complication of admixed distribution type introduced by a multivariate phenotype. As a result, the phenotype based similarity can be measured in a manner similar to that of cortical surface:
\begin{equation} \label{eq_wSY}
\begin{split}
  S_{ij}^Y &= wED(q_{i.},q_{j.}) \\
  &=\exp
  {
    \Big[-\frac{\sum_{m=1}^{|Y|}{w_m(q_{im}-q_{jm})^2}} {\sum_{m=1}^{|Y|}{w_m}}\Big]
  },
\end{split}
\end{equation}
where $q_{im}$ is the values of the $m th.$ component of the normalized phenotype; the weight $w_m$ is the ralative importance of that dimension. For a case control study of one dimensional phenotype, that is, $|Y|=1$, the similarity measurement simplifys to:
\begin{displaymath}
  S_{ij}^{y}=wED(q_i,q_j)=\exp{[-(q_i-q_j)^2]},
\end{displaymath}
All three types of the similarity are centralized by substracting the raw measurement from every pair $(i,j)$ with two sample means of all measurements involving $i$ and $j$, respectively, then adding the sample mean of all the pairs to it \cite{HWU}. Takeing the weighted genetic similarity as an example, the centralized similarity measurement is:
\begin{equation} \label{eq_wSY}
\begin{split}
  \tilde{S}_{ij}^{G} &= S_{ij}^{G}-E(S_{i.}^{G})-E(S_{.j}^{G})+E(S_{..}^{G}) \\
  &= S_{ij}^{G} - \frac{1}{N}\sum_{k=1}^N{S_{ik}} - \frac{1}{N}\sum_{l=1}^N{S_{lj}} + \frac{1}{N^2}\sum_{k=1,l=1}^{N,N}{S_{kl}},
\end{split}
\end{equation}
where $N$ is number of samples. The same centralization scheme is applied to the other two similarity measurements. The U statistics is the mean of the product of three centralized similarity measurement across all but mirrored pairs:
\begin{displaymath}
  U^{GVY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{G} \tilde{S}_{ij}^{V} \tilde{S}_{ij}^{Y}.
\end{displaymath}
The actual implementation calculated three symmetric $N \times N$ similarity matrices to facilitate later manipulation. The U statistic follows a mixture of $\chi_1^2$ distrubtion, a $p$ value can then be calculated using Davis method \cite{HWU}.

For now, the genomic and vertex similarity are treated U-weight terms, while the phenotype similarity acts as the U-kernel term. In general, the assignment of U-kernal and U-weight terms does not alter the limiting distribution of the final U statistics. More specific association test can be perform by dropping one of the U-weight terms. To see if only the phenotype and genomic profiles are associated, construct the U statistics without vertex similarity term:
\begin{displaymath}
  U^{GY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{G} \tilde{S}_{ij}^{Y}.
\end{displaymath}
Likewise, dropping the genomic similarity to test if only the phenotype and genomic profiles are uncorrelated:
\begin{displaymath}
  U^{VY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{V} \tilde{S}_{ij}^{Y}.
\end{displaymath}

\subsection{Stacked Autoencoder}
The purpose of an encoder is to abstract high order features from the raw input which is meaningful for decision making. A real life example is vewing a painting: what intrests the viewer is the concept of object or event the painting captures, not the exact colouring at every inch of the canvas. As for the cortical surface profile, being able to see the overall shrinkage and laceration sites in a aged brain, even without the exact volumn, location, and boundery of these sites, is far more important than knowning the exact thickness, curvature and coordinates at every vertex. An extreamly accurate and objective measurement at every unit of the cortical surface is, though highly informative, remotely helpful for diagnostic purpose without abstraction of high order feature from it. The Stacked Autoencoder (SA) to be implemented here is the result of recent development of deep artificial neuro-network, originally inspired by the sentinental processing of visual information. By definition, an abstraction must reduce the complexity of the raw image  considerablly so the host could possibly process and make a sound accessment. The SA will take the high dimensional cortical surface and output abstracted features of much lower dimensionality.

An SA is formed by layers of autoencoders stacked from bottom to top, hence "Stacked". An autoencoder represents a linear recombination of the input elements, followed by an element-wise non-linear transformatio. Usually, the output has a lower dimensionality then the input to ensure abstraction and dimension reduction. In symbolic form, an autoencoder at the $i$ th. layer of an SA can be written as:
\begin{equation} \label{eq:AE}
  \begin{split}
    \zEC_i^{d_i}   &= s(\WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}) \\
  \end{split}
\end{equation},
where $\zEI{i}{i}$ is the output and $\zEI{i-1}{i-1}$ is the input, which is also the output from the layer down below -- the $i-1$ th. autoencoder in the stack. Linear recombination of the input is achieved by the corss product between the input and the weight matrix $\WEI{i}{i}{i-1}$, followed by adding an offset of $\bEI{i}{i}$. The superscript $d_{i-1}$ and $d_i$ denote the dimensionality of both data and parameters. As mentioned before, to ensure that feature abstraction and dimension reduction actually happens, in most cases, $d_i$ is chosen to be smaller than $d_{i-1}$. As for the elementwise non-linear transformation, $s$, the most popular choice is inverse logit function:
\begin{equation} \label{eq:InvLgt}
  \begin{split}
    s(\etaEC_i^{d_i})     &= [logit^{-1}(\eta_{i1}), logit^{-1}(\eta_{i2}), \dots, logit^{-1}(\eta_{d_{id_i}})]^{\prime} \\
    \etaEC_i^{d_i}        &= \WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i} \\
    logit^{-1}(\eta_{ik}) &= \frac{1}{1+e^{-\eta_{ik}}}, \quad k=(1,2,\dots,d_i) \\
    \eta_{ik}             &= \wEC_{ik}^{d_{i-1}} \zEI{i-1}{i-1} + b_{ik}.
  \end{split}
\end{equation}
Here $\etaEC_i^{d_i}$ is the result of linear recombination of input $\zEI{i-1}{i-1}$, with the super script $d_i$ denoting its dimensionality and $k = 1, \dots, d_i$ indexing its $d_i$ elements. The "S" shaped inverse logit curve resembles the biological activation of the $k$ th. neuron in the $i$ th. layer when the sum of input from all $d_{i-1}$ neurons in the previous layer, $\zEI{i-1}{i-1}$, weighted by the $k$ th. row of $WEI{i}{i-1}{i}$, $\wEC_{ik}^{d_{i-1}}$, exceeds threshold $-b_{ik}$ which is the negation of $k$ th. elements in the offest vector $\bEI{i}{i}$. The actual biological activation of a neuron is a step function of the total input, which is steeper than the inverse logit, but the later, being smooth and differentiabe throughout the real line, will facilitate gradient gudided learning procedure in the future.

An SA of $M$ layers, $P$ dimensional raw input $\boldsymbol{x}$ and $Q$ dimensional output, can be assembled by recursively pluging the output of an autoencoder into the one above, and ensuring the output of the top layer is $Q$ dimensinal:
\begin{equation} \label{eq:ES}
  \begin{split}
    \yHT^Q &= \zEI{M}{M} \\
    \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M  }{M  }) \\
    \zEI{M-1}{M-1} &= s(\WEI{M-1}{M-1}{M-2} \zEI{M-2}{M-2} + \bEI{M-1}{M-1}) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{i  }{i  } &= s(\WEI{i  }{i  }{i-1} \zEI{i-1}{i-1} + \bEI{i  }{i  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{2  }{2  } &= s(\WEI{2  }{2  }{1  } \zEI{1  }{1  } + \bEI{2  }{2  }) \\
    \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1  }{1  }) \\
    \zEI{0  }{0  } &= \xEC^P,
  \end{split}
\end{equation}
where $\xEC^P$ and $\boldsymbol{\hat{y}}^Q$ denote the $P$ dimensional raw input, and the $Q$ dimensional output, repectively. The vector $\zEC_i^{d_i}$ is the intermidiate output of the $i$ th. autoencoder. Here we use superscripts to display dimensionalities. The $P$ dimensional input $\xEC^P$ is viewed as output of the non-existing $0$ th. autoencoder, denoted by $\zEC_0^{d_0}$, where $d_0=P$. From the bottom to up, the SA will gradually abstract higher order features from the $P$ dimensional input $\xEC^P$, until the dimensionality of the output is as low as $d_M=Q$.

The SA just constructed is useless without calibration. One must find a set of parameters $\pEC=\{\WEC_1, \bEC_1, \WEC_2, \bEC_2, \dots, \WEC_M, \bEC_M\} = \pEC_1 \cup \pEC_2, \dots, \pEC_M$ that truely represents body of knowlege regarding the observed data $\xEC$, only then it would be capable of abstracting meaningful features instead of haphazardly reducing a high dimensional input into a small but irrelevent output (e.g. a vector of $Q$ random numbers). The "capability" of the SA can be gauged by two different criteria. For one, it is reasonable to say an abstraction should help to better predict the outcome of interest. Turning strategies based on this criteria belong to the so called supervised machine learning. Another criteria, saying if an abstraction is indeed done better, we should be able to be reconstructed the original input from it more accurately, will resulting in the so called unsupervised machine learning. Here ``supervised'' means the ``goodness of abstraction'' is judged by asking how accurate the predictions were made using features extracted by the SA from the raw input, in comparison with the known external fact from a specific domain of problems. The SA thus learned will try to extract features better suited for problems from that domain (e.g. tell the future risk of neuralogical disorder by examing the cortical surface). The ``unsupervised'' learning, on the other hand, judges the ``goodness of abstraction'' by having the raw input comparing with its reconstructed self from the extracted features. An SA tuned in this manner will try to extract more general features not necessarily specific to a certain problem domain. In this work, the later, unsupervised learning is to be used to explore generic knowledge regarding the human cortical surface, not just for the purpose of diagnosing a certain type of neuralogical disorder (e.g. Alzheimer's Disease). As a matter of fact, all available T1 MRI samples contrubute to the ``machines's understanding of human brain'', even if more than half of them cannot enter the association analysis because of non-definite diagnosis.

Prior to unsupervised learning however, one has to come up with a reconstruction scheme -- a decoder counterpart of the SA. The most nature way is to mirror the structure of the SA, that is, the number of layers in the decoder is the same with the SA, each layer is also a linear recombination of its input followed by a element-wise non-linear transformation, but the dimensionality changes are exactly in reverse order of the SA. In symbolic form, the decoder can be structured in a similar manner:
\begin{equation} \label{eq:DS}
\begin{split}
  \xDC^P &= \zDI{0}{0} \\
  \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1  }{0  }) \\
  \zDI{1  }{1  } &= s(\WDI{2  }{1  }{2  } \zDI{2  }{2  } + \bDI{2  }{1  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{i-1}{i-1} &= s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{M-2}{M-2} &= s(\WDI{M-1}{M-2}{M-1} \zDI{M-1}{M-1} + \bDI{M-1}{M-2}) \\
  \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M  }{M-1}) \\
  \zDI{M  }{M  } &= \yHT^Q .
\end{split}
\end{equation}
From bottom to up, the decoder gradually restores details from the abstracted features $\yHT^Q$, and eventually reaches a reconstruction of the raw input $\xDC^P$. The encoder \ref{eq:ES} is linked from below by plugin its output into the bottom of the decoder through ${\boldsymbol{\hat{y}}}$. The entire encoding -- decoding structure can be written as:
\begin{equation} \label{eq:ED}
\begin{split}
  \xDC^P &= \zDI{0}{0} \\
  \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1}{0  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M}{M-1}) \\
  \zDI{M  }{M  } &= \yHT = \zEI{M}{M} \\
  \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M}{M  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1}{1  }) \\
  \zEI{0  }{0  } &= \xEC^P.
\end{split}
\end{equation}pppp
The discrepancy between $\xDC$ and $\xEC$ is a measure of how badly the reconstructed input resemble the original, which also tells us how poorly the encoder had performed, because a truely meaningful abstraction should not obstruct the recovery of details. Therefore, the learning of the SA is the process of minimizing the disagreement between $\xDC$ and $\xEC$ by fine tuning the structuring parameters in the entire autoencoder and decoder stack, which is:
\begin{equation} \label{eq:PAR}
\begin{split}
  \Par &= \pEC \cup \pDC = \{\WEC_1, \bEC_1, \dots, \WEC_M, \bEC_M \} \cup \{\WDC_1, \bDC_1, \dots, \WDC_M, \bDC_M\}
\end{split}
\end{equation}
The measurement of the disagreement between $\xEC^P$ and $\xDC^P$ is also called the loss function or reconstruction loss, the current most popular loss is cross entrophy, which has the following form:
\begin{equation} \label{eq:CE}
\begin{split}
  l_j(\xDC_j^P, \xEC_j^P) &= -\frac{1}{P}\sum_{k=1}^P[x_{jk}\log{\tilde{x}_{jk}}+(1-x_{jk})\log(1-\tilde{x}_{jk})] \\
  L(\XDC, \XEC) &= \sum_{j=1}^{N}{l(\xDC_j^P, \xEC_j^P)}
\end{split}
\end{equation}.
$L(\XDC, \XEC)$ measures the total reconstruction loss of the entire study of $N$ profiles, with two $N \times P$ matrices $\XDC$ and $\XEC$ representing the reconstructed and the original version, respectively. $l_j(\xDC_j^P, \xEC_j^P)$ measures the loss contributed by the $j$ th. sample, with $\xDC_j^P$ and $\xEC_j^P$ being the $j$ th. row vectors from $\XDC$ and $\XEC$, respectively. For each profile, $k$ indexes one of the $P$ demensions. The total reconstruction loss $L$ is monotonic with the poorness of the performance of SA, thus the search for the best set of parameter is expressed as a numerical optimization problem:
\[ \boldsymbol{\Theta} = \min_{\boldsymbol{\Theta}} L(\XDC, \XEC) \].
The optimization is done by gradient decent. Starting with a randomly initialized assignment of $\Theta^t$ at $t=0$, and compute the next assignment $\Theta^{t+1}$ by substracting from the current assignment $\Theta^t$ a small fraction of the gradient of reconstruction loss $L$ with respect to the current assignment ($\frac{\partial L}{\partial \boldsymbol{\Theta^{t}}}$). The small fraction is called a learning step, if the step is reasonablly small, the reconstruction loss $L$ will keep dropping. The learning will repeat until $L$ cease to drop. The final assignment is considered the optimal $\Theta$. 

The gradient $\PDV{L}{\Par^t}$ is calculated by a process called backward propagation [?], which relies heavily on the chain rule of derivatives. First, realize the total loss $L(\XDC, \XEC)$ is a summation of  of individual loss $l_j(\xDC_j^P, \xEC_j^P)$ for $j=1, \dots, N$, and $l_j(\xDC_j^P, \xEC_j^P)$ in turn is a function of $\xDC_j^P$ (the observed $\xEC_j^P$ is a constant vector), at last, $\xDC_j^P$ is a function of all structure parameters $\Par$, using the additive rule and chain rule we get:
\begin{equation*}
\begin{split}
  \PDV{L(\XDC, \XEC)}{\Par} &= \sum_{j=1}^{N}\PDV{l_j(\xDC_j^P, \xEC_j^P)}{\Par} \\
  \PDV{l_j(\xDC_j^P, \xEC_j^P)}{\Par} &= \PDV{l_j(\xDC_j^P, \xEC_j^P)}{\xDC_j^P} \PDV{\xDC_j^P}{\Par} \\
  \PDV{l_j(\xDC_j^P, \xEC_j^P)}{\xDC_j^P} &= \xEC_j^P \oslash \xDC_j^P -  (\one - \xEC_j^P) \oslash (\one - \xDC_j^P) \\
  \PDV{\xDC_j^P}{\Par} &= \PDV{\zDC_{0j}^{d_0}}{\Par}.
\end{split}
\end{equation*}
Here we use $\oslash$ to denote element-wise division. From now on we would focus on the $j$ th. sample and ommit $j$ from the subscript, since the top output $\xDC_j^P = \zDI{0j}{0}$ is constructed in exactly the same manner across all $N$ individuals. Recall the symbolic form of SA and the cooresponding decoders on its top (see \ref{eq:ES}, \ref{eq:DS} and \ref{eq:ED}), the output of any layer, either from an encoder or a decoder, is a function of its own structure parameters and the direct input supplied by the lower layer. The gradient of the output with respect to the paremeters and input can be calculated directly,
\begin{equation}\label{eq:GD}
  \begin{split}\
    \begin{array}{rl}
      \textrm{the i th. decoder:} & \begin{array}{rcl}
        \PDV{\zDI{i-1}{i-1}}{\pDC_i} & = & \diag{\zDI{i-1}{i-1}} \diag{\one{} - \zDI{i-1}{i-1}} [\zDIt{i}{i}, 1] \\
        \PDV{\zDI{i-1}{i-1}}{\zDI{i}{i}} & = & \diag{\zDI{i-1}{i-1}} \diag{\one{} - \zDI{i-1}{i-1}} \WDI{i}{i-1}{i} \\
      \end{array} \\ \\
      \textrm{the i th. encoder:} & \begin{array}{rcl}
        \PDV{\zEI{i}{i}}{\pEC_i} & = & \diag{\zEI{i}{i}}\diag{\one - \zEI{i}{i}} [\zEIt{i-1}{i-1}, 1] \\
        \PDV{\zEI{i}{i}}{\zEI{i-1}{i-1}} & = &  \diag{\zEI{i}{i}} \diag{\one - \zEI{i}{i}} \WEI{i}{i}{i-1}\\
      \end{array}
    \end{array}
  \end{split}
\end{equation}
Here $\diag{v}$ means creating a $0$ matrix and asign the vector $v$ to its diagonal. Since the input of a layer is essencially the output of the layer down below, who also has its own structure paremeters and input from the layer even lower, the gradient of the top output, $\zDC_0$, with respect to any lower layers' parameters, can be calculated by recursively invoking the chain rule, that is,
\begin{equation}\label{eq:BP}
  \newcommand{\PDT}[3]{\PDV{#1}{#2}\PDV{#2}{#3}}
  \newcommand{\CHN}[3]{\PDV{#1}{#3} & = & \PDT{#1}{#2}{#3}}
  \newcommand{\SEP}{\quad \quad}
  \arraycolsep=1.4pt\def\arraystretch{1.4}
  \begin{array}{rclcrcl}
    \CHN{\zDC_0}{\zDC_{1  }}{\pDC_{2  }} & \SEP & \CHN{\zDC_0}{\zDC_{1  }}{\zDC_{2  }} \\
    \CHN{\zDC_0}{\zDC_{2  }}{\pDC_{3  }} & \SEP & \CHN{\zDC_0}{\zDC_{2  }}{\zDC_{3  }} \\
    & \vdots & & \SEP & & \vdots & \\
    \CHN{\zDC_0}{\zDC_{M-2}}{\pDC_{M-1}} & \SEP & \CHN{\zDC_0}{\zDC_{M-2}}{\zDC_{M-1}} \\
    \CHN{\zDC_0}{\zDC_{M-1}}{\pDC_{M  }} & \SEP & \CHN{\zDC_0}{\zDC_{M-1}}{\zDC_{M  }} \\
    & & & & \PDV{\zDC_0}{\zEC_{M  }} & = & \PDV{\zDC_0}{\yHT} = \PDV{\zDC_0}{\zDC_M} \\
    \CHN{\zDC_0}{\zEC_{M  }}{\pEC_{M  }} & \SEP & \CHN{\zDC_0}{\zEC_{M  }}{\zEC_{M-1}} \\
    \CHN{\zDC_0}{\zEC_{M-1}}{\pEC_{M-1}} & \SEP & \CHN{\zDC_0}{\zEC_{M-1}}{\zEC_{M-2}} \\
    & \vdots & & \SEP & & \vdots & \\
    \CHN{\zDC_0}{\zEC_{2  }}{\pEC_{2  }} & \SEP & \CHN{\zDC_0}{\zEC_{2  }}{\zEC_{1  }} \\
    \CHN{\zDC_0}{\zEC_{1  }}{\pEC_{1  }} & \SEP & & &
  \end{array}
\end{equation}
In reverse to the "bottom to top" encoding and decoding procedure, the gradient is propagated from top to bottom, hince the name "backward propagation". Taking the $M$ layered SA and its decoder counterpart together, the total number of parameters to be calibrated is $|\Par| = |\pEC| + |\pDC| = \sum_{i=1}^{M}{(d_i + 1)d_{i-1}} + \sum_{j=M}^{1}{(d_{j-1} + 1)d_j}$. The optimization can be computationally intense because this number is usually huge. One commonly applied strategy for learning an SA is to constrain the weight matrix in the decoder layers to be the transpose of their counterpart in the SA, that is, forcing $\WDI{i}{i-1}{i} \equiv \WEIt{i}{i}{i-1}$ for $i=1 \ldots M$, and on top of the assignment of gradient layed out in \ref{eq:GD} and \ref{eq:BP}, the gradient of the top output $\zDI{0}{0}$ with respect to the weight matrix of any decoder will be absorded by the one in its encoder counterpart, that is, 
\begin{equation*}
  \begin{split}\
    \PDV{\zDC_0}{\WEC_i}^* &= \PDV{\zDC_0}{\WEC_i} + (\PDV{\zDC_0}{\WEC_i})^\prime.
  \end{split}
\end{equation*}
Doing so introduces slightly more computation for each learning step, but at the same time almost halve the number of tuning parameters to $\sum_{i=1}^{M}{(d_{i-1}) \times (d_i - 1)} + \sum_{i=M}^{1}{d_i}$, greatly speed up the convergence of $L(\XDC, \XEC)$.  Beside, the whole structure fits the common sense that, encoding and decoding are essentially symmetric operations. More importantly, the constraint encourages learning of an optimal SA instead of a sub-optimal SA coupled with a powerful decoder on its top, afterall, our best interest is the high order feature abstracted from the raw input, not its reconstruction.

Despite the benifit of constraint on decoder weight matrices, it is still powerless when the depth of the SA grows large. A deeper network is consideribaly harder to learn because the complexity of the function represented by the network grows exponentially alongside with the number of layers while holding the total number of parameters constant. In fact, a network with with more intermediate layers leaves more local minimum in the parameter space for the reconstruction loss $L$ to fall into. Yet, for the same reason, deeper network is enormously intrigging, because a exponentially richer funtion space means a much better chance to find a network that further reduce $L$, which in tune implies better abstraction of knowledge. Deep artificial neuro networks have gained popularity in recent years, thanks to the break through in machining leaning, or, the introduction of ``deep learning''. In this study we implement the layer-wise greedy pre-training procedure postulation made by Hilton et.al.\cite{DL:Greedy}. The idea is to first learn each encoder layer separately, then fine tune of the entire structure afterwards. During the pre-training, the output of $i$ th. autoencoder $\zEC_i$ is not sent to the $i+1$ th., but instead redirected to its decoder counterpart for an immediate reconstruction, creating a encoder--decoder tuple whose small number of parameters $\Par_i=\{\pEC_i, \pDC_i\}$ can be easily learned by minimizing the local reconstruction loss $L_i=L(\zEC_{i-1}, \zDC{i-1})$. A total of $M$ such tuples is constructed and learned, they can be written as:
\newcommand{\ED}[2]
{
  \arraycolsep=1.2pt
  \begin{array}{rcl}
    \zDC_{#1} &=& s(\WDC_{#2}\zDC_{#2} + \bDC_{#2}) \\
    \zDC_{#2} &=& \yHT_{#2} = \zEC_{#2} \\
    \zEC_{#2} &=& s(\WEC_{#2}\zEC_{#1} + \bEC_{#2}) \\
    \Par_{#2} &=& \min_{\Par_#2}L(\zEC_{#1},\zDC_{#1})
  \end{array}
}
\begin{equation}\label{eq:Greedy}
\begin{array}{cc}
  \ED{0}{1} \quad \ED{1}{2} \dots \ED{M-1}{M}
\end{array}
\end{equation}
What the greedy layer-wise pre-training has achieved is a non-randomly initialization of the entire structure to a state already close to optimum. After pre-training, all the encoders and the decoders are wired together and all paremeters is fine tuned together. The optimization procedure will reach convergence much faster and less likely to fall into a poor local minimum then a direct learning scheme without the pre-training. The pre-training cannot be parallelized because the higher tuples relies on the encoding output of lower ones, yet the extra computation is easily compensated by the much improved performace of the fining tuning stage which optimize all parameters together.

All 808 cortical surface profiles will be used to construct the stacked autoencoder (SA), even if diagnosis status being definite or unsure (e.g. intermediate status between MCI and AD). The SA thus create would encampus general knowledge of human cortical surface not limited to diagnostic purpose. We then pipe the profile of 327 subject with definite diagnossis (healthy control or Alzeimer's disease) throught the SA to obtain abstrated image code. This code, can then replace the original vertex data for the evaluation of pairwise similarity, which is one of the weight components of the aforementioned U statistics.
