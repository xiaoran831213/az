\section{Method}

To simoteniously detect association among genomics, cortical surface and phenotype profiles, a generalized multivariate similarity U statistic was used, postulated by Changshuai et. al. \cite{HWU}. Of all three types profile, the cortical surface has the most numerous variants, with 68 anatomical regions comprised of few hundreds to more than ten thousands of vertices. To deal with the dimensionality issue, a 4 layered Stacked Autoencoder(SA) was trained with a greed layer by layer manner, as suggested by the deep learning literatures. The SA is capable of abstracting high order features from the vertices in the cortical surface, and at the same time reducing its dimenstionality by 16 fold. 
The vertex similarity can then be calculated with the code instead of the orignial. Supposingly, any U statistic involving a vertex similarity weight should enjoy a power boost if the raw verties were to be replaced with the code, because high order features are less suscptable to random noise. to calculate will be cal by piping the encoded vertices can then by derived by summerizing the product of pairwise similarity between subjects, with respect to the aforementioned genetic and phenotipic profiles, and the encoded vertices. 

\subsection{Similarity U Test}
To derive the proposed U statistic, one U-kernel and one or more U-weights must be constructed. A kernal/weight measures the similarity of each pair of samples, with respect to one of the profiles. The measurement funciton can be flexable depending on the distribution of the profile and the hypothesis in mind, as long as it is symmetric and has finite second moment. Thus, function $f$ is a valid U kernal/weight if $f(x_i,x_j)=f(x_j,x_i)$ and $E(f^2(X_1, X_2))<+\infty$ are satisfied. For the three types of profile, three pairwise similarity measurement were chosen according to common practice.

For biallelic genetic variants whose value was taken from minor allele count ${0, 1, 2}$, a common choice is the weighted complement of Manhattan Distance (wMD):
\begin{equation} \label{eq:wSG}
\begin{split}
  S_{ij}^G &= wMD(g_{i.},g_{j.}) \\
  &= \frac{\sum_{m=1}^{|G|}{w_m(2-|g_{im}-g_{jm}|)}} {2\sum_{m=1}^{|G|}{w_m}},
\end{split}
\end{equation}
, which is also called identical by state (IBS), measurement. Here $g_{im}$($g_{jm}$) is the value of the $m th.$ variant (e.g. a SNP, indel, or deletion) in a testing unit (e.g. a gene) for the $i th.$($j th.$) sample; $|G|$ is the dimensionality of the genomic testing unit (e.g. total number of SNPs, indels and deletions in a gene). $w_m$ is the weight assigned to the $m th.$ variant depending on \textit{a prior} hypothesis, one such example is the reversed square root of minor allele frequency which places more emphasize on rarer variants:
\begin{displaymath}w_m=\frac{1}{\sqrt{MAF(g_m)(1-MAF(g_m))}}.\end{displaymath}
Without any \textit{a prior} hypothesis of the relative importance of genomic variants, an unweighted complement of Manhatten Distance (uMD) can be used by forcing $w_m \equiv 1$:
\begin{equation*} \label{eq:uSG}
\begin{split}
  S_{ij}^G &= uMD(g_{i.}, g_{j.}) \\
  &= \frac{\sum_{m = 1}^{|G|}{(2-|g_{im} - g_{jm}|)}}  {2|G|}.
\end{split}
\end{equation*}

For vertices in the reconstructed cortical surface, their values, for now, is taken from  rescaled $[0,1]$ white matter thickness, one could base the similarity measurement on weighted Euclidian Distance (wED):
\begin{equation} \label{eq:wSV}
\begin{split}
  S_{ij}^V &= wED(v_{i.},v_{j.}) \\
  &= \exp
  {
    \Big[-\frac{\sum_{m=1}^{|V|}{w_m(u_{im}-u_{jm})^2}} {\sum_{m=1}^{|V|}{w_m}}\Big]
  },
\end{split}
\end{equation}
, which is also called a Gaussian measurement. Here $v_{im}$($v_{jm}$) is the value of the $m$ th. vertex of the cortical surface testing unit which, for the simulation studies, is roughly a oval shaped region of 512 vertices, for real data analysis, is one of the 68 anatomical regions. $|V|$ is the dimensionality of (i.e. number of vertices in the oval/anatomy region). $w_m$ is the weight assigned to the $m$ th. vertex in the testing unit. Without pre-knowledge regarding the relative importance of the vertices, the similarity measurement is simplifed by equal weighting ($w_m \equiv 1$):
\begin{equation*} \label{eq_uSV}
\begin{split}
  S_{ij}^V &= uED(v_{i.},v_{j.}) \\
  &= \exp
  {
    \Big[-\frac{\sum_{m=1}^{|V|}{(u_{im}-u_{jm})^2}} {|V|}\Big]
  }.
\end{split}
\end{equation*}
Lastly, for a multivariate phenotype profile, first a rank normal quantile normalization were applied to all the components according to \cite{HWU}:
\begin{displaymath}
  q_{im}=\Phi^{-1}[(rank(y_{im})-0.5)/|Y|],
\end{displaymath} 
where $y_{im}$ is the value of the $m th.$ component of the phenotype for the $i th.$ sample, and $|Y|$ is the dimensionality of the phenotype (i.e. number of components). Doing so not only correct skewed components, but also remove the complication of admixed distribution type introduced by a multivariate phenotype. As a result, the phenotype based similarity can be measured in a manner similar to that of cortical surface:
\begin{equation} \label{eq_wSY}
\begin{split}
  S_{ij}^Y &= wED(q_{i.},q_{j.}) \\
  &=\exp
  {
    \Big[-\frac{\sum_{m=1}^{|Y|}{w_m(q_{im}-q_{jm})^2}} {\sum_{m=1}^{|Y|}{w_m}}\Big]
  },
\end{split}
\end{equation}
where $q_{im}$ is the values of the $m th.$ component of the normalized phenotype; the weight $w_m$ is the ralative importance of that dimension. For a case control study of one dimensional phenotype, that is, $|Y|=1$, the similarity measurement simplifys to:
\begin{displaymath}
  S_{ij}^{y}=wED(q_i,q_j)=\exp{[-(q_i-q_j)^2]},
\end{displaymath}
All three types of the similarity are centralized by substracting the raw measurement from every pair $(i,j)$ with two sample means of all measurements involving $i$ and $j$, respectively, then adding the sample mean of all the pairs to it \cite{HWU}. Takeing the weighted genetic similarity as an example, the centralized similarity measurement is:
\begin{equation} \label{eq_wSY}
\begin{split}
  \tilde{S}_{ij}^{G} &= S_{ij}^{G}-E(S_{i.}^{G})-E(S_{.j}^{G})+E(S_{..}^{G}) \\
  &= S_{ij}^{G} - \frac{1}{N}\sum_{k=1}^N{S_{ik}} - \frac{1}{N}\sum_{l=1}^N{S_{lj}} + \frac{1}{N^2}\sum_{k=1,l=1}^{N,N}{S_{kl}},
\end{split}
\end{equation}
where $N$ is number of samples. The same centralization scheme is applied to the other two similarity measurements. The U statistics is the mean of the product of three centralized similarity measurement across all but mirrored pairs:
\begin{displaymath}
  U^{GVY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{G} \tilde{S}_{ij}^{V} \tilde{S}_{ij}^{Y}.
\end{displaymath}
The actual implementation calculated three symmetric $N \times N$ similarity matrices to facilitate later manipulation. The U statistic follows a mixture of $\chi_1^2$ distrubtion, a $p$ value can then be calculated using Davis method \cite{HWU}.

For now, the genomic and vertex similarity are treated U-weight terms, while the phenotype similarity acts as the U-kernel term. In general, the assignment of U-kernal and U-weight terms does not alter the limiting distribution of the final U statistics. More specific association test can be perform by dropping one of the U-weight terms. To see if only the phenotype and genomic profiles are associated, construct the U statistics without vertex similarity term:
\begin{displaymath}
  U^{GY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{G} \tilde{S}_{ij}^{Y}.
\end{displaymath}
Likewise, dropping the genomic similarity to test if only the phenotype and genomic profiles are uncorrelated:
\begin{displaymath}
  U^{VY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{V} \tilde{S}_{ij}^{Y}.
\end{displaymath}

\subsection{Stacked Autoencoder}
The purpose of an encoder is to abstract high order features out of the raw input, which would then be meaningful for decision making. A real life example is vewing a painting: what intrests the viewer is the concept of object or event the painting captures, not the exact colouring at every inch of the canvas. As for the cortical surface profile, being able to see the overall shrinkage and laceration sites in a aged brain, even without knowning the exact volumn of the gray matter or the location and boundery of the sites, is far more important than knowning the exact thickness, curvature and coordinates of at every vertices. An extreamly accurate and objective measurement at every unit of the space is, though highly informative, but remotely helpful for diagnostic purpose without abstraction of high order information from it. The Stacked Autoencoder (SA) to be implemented here is the result of recent development of deep artificial neuro-network, originally inspired by the sentinental processing of visual information. By definition, an abstraction must reduce the complexity of the raw visual information considerablly so the host could possibly make a meaningful accessment, which, in term of the SA, the dimensionality of the vertex data must be reduced. 

The predecessor of the deep encoder is the Multiple Layer Perceptron (MLP), one of the earlist biologically inspired neuronetworks among Artifical Intellegence (AI) framework. An MLP allows one directional flow of data (e.g. a clique of clinical readings) through layers of "neurons" to reach a decision at the top (e.g. disease diagnosis). The neurons are fully connected between two adjecent layers, but not connected at all within a layer or non-adjecent ones. In symbolic form, using superscript $d_.$ to show the dimensionality of the data, the input for layer $\#i$ is the $d_{i-1}$ dimensional output of the layer below ($\#i-1$), denoted by vector $\boldsymbol{z_{i-1}^{d_{i-1}}}$.  

An SA is formed by layers of autoencoders stacked from bottom to top, hence the name "Stacked Autoencoder". An autoencoder represents a linear recombination of the elements of the input from its down below, followed by an element-wise non-linear transformatio. Usually, the recombinatino has a lower dimensionality (i.e. the number of elements) then the input from below. In symbolic form, an autoencoder placed at the $i$ th. layer of an SA can be written as:
\begin{equation} \label{eq:AE}
  \begin{split}
    \boldsymbol{z_i^{d_i}} = s(\boldsymbol{W_i^{d_i\times d_{i-1}}z_{i-1}^{d_{i-1}}} + \boldsymbol{b^{d_i}})\,
  \end{split}
\end{equation}
where $\boldsymbol{z_{i-1}^{d_i-1}}$ is the the input comming from the layer down below -- the output of the $i-1$ th. autoencoder in the stack), while $\boldsymbol{z_i^{d_i}}$ is the output of this autoencoder -- the $i$ th. in the stack. The linear recombination of the elements from the $i-1$ th. layer is achieved by its corss product with $\boldsymbol{W_i^{d_i\times d_{i-1}}}$ and the offest $\boldsymbol{b^{d_i}}$, which resembles the snapsis connection from one layers of neurons to another. Here, The superscript $d_{i-1}$ and $d_i$ denote the dimensionality of the data and the parameters in the $i-1$ and $i$ autoencoder in the stack, respectively. Although not mandotory, $d_i$ is usually chosen to be smaller than $d_{i-1}$ in order to promote a bottom to top feature abstraction and dimension reduction.

The elementwise non-linear transformation, here denoted by $s$, is usually chosen to be the inverse logit function $s(x)=\frac{L}{1+e^{-x}}$. The "S" shaped inverse logit curve resembles the steep biological activation of a neuron, yet its defferentiability on the whole real line $R$ also facilitates gradient gudided numerical optimization. 

An SA of $M$ layers, $P$ dimensional raw input $\boldsymbol{x}$ and $Q$ dimensional high order feature output, is constucted by recursively pluging the output from the autoencoder of lower layer into the one above, and making sure the output of the top layer is $Q$ dimensinal:
\begin{equation} \label{eq:MLP}
  \begin{split}
    \boldsymbol{\hat{y}}=
    \boldsymbol{z_{M  }^{d_{M  }}}         &= s(\boldsymbol{W_{M  }^{d_{M  } \times d_{M-1}}z_{M-1}^{d_{M-1}}}+\boldsymbol{b_{M  }^{d_{M  }}}) \\
    \boldsymbol{z_{M-1}^{d_{M-1}}}         &= s(\boldsymbol{W_{M-1}^{d_{M-1} \times d_{M-2}}z_{M-2}^{d_{M-2}}}+\boldsymbol{b_{M-1}^{d_{M-1}}}) \\
    ... &= ... \\
    \boldsymbol{z_{i  }^{d_{i  }}}         &= s(\boldsymbol{W_{i  }^{d_{i  } \times d_{i-1}}z_{i-1}^{d_{i-1}}}+\boldsymbol{b_{i  }^{d_{i  }}}) \\
    ... &= ... \\
    \boldsymbol{z_{2  }^{d_{2  }}}         &= s(\boldsymbol{W_{2  }^{d_{2  } \times d_{1  }}z_{1  }^{d_{1  }}}+\boldsymbol{b_{2  }^{d_{2  }}}) \\
    \boldsymbol{z_{1  }^{d_{1  }}}         &= s(\boldsymbol{W_{1  }^{d_{1  } \times d_{0  }}z_{0  }^{d_{0  }}}+\boldsymbol{b_{1  }^{d_{1  }}}) \\
    \boldsymbol{z_{0  }^{d_{0  }}}         &= \boldsymbol{x}
  \end{split},
\end{equation}
where $\boldsymbol{x}$ and $\boldsymbol{\hat{y}}$ denote the $P$ dimensional raw input and the final, $Q$ dimensional high order abstraction output, repectively. The vector $\boldsymbol{z_i^{d_i}}$ denotes the intermidiate abstraction output by the $i$ th. autoencoder while the superscript denotes its dimensionality. The $P$ dimensional input $\boldsymbol{x}$ is treated as output of the non-existing $0$ th. autoencoder, denoted by $\boldsymbol{z_0^{d_0}}$, where $d_0=P$. From the bottom to up, the SA should gradually extract higher order features from the $P$ dimensional raw input $x$, until the dimensionality of the abstraction is as low as $d_M=Q$.

An SA thus constructed is still useless without tuning, that is, one must find a set of parameters $\boldsymbol{\theta}=\{\boldsymbol{W_{1:M}},\boldsymbol{b_{1:M}}\}$ that makes the SA turely capable of abstracting meaningful features instead of haphazardly reducing a high dimensional input into a small but irrelevent output (e.g. a vector of $Q$ zeros). The "meaningfulness" of the SA can be gauged by two different criteria. For one, it is reasonable to say that, an abstraction should help to better predict the outcome of interest. Turning strategies based on this belong to the so called supervised machine learning. Another criteria state that, if an abstraction is indeed done better, the original input should be able to be reconstructed from it more accurately, resulting in the unsupervised machine learning. Here ``supervised'' means the ``goodness of abstraction'' is judged by asking how accurate the predictions were made using features extracted by the SA from the raw input, in comparison with the known external fact from a specific domain of problems. The SA thus learned will try to extract features better suited for problems from that domain (e.g. tell the future risk of neuralogical disorder by examing the cortical surface). The ``unsupervised'' learning, on the other hand, judges the ``goodness of abstraction'' by having the raw input comparing with its reconstructed self from the extracted features. An SA tuned in this manner will try to extract more general features not necessarily specific to a certain problem domain. In this work, the later, unsupervised learning was adopted to explore generic knowledge regarding the human cortical surface, not just for the purpose of diagnosing a certain type of neuralogical disorder (e.g. Alzheimer's Disease). As a matter of fact, all available T1 MRI samples contrubute to the ``machines's understanding of human brain'', even if more than half of them cannot enter the association analysis because of non-definite disease diagnosis.

Prior to unsupervised learning however, one has to come up with a reconstruction scheme -- a decoder counterpart of the stacked autoencoder. The most nature way is to mirror the structure of the SA, that is, the number of layers in the decoder is the same with the SA, each layer is also a linear recombination of its input followed by a element-wise non-linear transformation, but the dimensionality change are exactly in reverse order of the SA. In symbolic form, the decoder can be structured in a similar manner:
\begin{equation} \label{eq:DEC}
\begin{split}
  \boldsymbol{\tilde{x}}=
  \boldsymbol{\tilde{z}_{0  }^{d_{0  }}} &= s(\boldsymbol{\tilde{W}_{1  }^{d_{0  } \times d_{1  }} \tilde{z}_{1  }^{d_{1  }}}+\boldsymbol{\tilde{b}_{0  }^{d_{0  }}}) \\
  \boldsymbol{\tilde{z}_{1  }^{d_{1  }}} &= s(\boldsymbol{\tilde{W}_{2  }^{d_{1  } \times d_{2  }} \tilde{z}_{2  }^{d_{2  }}}+\boldsymbol{\tilde{b}_{1  }^{d_{1  }}}) \\
  ... &= ... \\
  \boldsymbol{\tilde{z}_{i  }^{d_{i  }}} &= s(\boldsymbol{\tilde{W}_{i+1}^{d_{i  } \times d_{i+1}} \tilde{z}_{i+1}^{d_{i+1}}}+\boldsymbol{\tilde{b}_{i  }^{d_{i  }}}) \\
  ... &= ... \\
  \boldsymbol{\tilde{z}_{M-2}^{d_{M-2}}} &= s(\boldsymbol{\tilde{W}_{M-1}^{d_{M-2} \times d_{M-1}} \tilde{z}_{M-1}^{d_{M-1}}}+\boldsymbol{\tilde{b}_{M-2}^{d_{M-2}}}) \\
  \boldsymbol{\tilde{z}_{M-1}^{d_{M-1}}} &= s(\boldsymbol{\tilde{W}_{M  }^{d_{M-1} \times d_{M  }} \tilde{z}_{M  }^{d_{M  }}}+\boldsymbol{\tilde{b}_{M-1}^{d_{M-1}}}) \\
  \boldsymbol{\tilde{z}_{M  }^{d_{M  }}} &= \boldsymbol{\hat{y}}.
\end{split}
\end{equation}
From bottom to up, the decoder gradually restore details from the abstracted features ${\boldsymbol{\hat{y}}}$, and eventually reach a reconstruction of the raw input $\boldsymbol{\tilde{x}}$. The encoder \ref{eq:MLP} is linked from below by plugin its output into the bottom of the decoder through ${\boldsymbol{\hat{y}}}$. The entire encoding -- decoding structure can be written as:
\begin{equation} \label{eq:SDA}
\begin{split}
  \boldsymbol{\tilde{x}}                 &= \boldsymbol{\tilde{z}_{0  }^{d_{0  }}} \\
  \boldsymbol{\tilde{z}_{0  }^{d_{0  }}} &= s(\boldsymbol{\tilde{W}_{1  }^{d_{0  } \times d_{1  }} \tilde{z}_{1  }^{d_{1  }}}+\boldsymbol{\tilde{b}_{0  }^{d_{0  }}}) \\
  ... &= ... \\
  \boldsymbol{\tilde{z}_{M-1}^{d_{M-1}}} &= s(\boldsymbol{\tilde{W}_{M  }^{d_{M-1} \times d_{M  }} \tilde{z}_{M  }^{d_{M  }}}+\boldsymbol{\tilde{b}_{M-1}^{d_{M-1}}}) \\
  \boldsymbol{\tilde{z}_{M  }^{d_{M  }}} &= \boldsymbol{\hat{y}} = \boldsymbol{z_{M  }^{d_{M  }}} \\
  \boldsymbol{z_{M  }^{d_{M  }}}         &= s(\boldsymbol{W_{M  }^{d_{M  } \times d_{M-1}} z_{M-1}^{d_{M-1}}}+\boldsymbol{b_{M  }^{d_{M  }}}) \\
  ... &= ... \\
  \boldsymbol{z_{1  }^{d_{1  }}}         &= s(\boldsymbol{W_{1  }^{d_{1  } \times d_{0  }} z_{0  }^{d_{0  }}}+\boldsymbol{b_{1  }^{d_{1  }}}) \\
  \boldsymbol{z_{0  }^{d_{0  }}}         &= \boldsymbol{x}.
\end{split}
\end{equation}
The difference between $\boldsymbol{\tilde{x}}$ and $\boldsymbol{x}$ is a rough measure of how badly the reconstructed input resemble the original, which is also telling us how badly the encoder had performed, because a truely meaningful abstraction should not obstruct the recovery of details. Therefore, the learning of the SA is the process of minimizing the disagreement between $\boldsymbol{\tilde{x}}$ and $\boldsymbol{x}$ by fine tuning the structuring parameters in the entire autoencoder and decoder stack, which is:
\[
\boldsymbol{\Theta} = \boldsymbol{\theta} \cup \boldsymbol{\tilde{\theta}}=\{\boldsymbol{W_{1:M}},\boldsymbol{b_{1:M}}\} \cup \{\boldsymbol{\tilde{W}_{1:M}}, \boldsymbol{\tilde{b}_{1:M}}\}.
\]
Two common measurement of the disagreement between $\boldsymbol{\tilde{x}}$ and $\boldsymbol{x}$ are least square error and corss entrophy, the former is more of a tradition, the later has gained popularity recently. In symbolic form, they can be written as:
\begin{equation} \label{eq:SE}
\begin{split}
  L^{SE}(\boldsymbol{\tilde{x}},\boldsymbol{x}) &= \sum_{i=1}^N\sum_{k=1}^Q\frac{1}{Q}(\tilde{x}_{ik}-x_{ik})^2,
\end{split}
\end{equation},
\begin{equation} \label{eq:CE}
\begin{split}
  L^{CE}(\boldsymbol{\tilde{x}},\boldsymbol{x}) &= -\sum_{i=1}^N\sum_{k=1}^Q[x_{ik}\log{\tilde{x}_{ik}}+(1-x_{ik})\log(1-\tilde{x}_{ik})],
\end{split}
\end{equation}
where $i$ indexes the samples, $k$ indexes the elements of the orignal/reconstruction input, both $L^{SE}$ and $L^{CE}$ are functions of $\boldsymbol{\Theta}$. In this study, the reconstruction loss takes the cross entrophy ($L=L^{CE}$) form, so the search of the best setup of parameters is expressed as a numerical optimization problem:
\[ \boldsymbol{\Theta} = \min_{\boldsymbol{\Theta}} L(\boldsymbol{\tilde{x}},\boldsymbol{x}). \]
Taking the $M$ layered SA and its decoder counterpart together, the total number of parameters is $|\boldsymbol{\Theta}| = |\boldsymbol{\theta}| + |\boldsymbol{\tilde{\theta}}| = \sum_{i=1}^{M}{(d_i + 1)d_{i-1}} + \sum_{j=M}^{1}{(d_{j-1} + 1)d_j}$. The optimization can be computationally intense because this number is usually huge.

One practical strategy to learn a SA is to constrain the linear reconbination in the decoder layers to be the transpose of their corresponding layers in the SA, that is, forcing $\boldsymbol{\tilde{W}_i^{d_{i-1} \times d_i}} \equiv (\boldsymbol{W_i^{d_i \times d_{i-1}})^{\prime}}$ for $i=1 \ldots M$. Doing so alost halve the number of parameters to $\sum_{i=1}^{M}{(d_{i-1}) \times (d_i - 1)} + \sum_{i=M}^{1}{d_i}$, and better fits the intuition that encoding and decoding are conceptually symmetric. More importantly, it also encourages the learning of an optimal SA instead of a sub-optimal SA coupled with a powerful decoder on its top, afterall, it is the best abstraction of the raw input being mostly wanted, not its reconstruction. 
The optimization is done by gradient decent. Starting with a randomly initialized assignment of $\Theta^t$ at $t=0$, and compute the next assignment $\Theta^{t+1}$ by substracting from the current assignment $\Theta^t$ a small fraction of the gradient of reconstruction loss $L$ with respect to the current assignment ($\frac{\partial L}{\partial \boldsymbol{\Theta^{t}}}$). The small fraction is called a learning step, if the step is reasonablly small, the reconstruction loss $L$ will keep dropping. The learning will repeat until $L$ cease to drop. The final assignment is considered the optimal $\Theta$. 

The arduous calculation of the gradient $\PDV{L}{\boldsymbol{\Theta}^t}$ is done by a process called back propagation [], which mainly relies on additive rule and chain rule of the derivatives. First, realize the reconstruction loss $L=L^{CE}(\xVO, \xVR)$ is a function of $\xVR$ while $\xVR$ is a function of $\Par$, using the chain rule, one obtains:
\begin{equation*}
\begin{split}
  \PDV{L}{\Par} &= (\PDV{L}{\xVR})^{\prime} \PDV{\xVR}{\Par} \\
  \PDV{L}{\xVR} &= \xVO \oslash \xVR - (\boldsymbol{1} - \xVO) \oslash (\boldsymbol{1} - \xVR).
\end{split}
\end{equation*}
Here $\oslash$ denotes element-wise division.


\begin{equation} \label{eq:unit encoder-decoder}
\begin{split}
  \boldsymbol{\tilde{z}_{i-1}^{d_{i-1}}} &= s(\boldsymbol{\tilde{W}_{i  }^{d_{i-1} \times d_{i  }} z_{i  }^{d_{i  }}}+\boldsymbol{\tilde{b}_{i-1}^{d_{i-1}}}) \\
  \boldsymbol{z_{i  }^{d_{i  }}}         &= s(\boldsymbol{W_{i  }^{d_{i  } \times d_{i-1}} z_{i-1}^{d_{i-1}}}+\boldsymbol{b_{i  }^{d_{i  }}}), \\
\end{split}
\end{equation}
where $i=(1 \ldots M)$. Thus, instead of pipping the output of the $i th$ encoder layer ($\boldsymbol{z_{i  }^{d_{i  }}}$) to the one above, it is pipped to the cooresponding $M-i th.$ layer in the docoder counterpart for an immediate reconstruction of the input $\boldsymbol{z_{i-1}}$. The unit encoder--decoder tuple can then be trained by minimizeing the reconstruction loss $L(\boldsymbol{z_{i-1}},\boldsymbol{\tilde{z}_{i-1}})$. A total of $M$ units can be formed and trained seperatetly. Although the pre-training cannot be done in parallel since the higher unit have to wait the lower for input, the process is very fast because the number of parameters is rather small in each unit compared with the whole network. The pre-training serves as a non-random parameter initilization, after which the whole network is already near convergence. Therefore, the subsequent fine-tuning of entire network would required much less steps to reach convergence.

The MLP or encoder thus trained may suffer the overfitting problem, that is, the network performs will on the known facts, producing low prediction/reconstruction loss, but deals poorly for the future, unknown data. To better balance the internal and external validy, one way is to add a regulator term to the loss funciton $L$, usually the $L1$ or $L2$ norm of the parameter elements. Doing so discourages large parameters which causes instability of the prediction/reconstruction. For encoder training, another way to ensure generalizability is to randomly corrupt the input $\boldsymbol{x}$ by setting some elements to zero [2008 Vincent]. The rationel is, the more rigours a feature is, the more like it will survive the random corruption. More training epoch is needed to ensure the randomness of corruption, but doing so encourages the network to seek major features of the input while neglecting the minor changes between facts. And since major feature are more likely to show up in the future, the external validty of the encoder is thus preserved.

For the current study howver, the purpose of deep-learning is not to construct an encoder encrypted with universal knowledge regarding white matter surface, but to produced the abstration the surface vertices at hand for the up-coming analysis. In other words, the code $\hat{y}$ is the only concern here, and the external validty will be implictly handled by the subsequent U-statistical analysis using such code. For now, an ad-hoc encoder trained without regulator term or ramdom corruption would surfice. The current implementation is a five layered encoder, each halves the dimensionality of its input. Thus the final encoding will be 32 times smaller then the raw input.
