\section{Method}

\subsection{Reduce Imaging Dimensionality with Stacked Autoencoder}
A Stacked Autoencoder (SA) is a type of artificial neural network (ANN) mimicking visual processing that abstracts high order features from the raw image. High order feature is more relevent to decision making. Taking our 3D surface as an example, being able to recognizance general location, size and shape of the laceration sites in the cortex, is far more informative than knowning the exact thickness, curvature and coordinates of every vertex in the profile. Thus, besides dimension reduction and increased noise/signal ratio, we also anticipate a power boost for any similarity subsequent association analysis by replacing raw imaging with with abstracted high order features.

An stacked autoencoder (SA), as its name suggest, comprises two or more encoders stacked on top of each other. The the $i$ th. encoder in the stack looks like
\begin{equation} \label{eq:AE}
  \zEC_i^{d_i} = \boldsymbol{s}(\WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}),
\end{equation}
where $\zEI{i}{i}$ is the ($i$ th.) encoder's $d_i$ dimensional output, while $\zEI{i-1}{i-1}$ being its $d_{i-1}$ dimensional input, which is also the output from down below (the $i-1$ th. encoder in the stack). The $j$ th. element in the output is the sum of input elements weighted by the $j$ th. row vector of the weight matrix $\WEI{i}{i}{i-1}$, plus the $j$ element in the offset vector $\bEI{i}{i}$, then put through a non-linear transformation $s(.)$ which is usually the inverse logit, that is, 
\[z_{i,j} = \frac{exp(\wEI{i,j}{i-1} \zEI{i-1}{i-1} + b_{i,j})}{1 + exp(\wEI{i,j}{i-1} \zEI{i-1}{i-1} + b_{i,j})}, \quad (j=1 \dots, d_i). \]
To ensure dimensional reduction and abstraction actually happen (hence ``encode''), the output size $d_i$ is made smaller than the input $d_{i-1}$. In our case, we let the encoders always halve their input.

To assemble a stack of $M$ encoders which accepts size $P$ input vector $\xEC^P$ and produces size $Q$ output $\yHT^Q$, we recursively wire the output of the lower encoder to the one above, and ensure the input dimension at the bottom, $d_0$, equals to $P$, and the output dimension on the top, $d_M$, equals to $Q$. The encoder stack looks like
\begin{equation} \label{eq:SE}
  \begin{split}
    \yHT^Q &= \zEI{M}{M} \\
    \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M  }{M  }) \\
    \zEI{M-1}{M-1} &= s(\WEI{M-1}{M-1}{M-2} \zEI{M-2}{M-2} + \bEI{M-1}{M-1}) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{i  }{i  } &= s(\WEI{i  }{i  }{i-1} \zEI{i-1}{i-1} + \bEI{i  }{i  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{2  }{2  } &= s(\WEI{2  }{2  }{1  } \zEI{1  }{1  } + \bEI{2  }{2  }) \\
    \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1  }{1  }) \\
    \zEI{0  }{0  } &= \xEC^P.
  \end{split}
\end{equation}
Here, the $P$ dimensional input $\xEC^P$ can be viewed as the output of the non-existing $0$ th. encoder. By ensureing $P = d_0 > d_1 > d_2 \dots \ge d_{M-1} > d_M = Q$, the SA abstracts $Q$ dimensional high order features from the $P$ dimensional raw profile.
The parameters in the SA, that is, the weights and offsets $\{\WEC_1, \bEC_1, \WEC_2, \bEC_2, \dots, \WEC_M, \bEC_M\}$ must be tuned to represent the body of knowledge that generated $\xEC^P$, which in our case is the knowledge of human cortex. To do so we first assemble a stacked decoder counterpart which exactly mirrors the topology and weight of the encoders:
\begin{equation} \label{eq:SD}
  \begin{split}
    \xDC^P &= \zDI{0}{0} \\
    \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1  }{0  }) \\
    \zDI{1  }{1  } &= s(\WDI{2  }{1  }{2  } \zDI{2  }{2  } + \bDI{2  }{1  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zDI{i-1}{i-1} &= s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}) \\
    & \quad \quad \quad \quad \vdots \\
    \zDI{M-2}{M-2} &= s(\WDI{M-1}{M-2}{M-1} \zDI{M-1}{M-1} + \bDI{M-1}{M-2}) \\
    \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M  }{M-1}) \\
  \end{split}
\end{equation}
By mirroring, it means
\begin{align}
  \WDI{M-i}{M-i-1}{M-i} \equiv (\WEI{M-i}{M-i}{M-i-1})^\prime, \quad (i=0, \dots, M),
\end{align}
which is quiet instinctive since decoding is the opposite of encoding, though the offsets in the decoders, $\bDI{M-i}{M-i-1}, \quad (i=0, \dots, M)$, are still allowed to be flexible. From bottom to top, the stack of decoders gradually restores details back to the abstracted state $\zDI{M}{M}$, and eventually produces a reconstructed input $\xDC^P$ at its top. Next thing to do is to wire \ref{eq:SE} and \ref{eq:SD} together by making $\zDI{M}{M} = \yHT^Q$,
\begin{equation} \label{eq:SA}
  \begin{split}
    \xDC^P &= \zDI{0}{0} \\
    \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1}{0  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M}{M-1}) \\
    \zDI{M  }{M  } &= \yHT = \zEI{M}{M} \\
    \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M}{M  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1}{1  }) \\
    \zEI{0  }{0  } &= \xEC^P,
  \end{split}
\end{equation}
which creats a stacked autoencoder (SA)\cite{DL:SDA1}. The aforementioned weights and offsets in the encoder stack (lower half of the SA), alongside with $M$ extra offsets in the decoder stack (higher half of the SA), constitute the parameters to be calibrated so the encoder stack can best represent the process generated $\xEC^P$. The calibration is done by minimize the discrepancy between the reconstructed input $xDC^P$ and the true input $xEC^P$, which turns into the optimization problem
\begin{equation} \label{eq:CE}
  \begin{split}
    \Par^* = \min_{\Par} \sum_{k=1}^N{d(\xDC_k^P, \xEC_k^P)}, \quad \Par = \cup_{i=1}^M \{\WEC_i, \bEC_i, \bDC_i\},
  \end{split}
\end{equation}
where $k$ indices the $N$ training samples; $d$ is the objective function measuring the disagreement between the reconstructed and the original input, a popular form of which is cross-entropy
\begin{align} \label{eq:CE}
  d(\xDC_k^P, \xEC_k^P) = \sum_{j=1}^P{x_{j,k}\log{\tilde{x}_{j,k}} + (1 - x_{j,k})\log{\tilde{x}_{j,k}}},
\end{align}
where $j$ indices the $P$ elements of the input. The numerical optimization of large number of parameters ($|\Par| = \sum_{i=1}^M{d_i d_{i-1} + d_i + d_{i-1}}$) is done by stochastic gradient descent (SGD) \cite{SGD1, SGD2}, also called back propagation (BP) by neural network literature concerning the computation of high dimensional gradient $\PDV{d(\xDC_k^P, \xEC_k^P)}{\Par}$ \cite{BP1, BP2, BP3}. For actual implementation, SGD and BP is programed using Theano \cite{Theano1} -- a computational differentiation library written in Python. 

To avoid shallow local minimum and counter difficulty of converging caused by a multi-layer ANN, that is, the SA, we follow the recent ``deep learning'' trend by which a layer-wise greedy pre-training procedure is applied prior to fine-tuning the entire network \cite{DL:DBN1, DL:SDA1}. To perform the layer-wise pre-training, the output of $i$ th. encoder, $\zEC_i$, is disconnected from the encoder above it and rewired to its counterpart decoder, forming a single layered autoencoder, which is then calibrated by minimizing the intermediate reconstruction loss $d_i(\zEC_{i-1}, \zDC_{i-1})$:
\begin{equation}\label{eq:Greedy}
  \begin{split}
    \zDC_{i-1} &= s(\WDC_i \zDC_i + \bDC_i) \\
    \zDC_{i  } &= \yHT_i = \zEC_i \quad \qquad \qquad \qquad (i = 0 \dots M) \\
    \zEC_{i  } &= s(\WEC_i\zEC_{i-1} + \bEC_i). \\
    \Par_i^* &= \min_{\Par_i}{d_i(\zEC_{i-1}, \zDC_{i-1})}, \qquad \Par_i = \{\WEC_i, \bEC_i, \bDC_i\},
  \end{split}
\end{equation}
which is easy given the small number of parameters $|\Par_i|=d_i d_{i-1} + d_i + d_{i-1}$. After all $M$ autoencoders are assembled and pre-trained, the encoders and decoders are wired back like (\ref{eq:SE}) and fine-tuned, which should converge fast and less likely to ``go down the wrong pit'' in the gradient terrian than a direct training scheme without the assistance of pre-training.

\subsection{Joint Test with Similarity U}
We use a similarity U statistic {\cite{UST1, UST2} to jointly test the existence of association among genomic, imaging and phenotype profiles. To derive the statistic, three kernel functions measuring pairwise similarity are chosen for each profile. The measurement $f$ can be flexible to suit the charisteristics of the profile (e.g., bounded or not, continuous or discrete), as long as $f$ is symmetric and finite in second moment, that is, $f(x_i,x_j) \equiv f(x_j,x_i)$ and $E(f^2(x_i, x_j))<+\infty$.
\newcommand{\vg}{\boldsymbol{g}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\vy}{\boldsymbol{y}}
\newcommand{\vG}{\boldsymbol{G}}
\newcommand{\vV}{\boldsymbol{V}}
\newcommand{\vY}{\boldsymbol{Y}}
\newcommand{\vq}{\boldsymbol{q}}

For genomic profiles, the identical by state (IBS) kernel is used,
\label{eq:wSG}
\[ f_G(\vg_{i.}, \vg_{j.}) = \frac{\sum_{m=1}^{|vG|}{w_m(2 - |g_{im} - g_{jm}|)}} {2\sum_{m=1}^{|vG|}{w_m}}, \]
where $(i,j)$ indices a pair of observations, and $m$ indices a genomic variant (i.e. a SNP) in the testing unit (e.g. a gene); $w_m$ is the weight assigned to the $m th.$ variant according to \textit{a prior} hypothesis, such as allele frequency (AF) based $w_m=\sqrt{AF(g_{.m})[1-AF(g_{.m})]}$ that emphasizes rare variants.

For 3D cortex profiles, similarity is measured by the Euclidian distance based Gaussian kernel which is better suited for continuous values (e.g. the thickness of gray matter),
\label{eq:wSV}
\[ f_V(v_{i.},v_{j.}) = \exp{ [-\frac{\sum_{m=1}^{|V|}{w_m(u_{im}-u_{jm})^2}} {\sum_{m=1}^{|V|}{w_m}}] }. \]
Here $m$ indices a imaging variant, that is, a vertex in the surface, the other notations are the same with the above genomic kernel.

Lastly, for a phenotype profile generated by unknown distribution, is first normalized with the rank normal quantile function
\[ q = \frac{\Phi^{-1}[rank(y - 0.5)]}{N}, \]
where $N$ is the number of observations. The pairwise phenotype similarity is also measured by a Gaussian kernel, simplied for nwo because the phenotype is one dimensional,
\[ f_Y(q_i, q_j) = \exp{[-(q_i - q_j)^2]}. \]

The similarity measure for a pair $(i, j)$ must be centralized by substracting the two marginal mean measure involving either $i$ or $j$, then adding the grand mean \cite{UST1}:
\begin{align*}
  \tilde{f}(*_i, *_j) = f(*_i, *_j)-\frac{1}{N} \sum_{k=1}^N{f(*_i, *_k)}-\frac{1}{N}\sum_{l=1}^N{f_G(*_l, *_j)} + \frac{1}{N^2} \sum_{k=1}^{N} \sum_{l=1}^{N} {f(*_l, *_k)}
\end{align*}
where $N$ is number of samples. 

Finally, the joint similarity U statistics $U_J$ is the mean of entrywise product of centralized measurements excluding the self-pairs, which is
\[ U_J = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_G(\vg_{i.}, \vg_{j.}) \circ \tilde{f}_V(\vv_{i.}, \vv_{j.}) \circ \tilde{f}_Y(y_i, y_j). \]
When there is no association among the three profiles, $U_J$ should be close to $0$ since all three measurements has mean $0$. Conversely, if $U_J$ deviates from $0$ significantly, it means the similarity regarding one profile is correlated to the similarity regarding some other profile, which implies the presence of association. More formally, under $H_0^J$: there is no association between any of the profiles, $U_J$ follows a $\chi_1^2$ mixture weighted by the squared engien values of $\tilde{f}_G(\vg_{i.}, \vg_{j.}) \circ \tilde{f}_V(\vv_{i.}, \vv_{j.}) \circ \tilde{f}_Y(y_i, y_j)$ \cite{UST1, UST2}. The $p$ is calculated using Davis method \cite{davies80}.

Alternatively, two simpler tests can be done by dropping either the imaging or genomic kernal from the similarity U statistic. So
\[ U_G = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_G(\vg_{i.}, \vg_{j.}) \circ \tilde{f}_Y(y_i, y_j) \]
tests $H_0^G$: there is no association between the phenotype and genomic profiles. And
\[ U_V = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_V(\vv_{i.}, \vv_{j.}) \circ \tilde{f}_Y(y_i, y_j) \]
tests $H_0^V$: there is no association between the phenotype and imaging profiles. The two simpler tests are more specific and may produce more parsimonious models, but also risk mis-specification, which is examed by simulation studies.

The method also applies grouping and signal aggregation on cortex profile. For comparison purpose, we also implement the vertex-wise analysis (VWA). Briefly speaking, it first smooth the testing unit with a Gaussian filter of standard deviation 2, which reduces the noise by grinding away trivial details in the cortical surface. Next, it treats $|V|$ vertices in the original testing unit as $|V|$ single dimensional testing unit, based on which $|V|$ $U_J$(or $U_V$) statistics are calculated, and $|V|$ FDR (false discovery rate) adjusted p-values are derived. Should any of the adjusted p-values below 0.05 threshold, the entire testing unit is significantly associated with some other profiles (phenotype, genomic, or both).

\subsection{Study Data Source}
The next generation sequencing (NGS) and magnetic resonance imaging (MRI) data were obtained from Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI). A totol of 808 subjects at the screening and baseline of ADNI1 and ADNI2 study have both profiles available, alongside with disease diagnosis, demographics, and the genotype of APOE $\epsilon$4. 

The structure MRI data first went throught a series of processing including special registration, skull stripping, cortical/subcortical segmentation, white/gray matter segregation, vexel intensity normalization, reconstruction of cortex, cortex registration, and cortex paceration. The entire pipline is implemented by \FS - a neuroimage analysis package developed by Fisher and Dale et.al. \cite{FS:Intro}, and currently maintained by \textit{the Laboratory for Computational Neuroimaging (LCN)}  at \textit {the Athinoula A. Martinos Center for Biomedical Imaging}. \FS is freely distributed online (\url{http://surfer.nmr.mgh.harvard.edu}). The reconstructed cortex is spaned by $327680$ vertices in 3D space. Each vertex is treated as a image variant, with a number of geometrical attribute attatched to it, such as the coordinate of the vertex, the gray matter thickness, average curvature, local area and volume around its vicinity. Currently we took the gray matter thickness as the value of each vertex. The last processing step of \FS -- cortex paceration, divides the cortical surface into 68 anatomical regions. For real data analysis, these regions are taken as testing units, for simulation study, small ovals of $512$ vertices (mean diametter=28mm) were randomly picked from the cortex as testing units. There are 2 smaples failed the image processing, the total sample left for the study is 806.

The NGS data has gone through rigorous quality control during variant calling process, thus the WGS data from ADNI do not require intensive processing. The testing units for both real data and simulation study are gene based. The chromosome location of known genes were queried from the table of genomic features of human reference genome assemble version 38, maintained by Genome Reference Consortium (GRCh38). An extra 5k flanking region were attached to both ends of a gene when group vairants in a testing unit. Despit the added flanking region, some unit contains no genomic variant, and they were excluded from further opertations. At the end, there are $40,039$ primary and alternative gene assembles eligible for the subsequent study.

\subsection{Implementation}
The proposed method is based on the joint GMSU statistic $U_J$, formulated with three kernel functions corresponding to the three profiles. To test its robustness and versatility, we simulated continuous and binary phenotypes from the effect of purely genomic base, purely vertex base and mixed. Under these scenarios, the statistical power of $U_J$ is compared with two parsimonious statistics $U_G$ and $U_V$ whose null hypothesis is more specific but less flexible.
The method also intents to replace the original testing unit of the cortex profile with high order features abstracted from it, by training a 4 layered stacked autoencoder with all 806 sample profiles. The newly derived U statistics are compared with the one relying on the original vertices.

Lastly, we pick out 327 individual whose real diagnosis status is certain (either healthy control or Alzheimer's disease/Dementia case) and apply the proposed method to screen for associations among diagnosis, genes and cortex regions.