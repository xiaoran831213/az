\section{Method}

\subsection{Generalized Multivariate Similarity U Statistic}
The goal of our method is to jointly test possible association existing among the genomic, cortex and the phenotype profiles using the generalized similarity U statistic (GMSU) {\cite{UST1, UST2}. To derive the GMSU, three kernel functions are chosen for each profile accordingly. A kernel function measure the similarity between a pair of samples with respect to one of the profiles. Depending on the charisteristics of that profile, the exact form of the kernal functions are flexible, as long as thay are symmetric and have finite second moment. That is, the similarity measurement $f$ is a valid U kernal function, if $f(x_i,x_j)=f(x_j,x_i)$ and $E(f^2(X_1, X_2))<+\infty$ are satisfied.
\newcommand{\vg}{\boldsymbol{g}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\vy}{\boldsymbol{y}}
\newcommand{\vG}{\boldsymbol{G}}
\newcommand{\vV}{\boldsymbol{V}}
\newcommand{\vY}{\boldsymbol{Y}}
\newcommand{\vq}{\boldsymbol{q}}
For genomic variants taking values from discrate minor allele count ${0, 1 \textrm{ and } 2}$, a fairly common choice of similarity mesurement is the identical by state (IBS) kernel function
\label{eq:wSG}
\[ f_G(\vg_{i.}, \vg_{j.}) = \frac{\sum_{m=1}^{|vG|}{w_m(2 - |g_{im} - g_{jm}|)}} {2\sum_{m=1}^{|vG|}{w_m}}, \]
where $g_{im}$ and $g_{jm}$ is the value of $m$ th. variant in the testing unit (e.g. a gene) taken from the $i$ th. and $j$ th. samples, respectively, and $|vG|$ is the dimensionality of that testing unit (e.g. number of polymorphism in that gene). $w_m$ is the weight assigned to the $m th.$ variant according to \textit{a prior} hypothesis, an example is the minor allele frequency (MAF) based $w_m=\frac{1}{\sqrt{MAF(g_{.m})(1-MAF(g_{.m}))}}$ which emphasize more on rare variants. Without prior knowledge though, the IBS kernal is simplifed to $\frac{\sum_{m = 1}^{|G|}{(2-|g_{im} - g_{jm}|)}}{2|G|}$ by setting $w_m \equiv 1$.

For cortex profiles whose variants (the vertices) taking continuous values in $[0,1]$, the euclidian distance based kernel function
\label{eq:wSV}
\[ f_V(v_{i.},v_{j.}) = \exp{ [-\frac{\sum_{m=1}^{|V|}{w_m(u_{im}-u_{jm})^2}} {\sum_{m=1}^{|V|}{w_m}}] } \]
is used to measure the simiarity between sample $i$ and $j$, which is also called a Gaussian kernel function. Here $v_{im}$ and $v_{jm}$ are values of the $m$ th. vertex in the cortical testing unit of the $i$ th. and $j$ th. sample, respectively, and $|V|$ denotes the number of vertices in the testing unit. The vertices can also be weighted by the vector $\boldsymbol{w}.$, but for now we have no prior knowledge of the relative importance of the vertices, the Gaussian kernal function is thus simplifed to $\exp{[-\frac{\sum_{m=1}^{|V|}{(v_{im}-v_{jm})^2}} {|V|}]}$.

Lastly, for a multivariate phenotype profile whose elements may be drawn from a variety of unknown distributions, we first normalize its elements with the rank normal quantile function
\[ \vq_{.m} = \frac{\Phi^{-1}[rank(\vy_{.m}) - 0.5)]}{N}, \quad m = 1 \dots |Y| \]
where $\vy_{.m}$ is the $m$ th. element of the phenotype profile, $|Y|$ is the dimensionality of the phenotype (i.e. number of elements), and $N$ is the number of samples. Doing so not only corrects skewed elements, but also bypass the complication of admixed distribution types introduced by such a multivariate phenotype. As a result, the pairwise similarity with regard to phenotype can also be measured by a Gaussian kernel function
\[ f_Y(\vq_{i.}, \vq_{j.}) = \exp{[-\frac{\sum_{m=1}^{|Y|}{w_m(q_{im}-q_{jm})^2}} {\sum_{m=1}^{|Y|}{w_m}}]} \]
where $q_{im}$ is the values of the $m$ th. element of the normalized phenotype profile of the $i$ th. sample, with weight $w_m$ denoting the ralative importance of the $m$ th. phenotype element. For a phenotype with only one dimension, that is, $|Y|=1$, the measurement simplifies to $\exp{[-(q_i - q_j)^2]}$.

All three kernel functions must be centralized, which is done by substracting the function value at each pair $(i,j)$ with the two marginal mean of all pairs involving $i$ and $j$, respectively, then adding the mean of all pairs to it \cite{UST1}. Taking the kernal function of genomic profile as an example, the centralized similarity measurement is
\begin{align*}
  \tilde{f}_G(\vg_{i.}, \vg_{j.})
  &= f_G(\vg_{i.}, \vg_{j.})-\frac{1}{N} \sum_{k=1}^N{f_G(\vg_{i.}, \vg_{k.})}-\frac{1}{N}\sum_{l=1}^N{f_G(\vg_{l.}, \vg_{j.})} \\ 
  &+ \frac{1}{N^2} \sum_{k=1}^{N} \sum_{l=1}^{N} {f_G(\vg_{l.}, \vg_{k.})}
\end{align*}
where $N$ is number of samples. 

Finally, the generalized multivariate similarity U statistics is the mean product of three similarity measurement of all pairs except the self-pairs, which is
\[ U_J = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_G(\vg_{i.}, \vg_{j.}) \tilde{f}_V(\vv_{i.}, \vv_{j.}) \tilde{f}_Y(y_i, y_j). \]
Under the null hypothesis that no correlation exists among all three profiles, the mean product of all pairs of similarity measurement should be $0$ since all three kernel functions are centralized. If the value of $U_J$ significantly deviates from $0$, it means the similarity regarding one profile is related to the similarity regarding one or more other profiles, which implies the presence of association. Under the null, $U_J$ follows a mixture of $\chi_1^2$ distrubtion, the $p$ value can be calculated using Davis method \cite{UST1, UST2}.

By dropping the kernal function of cortex vertices $f_V(\vv_{i.}, \vv_{j.})$, the simplified similarity U statistic
\[ U_G = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_G(\vg_{i.}, \vg_{j.}) \tilde{f}_Y(y_i, y_j) \]
tests the more specific null hypothesis that no association exists between the phenotype and genomic profiles. Likewise, by dropping the kernel function $f_G(\vg_{i.}, \vg_{j.})$, 
\[ U^V = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_V(\vv_{i.}, \vv_{j.}) \tilde{f}_Y(y_i, y_j) \]
test the null hypothesis that no association exists between the phenotype and the cortex profiles.

\subsection{Stacked Autoencoder}
The stacked autoencoder is an artificial neural network mimicking sentimental visual processing which abstracts high order features from the raw image input. The high order feature not only has lower dimensionality, but is also more relevent to decision making. Taking our data as an example, being able to see the approximate location and size of the laceration sites in the cortex, is far more important than knowning the exact thickness, curvature and coordinates of every vertex in the raw profile. Thus, besides dimension reduction, we also anticipate a power boost for any similarity U statistics involving cortex profile if the raw vertex data is replaced with abstracted feature.

An SA is formed by layers of autoencoders stacking on top of each other, hence the name "Stacked". What an autoencoder layer does is first linearly recombining the input elements, then applying a non-linear transformation to every element of the recombination. Under most circumstances, the recombination is made lower in dimensionality than the input to ensure feature abstraction and dimension reduction actually happen. The autoencoder at the $i$ th. layer of the stack is has the form
\begin{equation} \label{eq:AE}
  \zEC_i^{d_i} = s(\WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}),
\end{equation}
where $\zEI{i}{i}$ is the layer output and $\zEI{i-1}{i-1}$ is the layer input. They layer input is also the output of the autoencoder from down below, that is, the $i-1$ th. layer in the stack. The linear recombination of input elements is achieved by the cross product between the input vector $\zEI{i}{i}$ and the weight matrix $\WEI{i}{i}{i-1}$ plus the offset $\bEI{i}{i}$. The superscript $d_{i-1}$ and $d_i$ denote the dimensionality of data and structure parameters of the autoencoder layer. As mentioned, to ensure feature abstraction and dimension reduction actually happens, $d_i$ is made smaller than $d_{i-1}$. For our method, a autoencoder layer always halve the dimension of its input, that is, $d_i$ = $d_{i-1}/2$. Lastly, inverse logit function $logit^{-1}(x) = \frac{1}{1+exp(-x)}$ is chosen to non-linearly transform every element in the recombination, that is,
\begin{equation} \label{eq:InvLgt}
    s(\etaEC_i^{d_i})     = [\SGM{\eta_{i1}}, \SGM{\eta_{i2}}, \dots, \SGM{\eta_{d_{id_i}}}]^{\prime},
\end{equation}
where $\etaEC_i^{d_i} = \WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}$ is the linear recombination of the input $\zEI{i-1}{i-1}$; the super script $d_i$ denotes its dimensionality and $k = 1, \dots, d_i$ indexes its $d_i$ elements. Taking a closer look at the elements in the output $\zE{i}{i}$, they are
\[ z_{ik} = \frac{1}{1 + exp($\wEI{ik}{i-1} \zEI{i-1}{i-1}$)} \]
where $k = 1, \dots, d_i$. $z_{ik}$ is an ``S'' shaped of inverse logit function of all elements of the input, a resemblance of the biological activation of the $k$ th. neuron in the $i$ th. layer of visual cortex, when the weighted sum of stimuli from the $d_{i-1}$ neurons in the previous layer, $\wEI{ik}{i-1} \zEI{i-1}{i-1}$, exceeds a threshold $-b_{ik}$. The weight $\wEI{ik}{i-1}$ is the $k$ th. row vector of the weight matrix $\WEI{i}{i}{i-1}$, and the threshold $-b_{ik}$ is the negation of $k$ th. elements of the offest $\bEI{i}{i}$.

An SA of $M$ layers, of $P$ dimensional raw input $\xEC^P$ and $Q$ dimensional output $\yHT^Q$, is assembled by recursively taking the output of the lower autoencoder layer and piping it to the layer above as input, and ensuring the dimensionality of the output at the top is $Q$.
\begin{equation} \label{eq:ES}
  \begin{split}
    \yHT^Q &= \zEI{M}{M} \\
    \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M  }{M  }) \\
    \zEI{M-1}{M-1} &= s(\WEI{M-1}{M-1}{M-2} \zEI{M-2}{M-2} + \bEI{M-1}{M-1}) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{i  }{i  } &= s(\WEI{i  }{i  }{i-1} \zEI{i-1}{i-1} + \bEI{i  }{i  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{2  }{2  } &= s(\WEI{2  }{2  }{1  } \zEI{1  }{1  } + \bEI{2  }{2  }) \\
    \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1  }{1  }) \\
    \zEI{0  }{0  } &= \xEC^P,
  \end{split}
\end{equation}
where $\xEC^P$ is the $P$ dimensional raw input of one individual, which is viewed as the output of the non-existing $0$ th. autoencoder, with $d_0=P$. Reading from bottom to top, the SA gradually abstracts higher order features from the $P$ dimensional raw input $\xEC^P$, until the dimensionality of the output is as low as $d_M=Q$.

The SA thus constructed is worthless without calibration. One must find the set of structure parameters $\pEC=\{\WEC_1, \bEC_1, \WEC_2, \bEC_2, \dots, \WEC_M, \bEC_M\}$ that best represents the body of knowledge regarding the data, which, in our case, is the knowlege of human cortex. Only then the SA is truely capable of abstracting meaningful features out of the raw input instead of haphazardly reducing it into a small but irrelevent output (e.g. a vector of $Q$ random numbers). The ``goodness of abstraction'' can be inferred from the losted detail disagreement between the raw input, $\xEC^P$, and its mirrored self, $\xDC^P$, which is the input reconstructed from $\yHT^Q$, the abstracted high order features. The rationale is that, the disagreement between $\xEC^P$ and $\xDC^P$ measures how badly the restoration resemble the true original, which indirectly tells us how poorly the encoder had performed, because, a superior abstraction should be less likely to obstruct the recovery effort. Thus, the set of parameter that minimize the difference between the orignal $\xEC^P$ and the reconstructed $\xDC^P$ will be considered the optimal configuration of the SA. The calibration guilded by such criteria is called unsupervised training, or unsupervised machine learning. The term ``unsupervised'' states the fact that no external knowledge other than the raw input $\xEC^P$ is needed. Instead of tuning the paramters to appeal a certain problem (e.g. logistic regression aims to maximize the classification accuray), unsuersvied learning encourage the SA to turn itself into an encrypted knowlege of the data of interest. Not requiring labeled data is the greatest strength of unsurpervised learning (e.g. logistic regression requires not just $x$, but pairs of $(x,y)$ to fit regression coefficients), which in turn allows a much larger pool of samples to contribute to the calibration, and, as new samples keep popping up, the SA can be continuously refined, mimicing a sentient being's capabilty of learning new knowlege. In particular to our method, unsupervised learning ensure all 806 samples could contribute their cortex profiles to construct the SA, even if 427 of them cannot enter the joint U statistical analysis due to uncertainties in diagnosis.

The new issue on the table is how to reconstruct the input, that is, a decoder counterpart of the stacked autoencoder is needed. The most nature way to build a decoder is to mirror the encoder structure, thus the decoder will also be a stack of $M$ layers, each layer also performs linear recombination of its input, followed by a non-linear, element-wise transformation, but, the dimensionality change is in exactly reverse order of the SA. By mirrowing the $i$ th. encoder in the SA, the $i$ th. layer in the decoder stack is
\begin{equation*}
  \zDI{i-1}{i-1} = s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}).
\end{equation*}
With the above layer definition, the decoder stack can be assembled in the same way the encoder stack was assembled. Continue with the $M$ layered example SA in \ref{eq:ES}, its decoder counterpart is
\begin{equation} \label{eq:DS}
\begin{split}
  \xDC^P &= \zDI{0}{0} \\
  \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1  }{0  }) \\
  \zDI{1  }{1  } &= s(\WDI{2  }{1  }{2  } \zDI{2  }{2  } + \bDI{2  }{1  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{i-1}{i-1} &= s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{M-2}{M-2} &= s(\WDI{M-1}{M-2}{M-1} \zDI{M-1}{M-1} + \bDI{M-1}{M-2}) \\
  \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M  }{M-1}) \\
  \zDI{M  }{M  } &= \yHT^Q .
\end{split}
\end{equation}
Reading from bottom to top, the decoder gradually adds details back to the abstracted feature $\yHT^Q$, and eventually produce a retored state of the raw input on its top, denoted by $\xDC^P$. The restoration process is reflected, and driven by the dimenality change from $d_M = Q$ to $d_0 = P$, which is in exact reversed order of the SA. Now with both encoder and decoder stacks ready, the complete cycle of encoding and reconstruction is done by treating the top output of the SA (\ref{eq:ES}), that is, the abstracted code $\yHT^P$, as the lowest input of the decoder stack. The combined the structure is
\begin{equation} \label{eq:ED}
\begin{split}
  \xDC^P &= \zDI{0}{0} \\
  \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1}{0  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M}{M-1}) \\
  \zDI{M  }{M  } &= \yHT = \zEI{M}{M} \\
  \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M}{M  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1}{1  }) \\
  \zEI{0  }{0  } &= \xEC^P
\end{split}.
\end{equation}
In addition to structure mirroring, a common strategy to train an stacked  autoencoder is to constrain the weight matrix in a decoder layer to be the transpose of its encoder counterpart, that is, by forcing $\WDI{i}{i-1}{i} \equiv \WEIt{i}{i}{i-1}$, the $i$ th. decoder layer become
\begin{equation} \label{eq:CW}
  \zDI{i-1}{i-1} = s(\WEIt{i}{i}{i-1} \zDI{i  }{i  } + \bDI{i  }{i-1}).
\end{equation}
Our method adopts this strategy, which not only halve the number of parameter to be tuned but also follow the common sense that encoding and decoding are essentially symmetric concept. More importantly, the constraint encourages the optimization to create an optimal SA, not an inferior SA coupled with a superior decoder stack on its top. Afterall, our best interest is the high order features $\yHT$, not the reconstructed input $\xDC$.

The next thing to do is to measure the total disagreement between original profiles and reconstructed profiles for all the samples, which is called the reconstruction loss $L$. For now, the most popular form is cross entrophy
\newcommand{\XECD}{\XEC^{N \times P}}   % encoder input, with dimensions
\newcommand{\XDCD}{\XDC^{N \times P}}   % decoder output, with dimensions
\newcommand{\YHTD}{\YHT^{N \times P}}   % encoder output, with dimensions
\begin{equation} \label{eq:CE}
  L(\XECD, \XDCD) = -\sum_{j=1}^{N}{\sum_{k=1}^P[x_{jk}\log{\tilde{x}_{jk}}+(1-x_{jk})\log(1-\tilde{x}_{jk})]}.
\end{equation}
The two $N \times P$ matrices $\XECD$ and $\XDCD$ store the original and the reconstructed profiles of all $N$ samples, respectively, with each individual sample indexed by $j=(1, \dots, N)$, and the elements in each sample profile indexed by $k = (1, \dots, P)$. One could view the restoration of $\XEC$ from $YHT$ as an array of binary classification problems, with the true probabilities being $\XEC$, the predicted probability being $\XDC$. The reconstruction loss $L$ closely resembles the deviance of a logistic regression analysis which measures of how badly the fitted model reflects the observed reality. With the reconstruction loss $L$ defined, the calibration of SA become a numerical optimization problem
\[ \Par = \min_{\Par} L(\XDCD, \XECD). \]
With the constraint (\ref{eq:CW}) on the weight matrices in the decoder stack, the set of parameters to be tuned is
\[ \Par = \{\WEI{1}{1}{0}, \bEI{1}{1}, \bDI{1}{0}\} \cup \{\WEI{2}{2}{1}, \bEI{2}{2}, \bDI{2}{1}\} \dots \cup \{\WEI{M}{M}{M-1}, \bEI{M}{M}, \bDI{M}{M-1}\}, \]
whose size is $|\Par| = \sum_{i=1}^M{d_i d_{i-1}} + \sum_{j=1}^M{(d_j + d_{j-1})}$, which is $\sum_{i=1}^M{d_i d_{i-1}}$ parameters less then the non-constrained decoder. The optimization procedure is is done by gradient guided iterative algorithm, which is covered in the appendix section.

The optimization is computational intense, because the number of parameters $|\Par|$ is usually large. When the number of layers in the encoder -- decoders stacks is also large, the computation use to be inhibitively hard, because with the same number of allowed parameters, the complexity of the function represented by a network grows exponentially with the number of layers, that is, a deeper SA has more local minimum in the parameter space for the reconstruction loss $L$ to fall into. Yet, complex function also stands for high flexibility, which makes deeper network enormously intrigging, since a exponentially richer funtion space means a much better chance to find a network that could further reduce $L$ and at the same time produce even more compact abstraction. Deep artificial neuro networks have revived its popularity in recent years, thanks to the break through in its training procedure, which is now popularly dubbed ``deep learning''. For our method, we implement the layer-wise greedy pre-training procedure \cite{DL:DBN1, DL:SDA1}. The idea is to first train each encoder layer separately, then fine tune of the entire structure afterwards. To perform the layer-wise pre-training, the output of $i$ th. autoencoder $\zEC_i$ is not sent to the $i+1$ th. autoencoder like (\ref{eq:ED}) does, but instead is redirected to its decoder counterpart immediately to produce an intermidiate reconstruction $\zDC_i$, 
\begin{equation}\label{eq:Greedy}
  \begin{split}
    \zDC_{i-1} &= s(\WDC_i \zDC_i + \bDC_i) \\
    \zDC_{i  } &= \yHT_i = \zEC_i \\
    \zEC_{i  } &= s(\WEC_i\zEC_{i-1} + \bEC_i) \\
  \end{split}
\end{equation}
where $i = 0 \dots M$. The $i$ th. encoder--decoder tuple can be trained by minimizing the local reconstruction loss $L_i=L(\zEC_{i-1}, \zDC{i-1})$, which is fairly easy thanks to the small number of parameters in $\Par_i=\{\WEI{i}{i}{i-1}, \bEI{i}{i}, \bDI{i}{i-1}\}$. A total of $M$ such tuples is formed and trained separately. What the greedy layer-wise pre-training has done is non-randomly initialize the entire structure to a state closer to optimum. After pre-training, all the encoders and the decoders are wired together like \ref{eq:ED} and fine tuned together. The comprehensive fine tuning will reach convergence much faster and less likely to fall into a poor local minimum then a direct training scheme without the pre-training. 

\subsection{Implementation}
The method we propose will be based on the joint GMSU statistic $U_J$ formulated with three kernel functions corresponding to the three profiles. To test the rubostness and versatility of the proposed method, we simulated continuous and dichotomous phenotypes from the effect of purely genomic, purely vertex and mixed basis. Under these scenarios, the statistical power of $U_J$ is compared with simplified $U_G$ and $U_V$ whose null hypothesis is more specific but supposedly less flexible.

The method also suggests replacing the vertices in the cortex profile with high order features abstracted from them through the training of a stacked autoencoder (SA). In every iteration of the aforementioned simulation scenarios, all 806 cortex sample profiles are utilized to train a 4 layered SA, which is used to replace the original profile with the abstracted features whose dimensionality is 16 times smaller. The newly derived U statistics are compared with the one relying on the original vertices.

Lastly, we pick out 327 individual whose real diagnosis status is certain (either healthy control or Alzheimer's disease/Dementia case) and apply the proposed method to screen for associations among diagnosis, genes and cortex regions.