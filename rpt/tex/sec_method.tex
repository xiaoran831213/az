\section{Method}

\subsection{Generalized Multivariate Similarity U Statistic}
The goal of our method is to jointly test the possible association among the genomic, cortex and the phenotype profiles. The mothod in mind must be robust, versatile and fast in order to minimize the power lost due to model mis-specification, to avoid the bias due to strong distribution assumptions, and to limit the time need for screening a large number of high dimensional combinations. In this study we implement the Generalized Multivariate Similarity U statistic (GMSU) postulated by Changshuai et. al. \cite{HWU}. GMSU is computationally efficient, and highly flexible since no distribution assumption will be imposed on genomic and cortex profiles, together with a rank based normalization procedure, the U statistic is also invulnerable to multidimensional phenotype whose elements come from a mixture of unknown distributions.
To derive the generalized similarity U statistic, three kernel functions are chosen for each of the profiles accordingly. A kernel function measure the similarity between a pair of samples with respect to one of the profiles. Depending on the charisteristics of that profile, the exact form of kernal function can be flexible, as long as it is symmetric and has finite second moment. Thus, the similarity measurement $f$ is a valid U kernal function if $f(x_i,x_j)=f(x_j,x_i)$ and $E(f^2(X_1, X_2))<+\infty$ are satisfied. For the current stuty, the kernel functions are chosen according to common practices.
\newcommand{\vg}{\boldsymbol{g}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\vy}{\boldsymbol{y}}
\newcommand{\vG}{\boldsymbol{G}}
\newcommand{\vV}{\boldsymbol{V}}
\newcommand{\vY}{\boldsymbol{Y}}
\newcommand{\vq}{\boldsymbol{q}}
For genomic variants taking values from discrate minor allele count ${0, 1 \textrm{ and } 2}$, the common choice of similarity mesurement is the identical by state (IBS) kernel function
\label{eq:wSG}
\[ f_G(\vg_{i.}, \vg_{j.}) = \frac{\sum_{m=1}^{|vG|}{w_m(2 - |g_{im} - g_{jm}|)}} {2\sum_{m=1}^{|vG|}{w_m}}, \]
where $g_{im}$ and $g_{jm}$ is the value of $m$ th. variant in the testing unit (e.g. a gene) taken from the $i$ th. and $j$ th. samples, respectively, and $|vG|$ is the dimensionality of the testing unit (e.g. number of polymorphism in a gene). $w_m$ is the weight assigned to the $m th.$ variant according to \textit{a prior} hypothesis, an example is the minor allele frequency (MAF) based $w_m=\frac{1}{\sqrt{MAF(g_{.m})(1-MAF(g_{.m}))}}$ which gives more emphasize on rare variants. Without prior knowledge though, the IBS kernal is simplifed to $\frac{\sum_{m = 1}^{|G|}{(2-|g_{im} - g_{jm}|)}}{2|G|}$ by setting $w_m \equiv 1$.

For cortex profiles whose variants (the vertices) take continuous values within $[0,1]$, we use the euclidian distance based kernel function
\label{eq:wSV}
\[ f_V(v_{i.},v_{j.}) = \exp{ [-\frac{\sum_{m=1}^{|V|}{w_m(u_{im}-u_{jm})^2}} {\sum_{m=1}^{|V|}{w_m}}] } \]
to measure the simiarity between sample $i$ and $j$, which is also called a Gaussian kernel. Here $v_{im}$ and $v_{jm}$ are values of the $m$ th. vertex in the cortex testing unit of the $i$ th. and $j$ th. sample, respectively, and $|V|$ denote the number of vertices in the testing unit. The vertices in the cortex profile can also be weighted by vector $\boldsymbol{w}.$, but for now we have no prior knowledge of the relative importance of the vertices, the Gaussian kernal function is simplifed to $\exp{[-\frac{\sum_{m=1}^{|V|}{(v_{im}-v_{jm})^2}} {|V|}]}$.

Lastly, for a multivariate phenotype profile whose elements may be drawn from a variety of unknown distributions, we first normalize its elements with the rank normal quantile function
\[ \vq_{.m} = \frac{\Phi^{-1}[rank(\vy_{.m}) - 0.5)]}{N}, \quad m = 1 \dots |Y| \]
where $\vy_{.m}$ is the $m$ th. element of the phenotype profile, $|Y|$ is the dimensionality of the phenotype (i.e. number of elements), and $N$ is the number of samples. Doing so not only corrects skewed elements, but also bypass the complication of admixed distribution types introduced by a multivariate phenotype. As a result, the pairwise similarity with regard to phenotype can also be measured by a Gaussian kernel function
\[ f_Y(\vq_{i.}, \vq_{j.}) = \exp{[-\frac{\sum_{m=1}^{|Y|}{w_m(q_{im}-q_{jm})^2}} {\sum_{m=1}^{|Y|}{w_m}}]} \]
where $q_{im}$ is the values of the $m$ th. element of the normalized phenotype profile of the $i$ th. sample, again with weight $w_m$ denoting the ralative importance of every phenotype element. For a phenotype with only one dimension, that is, $|Y|=1$, the above measurement simplifies to $\exp{[-(q_i - q_j)^2]}$.

All three kernel functions must be centralized, which is done by substracting the function value at each pair $(i,j)$ with the two marginal mean of all pairs involving $i$ and $j$, respectively, then adding the overall mean of all pairs to it \cite{HWU}. Taking the kernal function of genomic profile as an example, the centralized similarity measurement is
\begin{align*}
  \tilde{f}_G(\vg_{i.}, \vg_{j.})
  &= f_G(\vg_{i.}, \vg_{j.})-\frac{1}{N} \sum_{k=1}^N{f_G(\vg_{i.}, \vg_{k.})}-\frac{1}{N}\sum_{l=1}^N{f_G(\vg_{l.}, \vg_{j.})} \\ 
  &+ \frac{1}{N^2} \sum_{k=1}^{N} \sum_{l=1}^{N} {f_G(\vg_{l.}, \vg_{k.})}
\end{align*}
where $N$ is number of samples. 

Finally, the generalized multivariate similarity U statistics is the mean product of three similarity measurement of all pairs except the self-pairs, which is
\[ U_J = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N} \tilde{f}_G(\vg_{i.}, \vg_{j.}) \tilde{f}_V(\vv_{i.}, \vv_{j.}) \tilde{f}_Y(y_i, y_j). \]
Under the null hypothesis that no correlation exists among all three profiles, the mean product of all pairs of similarity measurement should be $0$ since all three kernel functions are centralized. If the value of $U_J$ significantly deviates from $0$, the null hypothesis should be rejected and some association among the three profiles is detected. Under the null, $U_J$ follows a mixture of $\chi_1^2$ distrubtion, the $p$ value can be calculated using Davis method \cite{HWU}.

By dropping the kernal function of cortex vertices $f_V(\vv_{i.}, \vv_{j.})$, the simplified similarity U statistic
\[ U_G = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N} \tilde{f}_G(\vg_{i.}, \vg_{j.}) \tilde{f}_Y(y_i, y_j) \]
tests the more specific null hypothesis that no association exists between the phenotype and genomic profiles. Likewise, by dropping the kernel function $f_G(\vg_{i.}, \vg_{j.})$, 
\[ U^V = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N} \tilde{f}_V(\vv_{i.}, \vv_{j.}) \tilde{f}_Y(y_i, y_j) \]
test the null hypothesis that no association exists between the phenotype and the cortex profiles.

\subsection{Stacked Autoencoder}
The stacked autoencoder is an artificial neural network mimicking sentimental visual processing, its purpose is to abstract high order features from the raw image profile. The high order feature not only has lower dimensionality, but is also more relevent to decision making. Taking our data as an example, being able to see the approximate location and size of the laceration sites in the cortex, is far more important than knowning the exact thickness, curvature and coordinates of every vertex in the raw profile. Thus, besides dimension reduction, we also anticipate a power boost for any similarity U statistics involving cortex profile if the raw vertex data is replaced with abstracted feature.

An SA is formed by layers of autoencoders stacking on top of each other, hence the name "Stacked". An autoencoder layer performs a linear recombination of the input elements, followed by an element-wise non-linear transformation. Usually, we make sure the output has a lower dimensionality then the input to ensure feature abstraction and dimension reduction. The autoencoder at the $i$ th. layer of the stack is written as:
\begin{equation} \label{eq:AE}
    \zEC_i^{d_i} = s(\WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}) \\
\end{equation},
where $\zEI{i}{i}$ is the layer output and $\zEI{i-1}{i-1}$ is the layer input, which is also the output of the autoencoder from down below, that is, the $i-1$ th. layer in the stack. The cross product between the input vector $\zEI{i}{i}$ and the weight matrix $\WEI{i}{i}{i-1}$ followed by the addition of the offset $\bEI{i}{i}$ achieves the linear recombination of input elements. The superscript $d_{i-1}$ and $d_i$ denote the dimensionality of data and structure parameters of the autoencoder layer. As mentioned before, to ensure feature abstraction and dimension reduction actually happens, $d_i$ is made smaller than $d_{i-1}$. For our method, a autoencoder layer always halve the dimension of its input, that is, $d_i$ = $d_{i-1}/2$. Lastly, the inverse logit is chosen for the elementwise non-linear transformation, thus
\begin{equation} \label{eq:InvLgt}
    s(\etaEC_i^{d_i})     = [\SGM{\eta_{i1}}, \SGM{\eta_{i2}}, \dots, \SGM{\eta_{d_{id_i}}}]^{\prime},
\end{equation}
where $\etaEC_i^{d_i} = \WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}$ is the linear recombination of the input $\zEI{i-1}{i-1}$; the super script $d_i$ denotes its dimensionality and $k = 1, \dots, d_i$ indexes its $d_i$ elements. The "S" shaped inverse logit curve $\SGM{\eta_{ik}}$ resembles the biological activation of the $k$ th. neuron in the $i$ th. layer of visual cortex when the weighted sum of simulations from all $d_{i-1}$ neurons in the previous layer, $\zEI{i-1}{i-1}$, exceeds a threshold. The weight is taken from the $k$ th. row vector of $\WEI{i}{i-1}{i}$,  and the threshold is the negation of $k$ th. elements in the offest vector $\bEI{i}{i}$.

An SA of $M$ layers, of $P$ dimensional raw input $\xEC^P$ and $Q$ dimensional output $\yHT^Q$, is assembled by recursively taking the output of the lower autoencoder layer as the input of the layer above, and ensuring the dimensionality of the output at the top is $Q$.
\begin{equation} \label{eq:ES}
  \begin{split}
    \yHT^Q &= \zEI{M}{M} \\
    \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M  }{M  }) \\
    \zEI{M-1}{M-1} &= s(\WEI{M-1}{M-1}{M-2} \zEI{M-2}{M-2} + \bEI{M-1}{M-1}) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{i  }{i  } &= s(\WEI{i  }{i  }{i-1} \zEI{i-1}{i-1} + \bEI{i  }{i  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{2  }{2  } &= s(\WEI{2  }{2  }{1  } \zEI{1  }{1  } + \bEI{2  }{2  }) \\
    \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1  }{1  }) \\
    \zEI{0  }{0  } &= \xEC^P,
  \end{split}
\end{equation}
where $\xEC^P$ is the $P$ dimensional raw input of one individual, which is viewed as the output of non-existing $0$ th. autoencoder, with $d_0=P$. Reading from bottom to top, the SA gradually abstracts higher order features from the $P$ dimensional raw input $\xEC^P$, until the dimensionality of the output is as low as $d_M=Q$.

The SA thus constructed is worthless without calibration. One must find the set of structure parameters $\pEC=\{\WEC_1, \bEC_1, \WEC_2, \bEC_2, \dots, \WEC_M, \bEC_M\}$ that best represents the body of knowledge regarding the data, which, in our case, is the knowlege of human cortex. Only then the SA is truely capable of abstracting meaningful features out of the raw input instead of haphazardly reducing it into a small but irrelevent output (e.g. a vector of $Q$ random numbers). The ``goodness of abstraction'' can be inferred from the disagreement between the raw input, $\xEC^P$, and its mirrored self, $\xDC^P$, which is the input reconstructed from $yHT^Q$, the abstracted high order features. The rationale is that, the disagreement between $\xEC^P$ and $\xDC^P$ measures how badly the restoration resemble the true original, which indirectly tells us how poorly the encoder had performed, because, a superior abstraction should be less likely to obstruct the recovery effort. Thus, the set of parameter that minimize the difference between the orignal $\xEC^P$ and the reconstructed $xDC^P$ will be considered the optimal configuration of the SA. The calibration guilded by such criteria is called unsupervised training, or unsupervised machine learning. The term ``unsupervised'' states the fact that no external knowledge other than the raw input $xEC^P$ is needed. Instead of tuning the paramters to appeal a certain problem (e.g. logistic regression aims to maximize the classification accuray), unsuersvied learning encourage the SA to manifest itself into an encrypted knowlege of the data of interest, which, in our case, is the knowledge of the human cortial surface. Not requiring labeled data is the greatest strength of unsurpervised learning (e.g. logistic regression requires not just $x$, but pairs of $(x,y)$ to fit $\beta$), which in turn enables a much larger number of sample to contribute to the calibration, and, when new sample become avaible, the training can continue on without reset, mimicing the acceptance of new knowlege. In particular to our method, unsupervised learning make sure all 806 samples could contribute their cortex profiles to construct the SA, even if 427 of them cannot enter the joint U statistical analysis because their uncertain disease diagnosis at the baseline.

The new issue on the table is the reconstruction of input, that is, a decoder counterpart of the SA is needed. The most nature way to build the decoder do so is to mirror the encoder structure, so the decoder will also be a stack of unit layers, the number of layers is the same with the SA, each layer also performs linear recombination of its input, followed by a non-linear, element-wise transformation, but, the dimensionality change is in exactly reverse order of the SA. By mirrowing the $i$ th. encoder in the SA, the $i$ th. layer in the decoder stack is
\begin{equation*}
  \zDI{i-1}{i-1} = s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}).
\end{equation*}
 With the layer definition being done, the decoder stack can be assembled in the same way of the SA. Continue with the $M$ layered encoder in \ref{eq:ES}, its decoder counterpart is
\begin{equation} \label{eq:DS}
\begin{split}
  \xDC^P &= \zDI{0}{0} \\
  \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1  }{0  }) \\
  \zDI{1  }{1  } &= s(\WDI{2  }{1  }{2  } \zDI{2  }{2  } + \bDI{2  }{1  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{i-1}{i-1} &= s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{M-2}{M-2} &= s(\WDI{M-1}{M-2}{M-1} \zDI{M-1}{M-1} + \bDI{M-1}{M-2}) \\
  \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M  }{M-1}) \\
  \zDI{M  }{M  } &= \yHT^Q .
\end{split}
\end{equation}
Reading from bottom to top, the decoder gradually adds details back to the abstracted feature $\yHT^Q$, and eventually produce a retored state of the raw input on its top, denoted by $\xDC^P$. The restoration process is reflected, and driven by the dimenality change from $d_M = Q$ to $d_0 = P$, which is in exact reversed order of the SA. Now with both encoder and decoder stacks ready, the complete cycle of encoding and reconstruction is done by treating the top output of the SA (\ref{eq:ES}), that is, the abstracted code $yHT^P$, as the lowest input of the decoder stack. The combined the structure is
\begin{equation} \label{eq:ED}
\begin{split}
  \xDC^P &= \zDI{0}{0} \\
  \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1}{0  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M}{M-1}) \\
  \zDI{M  }{M  } &= \yHT = \zEI{M}{M} \\
  \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M}{M  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1}{1  }) \\
  \zEI{0  }{0  } &= \xEC^P
\end{split}.
\end{equation}
In addition to structure mirroring, a common strategy to train an stacked  autoencoder is to constrain the weight matrix in a decoder layer to be the transpose of its encoder counterpart, that is, by forcing $\WDI{i}{i-1}{i} \equiv \WEIt{i}{i}{i-1}$, the $i$ th. decoder layer become
\begin{equation} \label{eq:CW}
  \zDI{i-1}{i-1} = s(\WEIt{i}{i}{i-1} \zDI{i  }{i  } + \bDI{i  }{i-1}).
\end{equation}
Our method adopts this behavior, because doing so almost halve the number of parameter to be tuned, which is a great boost to the computation, besides, the encoder -- decoder tuples follow the common sense that they are essentially symmetric operations. Most importantly, the constraint encourages the calibration of an optimal SA, instead of a inferior SA coupled with a superior decoder partner. Afterall, our best interest is the abstracted high order features, not the reconstructed input.

The next thing to do is to measure the total disagreement between original profiles and reconstructed profiles for all the samples, which is called the reconstruction loss $L$. For now, the most popular form is cross entrophy
\newcommand{\XECD}{\XEC^{N \times P}}   % encoder input, with dimensions
\newcommand{\XDCD}{\XDC^{N \times P}}   % decoder output, with dimensions
\newcommand{\YHTD}{\YHT^{N \times P}}   % encoder output, with dimensions
\begin{equation} \label{eq:CE}
  L(\XECD, \XDCD) = -\sum_{j=1}^{N}{\sum_{k=1}^P[x_{jk}\log{\tilde{x}_{jk}}+(1-x_{jk})\log(1-\tilde{x}_{jk})]}.
\end{equation}
The two $N \times P$ matrices $\XECD$ and $\XDCD$ store the original and the reconstructed profiles of all $N$ samples, respectively, with each individual sample indexed by $j=(1, \dots, N)$, and the elements in each sample profile indexed by $k = (1, \dots, P)$. One could view the restoration of $\XEC$ from $YHT$ as an array of binary classification problems, with the true probabilities being $\XEC$, the predicted probability being $\XDC$. The reconstruction loss $L$ closely resembles the deviance of a logistic regression analysis which measures of how badly the fitted model reflects the observed reality. With the reconstruction loss $L$ defined, the calibration of SA become a numerical optimization problem
\[ \Par = \min_{\Par} L(\XDCD, \XECD). \]
With the constraint (\ref{eq:CW}) on the weight matrices in the decoder stack, the set of parameters to be tuned is
\[ \Par = \{\WEI{1}{1}{0}, \bEI{1}{1}, \bDI{1}{0}\} \cup \{\WEI{2}{2}{1}, \bEI{2}{2}, \bDI{2}{1}\} \dots \cup \{\WEI{M}{M}{M-1}, \bEI{M}{M}, \bDI{M}{M-1}\}, \]
whose size is $|\Par| = \sum_{i=1}^M{d_i d_{i-1}} + \sum_{j=1}^M{(d_j + d_{j-1})}$, which is $\sum_{i=1}^M{d_i d_{i-1}}$ parameters less then the non-constrained decoder. The optimization procedure is is done by gradient guided iterative algorithm, which is covered in the appendix section.

The optimization is computational intense, because the number of parameters $|\Par|$ is usually large. When the number of layers in the encoder -- decoders stacks is also large, the computation use to be inhibitively hard, because with the same number of allowed parameters, the complexity of the function represented by a network grows exponentially with the number of layers, that is, a deeper SA has more local minimum in the parameter space for the reconstruction loss $L$ to fall into. Yet, complex function also stands for high flexibility, which makes deeper network enormously intrigging, since a exponentially richer funtion space means a much better chance to find a network that could further reduce $L$ and at the same time produce even more compact abstraction. Deep artificial neuro networks have revived its popularity in recent years, thanks to the break through in its training procedure, which is now popularly dubbed ``deep learning''. For our method, we implement the layer-wise greedy pre-training procedure \cite{DL:DBN1, DL:SDA1}. The idea is to first train each encoder layer separately, then fine tune of the entire structure afterwards. To perform the layer-wise pre-training, the output of $i$ th. autoencoder $\zEC_i$ is not sent to the $i+1$ th. autoencoder like (\ref{eq:ED}) does, but is instead redirected to its decoder counterpart immediately to produce the intermidiate reconstruction $zDC_i$, the encoder--decoder tuple can then be easily trained by minimizing the local reconstruction loss $L_i=L(\zEC_{i-1}, \zDC{i-1})$. The training is easy, becase of the small number of parameters $\Par_i=\{\WEI{i}{i}{i-1}, \bEI{i}{i}, \bDI{i}{i-1}\}$. A total of $M$ such tuples is formed and trained separately like
\newcommand{\ED}[2]
{
  \arraycolsep=1.2pt
  \begin{array}{rcl}
    \zDC_{#1} &=& s(\WDC_{#2}\zDC_{#2} + \bDC_{#2}) \\
    \zDC_{#2} &=& \yHT_{#2} = \zEC_{#2} \\
    \zEC_{#2} &=& s(\WEC_{#2}\zEC_{#1} + \bEC_{#2}) \\
    \Par_{#2} &=& \min_{\Par_#2}L(\zEC_{#1},\zDC_{#1})
  \end{array}
}
\begin{equation}\label{eq:Greedy}
\begin{array}{cc}
  \ED{0}{1} \quad \ED{1}{2} \dots \ED{M-1}{M}
\end{array}
\end{equation}
What the greedy layer-wise pre-training has done is non-randomly initialize the entire structure to a state closer to optimum. After pre-training, all the encoders and the decoders are wired together like \ref{eq:ED} and fine tuned together. The comprehensive fine tuning will reach convergence much faster and less likely to fall into a poor local minimum then a direct training scheme without the pre-training. 

\subsection{Implementation}
The method we propose will be based on the joint GMSU statistic $U_J$ formulated with all three types of profile. To test the rubostness and versatility of the proposed method, we simulated continuous and dichotomous phenotypes from the effect of purely genomic, purely vertex and mixed basis. Under these scenarios, the statistical power of $U_J$ is compared with simplified $U_G$ and $U_V$ whose null hypothesis is more specific.

The method also suggests replacing the vertices in the cortex profile with high order features abstracted from them through the training of a stacked autoencoder (SA). In every iteration of the aforementioned simulation scenarios, all 806 cortex sample profiles are utilized to train a 4 layered SA, which is immediately used to replace the original profile with the abstracted features. The newly derived U statistics are then compared with the original ones to see if the compact feature actually helps the statistics to gain power.

Lastly, we pick out 327 individual whose real diagnosis status is certain (either healthy control or Alzheimer's disease/Dementia case) and apply the proposed method to screen for associations of real diagnosis with the combinations of all genes and 68 cortical regions.