\section{Method}

To simoteniously detect association among genomics, cortical surface and phenotype profiles, a generalized multivariate similarity U statistic was used, postulated by Changshuai et. al. \cite{HWU}. Of all three types profile, the cortical surface has the most numerous variants, with 68 anatomical regions comprised of few hundreds to more than ten thousands of vertices. To deal with the dimensionality issue, a 4 layered Stacked Autoencoder(SA) was trained with a greed layer by layer manner, as suggested by the deep learning literatures. The SA is capable of abstracting high order features from the vertices in the cortical surface, and at the same time reducing its dimenstionality by 16 fold. 
The vertex similarity can then be calculated with the code instead of the orignial. Supposingly, any U statistic involving a vertex similarity weight should enjoy a power boost if the raw verties were to be replaced with the code, because high order features are less suscptable to random noise. to calculate will be cal by piping the encoded vertices can then by derived by summerizing the product of pairwise similarity between subjects, with respect to the aforementioned genetic and phenotipic profiles, and the encoded vertices. 

\subsection{Similarity U Test}
To derive the proposed U statistic, one U-kernel and one or more U-weights must be constructed. A kernal/weight measures the similarity of each pair of samples, with respect to one of the profiles. The measurement funciton can be flexable depending on the distribution of the profile and the hypothesis in mind, as long as it is symmetric and has finite second moment. Thus, function $f$ is a valid U kernal/weight if $f(x_i,x_j)=f(x_j,x_i)$ and $E(f^2(X_1, X_2))<+\infty$ are satisfied. For the three types of profile, three pairwise similarity measurement were chosen according to common practice.

For biallelic genetic variants whose value was taken from minor allele count ${0, 1, 2}$, a common choice is the weighted complement of Manhattan Distance (wMD):
\begin{equation} \label{eq:wSG}
\begin{split}
  S_{ij}^G &= wMD(g_{i.},g_{j.}) \\
  &= \frac{\sum_{m=1}^{|G|}{w_m(2-|g_{im}-g_{jm}|)}} {2\sum_{m=1}^{|G|}{w_m}},
\end{split}
\end{equation}
, which is also called identical by state (IBS), measurement. Here $g_{im}$($g_{jm}$) is the value of the $m th.$ variant (e.g. a SNP, indel, or deletion) in a testing unit (e.g. a gene) for the $i th.$($j th.$) sample; $|G|$ is the dimensionality of the genomic testing unit (e.g. total number of SNPs, indels and deletions in a gene). $w_m$ is the weight assigned to the $m th.$ variant depending on \textit{a prior} hypothesis, one such example is the reversed square root of minor allele frequency which places more emphasize on rarer variants:
\begin{displaymath}w_m=\frac{1}{\sqrt{MAF(g_m)(1-MAF(g_m))}}.\end{displaymath}
Without any \textit{a prior} hypothesis of the relative importance of genomic variants, an unweighted complement of Manhatten Distance (uMD) can be used by forcing $w_m \equiv 1$:
\begin{equation*} \label{eq:uSG}
\begin{split}
  S_{ij}^G &= uMD(g_{i.}, g_{j.}) \\
  &= \frac{\sum_{m = 1}^{|G|}{(2-|g_{im} - g_{jm}|)}}  {2|G|}.
\end{split}
\end{equation*}

For vertices in the reconstructed cortical surface, their values, for now, is taken from  rescaled $[0,1]$ white matter thickness, one could base the similarity measurement on weighted Euclidian Distance (wED):
\begin{equation} \label{eq:wSV}
\begin{split}
  S_{ij}^V &= wED(v_{i.},v_{j.}) \\
  &= \exp
  {
    \Big[-\frac{\sum_{m=1}^{|V|}{w_m(u_{im}-u_{jm})^2}} {\sum_{m=1}^{|V|}{w_m}}\Big]
  },
\end{split}
\end{equation}
, which is also called a Gaussian measurement. Here $v_{im}$($v_{jm}$) is the value of the $m$ th. vertex of the cortical surface testing unit which, for the simulation studies, is roughly a oval shaped region of 512 vertices, for real data analysis, is one of the 68 anatomical regions. $|V|$ is the dimensionality of (i.e. number of vertices in the oval/anatomy region). $w_m$ is the weight assigned to the $m$ th. vertex in the testing unit. Without pre-knowledge regarding the relative importance of the vertices, the similarity measurement is simplifed by equal weighting ($w_m \equiv 1$):
\begin{equation*} \label{eq_uSV}
\begin{split}
  S_{ij}^V &= uED(v_{i.},v_{j.}) \\
  &= \exp
  {
    \Big[-\frac{\sum_{m=1}^{|V|}{(u_{im}-u_{jm})^2}} {|V|}\Big]
  }.
\end{split}
\end{equation*}
Lastly, for a multivariate phenotype profile, first a rank normal quantile normalization were applied to all the components according to \cite{HWU}:
\begin{displaymath}
  q_{im}=\Phi^{-1}[(rank(y_{im})-0.5)/|Y|],
\end{displaymath} 
where $y_{im}$ is the value of the $m th.$ component of the phenotype for the $i th.$ sample, and $|Y|$ is the dimensionality of the phenotype (i.e. number of components). Doing so not only correct skewed components, but also remove the complication of admixed distribution type introduced by a multivariate phenotype. As a result, the phenotype based similarity can be measured in a manner similar to that of cortical surface:
\begin{equation} \label{eq_wSY}
\begin{split}
  S_{ij}^Y &= wED(q_{i.},q_{j.}) \\
  &=\exp
  {
    \Big[-\frac{\sum_{m=1}^{|Y|}{w_m(q_{im}-q_{jm})^2}} {\sum_{m=1}^{|Y|}{w_m}}\Big]
  },
\end{split}
\end{equation}
where $q_{im}$ is the values of the $m th.$ component of the normalized phenotype; the weight $w_m$ is the ralative importance of that dimension. For a case control study of one dimensional phenotype, that is, $|Y|=1$, the similarity measurement simplifys to:
\begin{displaymath}
  S_{ij}^{y}=wED(q_i,q_j)=\exp{[-(q_i-q_j)^2]},
\end{displaymath}
All three types of the similarity are centralized by substracting the raw measurement from every pair $(i,j)$ with two sample means of all measurements involving $i$ and $j$, respectively, then adding the sample mean of all the pairs to it \cite{HWU}. Takeing the weighted genetic similarity as an example, the centralized similarity measurement is:
\begin{equation} \label{eq_wSY}
\begin{split}
  \tilde{S}_{ij}^{G} &= S_{ij}^{G}-E(S_{i.}^{G})-E(S_{.j}^{G})+E(S_{..}^{G}) \\
  &= S_{ij}^{G} - \frac{1}{N}\sum_{k=1}^N{S_{ik}} - \frac{1}{N}\sum_{l=1}^N{S_{lj}} + \frac{1}{N^2}\sum_{k=1,l=1}^{N,N}{S_{kl}},
\end{split}
\end{equation}
where $N$ is number of samples. The same centralization scheme is applied to the other two similarity measurements. The U statistics is the mean of the product of three centralized similarity measurement across all but mirrored pairs:
\begin{displaymath}
  U^{GVY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{G} \tilde{S}_{ij}^{V} \tilde{S}_{ij}^{Y}.
\end{displaymath}
The actual implementation calculated three symmetric $N \times N$ similarity matrices to facilitate later manipulation. The U statistic follows a mixture of $\chi_1^2$ distrubtion, a $p$ value can then be calculated using Davis method \cite{HWU}.

For now, the genomic and vertex similarity are treated U-weight terms, while the phenotype similarity acts as the U-kernel term. In general, the assignment of U-kernal and U-weight terms does not alter the limiting distribution of the final U statistics. More specific association test can be perform by dropping one of the U-weight terms. To see if only the phenotype and genomic profiles are associated, construct the U statistics without vertex similarity term:
\begin{displaymath}
  U^{GY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{G} \tilde{S}_{ij}^{Y}.
\end{displaymath}
Likewise, dropping the genomic similarity to test if only the phenotype and genomic profiles are uncorrelated:
\begin{displaymath}
  U^{VY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{V} \tilde{S}_{ij}^{Y}.
\end{displaymath}

\subsection{Stacked Autoencoder}
The purpose of an encoder is to abstract high order features out of the raw input, which would then be meaningful for decision making. A real life example is vewing a painting: what intrests the viewer is the concept of object or event the painting captures, not the exact colouring at every inch of the canvas. As for the cortical surface profile, being able to see the overall shrinkage and laceration sites in a aged brain, even without knowning the exact volumn of the gray matter or the location and boundery of the sites, is far more important than knowning the exact thickness, curvature and coordinates of at every vertices. An extreamly accurate and objective measurement at every unit of the space is, though highly informative, but remotely helpful for diagnostic purpose without abstraction of high order information from it. The Stacked Autoencoder (SA) to be implemented here is the result of recent development of deep artificial neuro-network, originally inspired by the sentinental processing of visual information. By definition, an abstraction must reduce the complexity of the raw visual information considerablly so the host could possibly make a meaningful accessment, which, in term of the SA, the dimensionality of the vertex data must be reduced. 

The predecessor of the deep encoder is the Multiple Layer Perceptron (MLP), one of the earlist biologically inspired neuronetworks among Artifical Intellegence (AI) framework. An MLP allows one directional flow of data (e.g. a clique of clinical readings) through layers of "neurons" to reach a decision at the top (e.g. disease diagnosis). The neurons are fully connected between two adjecent layers, but not connected at all within a layer or non-adjecent ones. In symbolic form, using superscript $d_.$ to show the dimensionality of the data, the input for layer $\#i$ is the $d_{i-1}$ dimensional output of the layer below ($\#i-1$), denoted by vector $\boldsymbol{z_{i-1}^{d_{i-1}}}$.  

An SA is formed by layers of autoencoders stacked from bottom to top, hence the name "Stacked Autoencoder". An autoencoder represents a linear recombination of the elements of the input from its down below, followed by an element-wise non-linear transformatio. Usually, the recombinatino has a lower dimensionality (i.e. the number of elements) then the input from below. In symbolic form, an autoencoder placed at the $i$ th. layer of an SA can be written as:
\begin{equation} \label{eq:AE}
  \begin{split}
    \boldsymbol{z_i^{d_i}} = s(\boldsymbol{W_i^{d_i\times d_{i-1}}z_{i-1}^{d_{i-1}}} + \boldsymbol{b^{d_i}})\,
  \end{split}
\end{equation}
where $\boldsymbol{z_{i-1}^{d_i-1}}$ is the the input comming from the layer down below -- the output of the $i-1$ th. autoencoder in the stack), while $\boldsymbol{z_i^{d_i}}$ is the output of this autoencoder -- the $i$ th. in the stack. The linear recombination of the elements from the $i-1$ th. layer is achieved by its corss product with $\boldsymbol{W_i^{d_i\times d_{i-1}}}$ and the offest $\boldsymbol{b^{d_i}}$, which resembles the snapsis connection from one layers of neurons to another. Here, The superscript $d_{i-1}$ and $d_i$ denote the dimensionality of the data and the parameters in the $i-1$ and $i$ autoencoder in the stack, respectively. Although not mandotory, $d_i$ is usually chosen to be smaller than $d_{i-1}$ in order to promote a bottom to top feature abstraction and dimension reduction.

The elementwise non-linear transformation, here denoted by $s$, is usually chosen to be the inverse logit function $s(x)=\frac{L}{1+e^{-x}}$. The "S" shaped inverse logit curve resembles the steep biological activation of a neuron, yet its defferentiability on the whole real line $R$ also facilitates gradient gudided numerical optimization. 

An SA of $M$ layers, $P$ dimensional raw input $\boldsymbol{x}$ and $Q$ dimensional high order feature output, is constucted by recursively pluging the output from the autoencoder of lower layer into the one above, and making sure the output of the top layer is $Q$ dimensinal:
\begin{equation} \label{eq:MLP}
  \begin{split}
    \boldsymbol{\hat{y}}=
    \boldsymbol{z_{M  }^{d_{M  }}}         &= s(\boldsymbol{W_{M  }^{d_{M  } \times d_{M-1}}z_{M-1}^{d_{M-1}}}+\boldsymbol{b_{M  }^{d_{M  }}}) \\
    \boldsymbol{z_{M-1}^{d_{M-1}}}         &= s(\boldsymbol{W_{M-1}^{d_{M-1} \times d_{M-2}}z_{M-2}^{d_{M-2}}}+\boldsymbol{b_{M-1}^{d_{M-1}}}) \\
    ... &= ... \\
    \boldsymbol{z_{i  }^{d_{i  }}}         &= s(\boldsymbol{W_{i  }^{d_{i  } \times d_{i-1}}z_{i-1}^{d_{i-1}}}+\boldsymbol{b_{i  }^{d_{i  }}}) \\
    ... &= ... \\
    \boldsymbol{z_{2  }^{d_{2  }}}         &= s(\boldsymbol{W_{2  }^{d_{2  } \times d_{1  }}z_{1  }^{d_{1  }}}+\boldsymbol{b_{2  }^{d_{2  }}}) \\
    \boldsymbol{z_{1  }^{d_{1  }}}         &= s(\boldsymbol{W_{1  }^{d_{1  } \times d_{0  }}z_{0  }^{d_{0  }}}+\boldsymbol{b_{1  }^{d_{1  }}}) \\
    \boldsymbol{z_{0  }^{d_{0  }}}         &= \boldsymbol{x}
  \end{split},
\end{equation}
where $\boldsymbol{x}$ and $\boldsymbol{\hat{y}}$ denote the $P$ dimensional raw input and the final, $Q$ dimensional high order abstraction output, repectively. The vector $\boldsymbol{z_i^{d_i}}$ denotes the intermidiate abstraction output by the $i$ th. autoencoder while the superscript denotes its dimensionality. The $P$ dimensional input $\boldsymbol{x}$ is treated as output of the non-existing $0$ th. autoencoder, denoted by $\boldsymbol{z_0^{d_0}}$, where $d_0=P$. From the bottom to up, the SA should gradually extract higher order features from the $P$ dimensional raw input $x$, until the dimensionality of the abstraction is as low as $d_M=Q$.

An SA thus constructed is still useless without tuning, that is, one must find a set of parameters $\boldsymbol{\theta}=\{\boldsymbol{W_{1:M}},\boldsymbol{b_{1:M}}\}$ that makes the SA turely capable of abstracting meaningful features instead of haphazardly reducing a high data input into a small but irrelevent output (e.g. a vector of $Q$ zeros). Here, the "meaningfulness" of the SA can be gauged by two different ways, for one, a good abstraction should be able to predict the concerned outcome better, turning strategies based on this criteria result in the so called supervised machine learning; another way, is equavelent to say if an abstraction is better, the original input should be readily reconstructed from it more accurately, which results in the unsupervised machine learning. Here "supervised" means the "quality of abstraction" isjudged by ask how close the prediction made by the SA from the raw input is, compared to the external fact from a specific domain of problems other than the input (e.g. gender of that brain), and the SA thus learned will try to extract features be better suited for problems from that domain (e.g. tell the gender of the brain by read the brain surface). The "unsupervised" learning, on the other hand, means the "goodness of abstraction" is solely judged by having the raw input comparing with its reconstructed self, and the SA so tuned will try to extract more general features not specific to a certain problem domain. In this study, the later, unsupervised learning is adopted to explore generic knowledge regarding the human cortical surface, not just for the purpose of diagnosing a certain neuralogical disorder (e.g. Alzheimer's Disease). As a matter of fact, all available T1 MRI samples can be used to enhance the "machines's understanding of human brain", even if more than half of them cannot enter the analysis due to non-definite disease diagnosis.

Prior to unsupervised learning however, one has to come up with a reconstruction scheme -- a decoder counterpart of the stacked autoencoder. The most nature way is to mirror the structure of the SA, that is, the number of layers in the decoder is the same with the SA, each layer is also a linear recombination of its input followed by a element-wise non-linear transformation, but the dimensionality change are exactly in reverse order of the SA. In symbolic form, the decoder can be structured in a similar manner:
\begin{equation} \label{eq:DEC}
\begin{split}
  \boldsymbol{\tilde{x}}=
  \boldsymbol{\tilde{z}_{0  }^{d_{0  }}} &= s(\boldsymbol{\tilde{W}_{1  }^{d_{0  } \times d_{1  }} \tilde{z}_{1  }^{d_{1  }}}+\boldsymbol{\tilde{b}_{0  }^{d_{0  }}}) \\
  \boldsymbol{\tilde{z}_{1  }^{d_{1  }}} &= s(\boldsymbol{\tilde{W}_{2  }^{d_{1  } \times d_{2  }} \tilde{z}_{2  }^{d_{2  }}}+\boldsymbol{\tilde{b}_{1  }^{d_{1  }}}) \\
  ... &= ... \\
  \boldsymbol{\tilde{z}_{i  }^{d_{i  }}} &= s(\boldsymbol{\tilde{W}_{i+1}^{d_{i  } \times d_{i+1}} \tilde{z}_{i+1}^{d_{i+1}}}+\boldsymbol{\tilde{b}_{i  }^{d_{i  }}}) \\
  ... &= ... \\
  \boldsymbol{\tilde{z}_{M-2}^{d_{M-2}}} &= s(\boldsymbol{\tilde{W}_{M-1}^{d_{M-2} \times d_{M-1}} \tilde{z}_{M-1}^{d_{M-1}}}+\boldsymbol{\tilde{b}_{M-2}^{d_{M-2}}}) \\
  \boldsymbol{\tilde{z}_{M-1}^{d_{M-1}}} &= s(\boldsymbol{\tilde{W}_{M  }^{d_{M-1} \times d_{M  }} \tilde{z}_{M  }^{d_{M  }}}+\boldsymbol{\tilde{b}_{M-1}^{d_{M-1}}}) \\
  \boldsymbol{\tilde{z}_{M  }^{d_{M  }}} &= \boldsymbol{\hat{y}}.
\end{split}
\end{equation}
From bottom to up, the decoder gradually restore details from the abstracted features ${\boldsymbol{\hat{y}}}$, and eventually reach a reconstruction of the raw input $\boldsymbol{\tilde{x}}$. The encoder \ref{eq:MLP} is linked from below by plugin its output into the bottom of the decoder through ${\boldsymbol{\hat{y}}}$. The entire encoding -- decoding structure can be written as:
\begin{equation} \label{eq:SDA}
\begin{split}
  \boldsymbol{\tilde{x}}=
  \boldsymbol{\tilde{z}_{0  }^{d_{0  }}} &= s(\boldsymbol{\tilde{W}_{1  }^{d_{0  } \times d_{1  }} \tilde{z}_{1  }^{d_{1  }}}+\boldsymbol{\tilde{b}_{0  }^{d_{0  }}}) \\
  ... &= ... \\
  \boldsymbol{\tilde{z}_{M-1}^{d_{M-1}}} &= s(\boldsymbol{\tilde{W}_{M  }^{d_{M-1} \times d_{M  }} \tilde{z}_{M  }^{d_{M  }}}+\boldsymbol{\tilde{b}_{M-1}^{d_{M-1}}}) \\
  \boldsymbol{\tilde{z}_{M  }^{d_{M  }}} &= \boldsymbol{\hat{y}} = \boldsymbol{z_{M  }^{d_{M  }}} \\
  \boldsymbol{z_{M  }^{d_{M  }}}         &= s(\boldsymbol{W_{M  }^{d_{M  } \times d_{M-1}} z_{M-1}^{d_{M-1}}}+\boldsymbol{b_{M  }^{d_{M  }}}) \\
  ... &= ... \\
  \boldsymbol{z_{1  }^{d_{1  }}}         &= s(\boldsymbol{W_{1  }^{d_{1  } \times d_{0  }} z_{0  }^{d_{0  }}}+\boldsymbol{b_{1  }^{d_{1  }}}) \\
  \boldsymbol{z_{0  }^{d_{0  }}}         &= \boldsymbol{x}.
\end{split}
\end{equation}
The difference between $\boldsymbol{\tilde{x}}$ and $\boldsymbol{x}$ is a rough measure of how badly the recovered input resemble the original, which is also telling how badly the encoder performed, since a truely meaningful abstraction should not obstruct the recovery of details. Viewing the "badness of reconstruction" as a function of the neural network structure parameters $\boldsymbol{\theta} \cup \boldsymbol{\tilde{\theta}}=\{\boldsymbol{W_{1:M}},\boldsymbol{b_{1:M}}\} \cup \{\boldsymbol{\tilde{W}_{1:M}}, \boldsymbol{\tilde{b}_{1:M}}\}$, the entire autoencoder -- decoder stack can be fine tuned by searching the best set of parameters that minimize the "badness". Two most common measurement of the disagreement between $\boldsymbol{\tilde{x}}$ and $\boldsymbol{x}$ are least square error and corss entrophy, the former is more of a tradition, while the later has gained popularity now. In symbolic form, they can be written as:
\begin{equation} \label{eq:SE}
\begin{split}
  L^{SE}(\boldsymbol{\tilde{x}},\boldsymbol{x}) &= \sum_{i=1}^N\sum_{k=1}^Q\frac{1}{Q}(\tilde{x}_{ik}-x_{ik})^2,
\end{split}
\end{equation},
\begin{equation} \label{eq:CE}
\begin{split}
  L^{CE}(\boldsymbol{\tilde{x}},\boldsymbol{x}) &= -\sum_{i=1}^N\sum_{k=1}^Q[x_{ik}\log{\tilde{x}_{ik}}+(1-x_{ik})\log(1-\tilde{x}_{ik})],
\end{split}
\end{equation}
where $i$ indexes the samples, $k$ indexes the elements of the input/reconstruction. The search for the best set of $\boldsymbol{\tilde{x}}$ and $\boldsymbol{x}$ is thus expressed in a numerical optimization problem:
or the soand the tuning based on such while the turning of $\boldsymbol{\theta}$ is called machine learning or training. The "known facts" are tuples of $(x, y)$ representing the observed raw input and true outcome, for example, $x^q$ could be a $M$ item food checklist taken from a study subject, and $y^q$ is a $q$ dimensional binary coding of hypertension, cardiovascular disease, diabetes and non-disease (thus $q=4$ in this case). The hope is, after surficient training with known facts, the MLP could make creditable assessment of unknown future diseases given a new dietary checklist, and the parameters $\boldsymbol{\theta}$ so found will be an encrypted knowledge relating diet to chronic disease. Leaning is done by minimizing the value of an loss function $L$ who measures the wrongfulness of the prediction made by the MLP ($\boldsymbol{\hat{y}^q}$) relative to the actual fact observed ($\boldsymbol{y}$). 
One choice of the total loss of N obervations is the sum of Euclidian Distance (ED) between predictions and true outcomes:

Here $N$ is the number of observed facts, indexed by $i$. The search of the best $\boldsymbol{\theta}$ is a numerical optimiation problem:
\[\boldsymbol{\hat{\theta}}=\min_{\theta}L(\boldsymbol{\hat{y}},\boldsymbol{y})\]
The total number of parameters must be turned tune is $|\boldsymbol{\theta}|=\sum_{i=1}^{M}{(d_{i-1}+1) \times d_i}$. The optimization problem can be computationally intense if $|\boldsymbol{\theta}|$ is huge. For a small $\theta$, Newton–Raphson method can be used for fast convergence. Here we use the gradient decent method, starting with a randomly initialized assignment of $\theta^(0)$ at $t=0$, and compute the next assignment $\theta^{(t+1)}$ by substracting a tiny fraction (called a learning step) of the gradient of loss function $L$ with respect to the current assignment ($\frac{\partial L}{\partial \boldsymbol{\theta^{t}}}$). The process will repeat until the loss $L$ cease to drop marketable between two batch of assignments. The final assignment is considered the optimal $\theta$.

The structure of an encoder is identical to an MLP, what's different is how they are trained. Tuning a MLP required a duel of facts known $a priori$: the predictors $x$ characterizing a subject, and the outcomes $y$ predictable by those characteristics, which is called supervised training. For an SDA, the training can be and is usually unsupervised, that is, knowing the outcome $y$ is only optional. However, tunning an SDA requires a decoder counterpart whose purpose is to reconstruct the  high-dimensional raw input $x$ from the abstrated, low-dimensional code $y$. By piping the original feature $x$ through the encoder for the code $\hat{y}$, then piping the code $\hat{y}$ throught the decoder for a reconstructed $\tilde{x}$, the encoder can then be tuned by minimizing the loss between the true data $x$ and the reconstructed $x'$. The retionel is, a optimal encoder should extrapolate the most significant features of the raw data, so the reconstructed version could resemble the true original at best. After the training with known $x$, the SDA can serve as a dimensionality reduction tool.
A decoder is also a series of linear recombination and offseting, followed by non-linear elementwise transformation, recersively applied to the code $\hat{y}$. The most nature way to structure a decoder is to mirror the encoder. Treating the aforementioned MLP \ref{eq:MLP} as an encoder, one could build a corresponding counterpart by expanding the dimension in exact reverse order:

where the superscript shows the dimensionality; the non-linear elementwise transformation $s$ can again be the reverse-logit. Reading bottom-up, 

To measure the reconstruction loss $L(x,\tilde{x})$, cross-entrophy (CE) can be employed again:
\begin{equation*}
\begin{split}
  L^{CE}(\boldsymbol{x},\boldsymbol{\tilde{x}}) = \\
  -\sum_{i=1}^N\sum_{k=1}^P[x_{ik}\log{\tilde{x}_{ik}}+(1-x_{ik})\log(1-\tilde{x}_{ik})].
\end{split}
\end{equation*}
The parameters to be tuned are $\boldsymbol{\theta} \cup \boldsymbol{\tilde{\theta}}=\{\boldsymbol{W_{1:M}},\boldsymbol{b_{1:M}}\} \cup \{\boldsymbol{\tilde{W}_{1:M}}, \boldsymbol{\tilde{b}_{1:M}}\}$, that is, the the decoder is also implicitly trained. One common practise is to constrain the liner reconbination in the decoder as the tanspose of their encoder counterpart, that is, by forcing $\boldsymbol{\tilde{W}_i}=\boldsymbol{W_i}^{\prime} (i=1 \ldots M)$, which almost halves the number of parameters and fits the intuition better since encoding and decoding are conceptually symmetric. Once the parameters and loss function is decided, training can be done by gradient decent.

A deep encoder is fundermentally an encoder, albeit with much larger number of layers and nodes, allowing input of very high dimension such as medical images and genetic sequences. The increase in depth and input causes some problems. The training tends to be unstable due to the large number of parameters, so one have to use smaller learning step to ensure the loss $L$ always decrease when the training advances. Slow convergence and large number of tuning paramter all adds to computation burden. The deep-learning method was postulated as a counter measure, which is a greedy, layer by layer pre-training process followed by overall fine-tuningby [2008 Vincent]. The idea of layer by layer pre-training is to form and train encoder--decoder by unit. Using similar symbolic form, the $M \time 2$ layered encoder--decoder network can be seperated into $M$ units, written as:
\begin{equation} \label{eq:unit encoder-decoder}
\begin{split}
  \boldsymbol{\tilde{z}_{i-1}^{d_{i-1}}} &= s(\boldsymbol{\tilde{W}_{i  }^{d_{i-1} \times d_{i  }} z_{i  }^{d_{i  }}}+\boldsymbol{\tilde{b}_{i-1}^{d_{i-1}}}) \\
  \boldsymbol{z_{i  }^{d_{i  }}}         &= s(\boldsymbol{W_{i  }^{d_{i  } \times d_{i-1}} z_{i-1}^{d_{i-1}}}+\boldsymbol{b_{i  }^{d_{i  }}}), \\
\end{split}
\end{equation}
where $i=(1 \ldots M)$. Thus, instead of pipping the output of the $i th$ encoder layer ($\boldsymbol{z_{i  }^{d_{i  }}}$) to the one above, it is pipped to the cooresponding $M-i th.$ layer in the docoder counterpart for an immediate reconstruction of the input $\boldsymbol{z_{i-1}}$. The unit encoder--decoder tuple can then be trained by minimizeing the reconstruction loss $L(\boldsymbol{z_{i-1}},\boldsymbol{\tilde{z}_{i-1}})$. A total of $M$ units can be formed and trained seperatetly. Although the pre-training cannot be done in parallel since the higher unit have to wait the lower for input, the process is very fast because the number of parameters is rather small in each unit compared with the whole network. The pre-training serves as a non-random parameter initilization, after which the whole network is already near convergence. Therefore, the subsequent fine-tuning of entire network would required much less steps to reach convergence.

The MLP or encoder thus trained may suffer the overfitting problem, that is, the network performs will on the known facts, producing low prediction/reconstruction loss, but deals poorly for the future, unknown data. To better balance the internal and external validy, one way is to add a regulator term to the loss funciton $L$, usually the $L1$ or $L2$ norm of the parameter elements. Doing so discourages large parameters which causes instability of the prediction/reconstruction. For encoder training, another way to ensure generalizability is to randomly corrupt the input $\boldsymbol{x}$ by setting some elements to zero [2008 Vincent]. The rationel is, the more rigours a feature is, the more like it will survive the random corruption. More training epoch is needed to ensure the randomness of corruption, but doing so encourages the network to seek major features of the input while neglecting the minor changes between facts. And since major feature are more likely to show up in the future, the external validty of the encoder is thus preserved.

For the current study howver, the purpose of deep-learning is not to construct an encoder encrypted with universal knowledge regarding white matter surface, but to produced the abstration the surface vertices at hand for the up-coming analysis. In other words, the code $\hat{y}$ is the only concern here, and the external validty will be implictly handled by the subsequent U-statistical analysis using such code. For now, an ad-hoc encoder trained without regulator term or ramdom corruption would surfice. The current implementation is a five layered encoder, each halves the dimensionality of its input. Thus the final encoding will be 32 times smaller then the raw input.
