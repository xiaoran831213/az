\section{Material and Method}

\subsection{Study Data Source}
The next generation sequencing (NGS) and magnetic resonance imaging (MRI) data were obtained from Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI). A total of 808 subjects at the screening and baseline of ADNI1 and ADNI2 study have both profiles available, alongside with disease diagnosis, demographics, and the genotype of APOE $\epsilon$4. 

The structure MRI data first went through a series of processing including special registration, skull stripping, cortical/sub-cortical segmentation, white/gray matter segregation, voxel intensity normalization, reconstruction of cortex, cortex registration, and cortex paceration. The entire pipeline is implemented by \FS - a neuroimaging analysis package developed by Fisher and Dale et.al. \cite{FS:Intro}, and currently maintained by \textit{the Laboratory for Computational Neuroimaging (LCN)}  at \textit {the Athinoula A. Martinos Center for Biomedical Imaging}. \FS is freely distributed online (\url{http://surfer.nmr.mgh.harvard.edu}). The reconstructed cortex is spanned by $3,276,800$ vertices in 3D space. Each vertex is treated as a image variant, with a number of geometrical attribute attached to it, such as the coordinate of the vertex, the gray matter thickness, average curvature, local area and volume around its vicinity. Currently we took the gray matter thickness as the value of each vertex. The last processing step of \FS -- cortex paceration, divides the cortical surface into 68 anatomical regions. For real data analysis, these regions are taken as testing units, for simulation study, small ovals of $512$ vertices (mean diameter=28mm) were randomly picked from the cortex as testing units. There are 2 samples failed the image processing, the total sample left for the study is 806.

The NGS data has gone through rigorous quality control during variant calling process, thus the NGS data from ADNI do not require intensive processing. The testing units for both real data and simulation study are gene based. The chromosome location of known genes were queried from the table of genomic features of human reference genome assemble version 38, maintained by Genome Reference Consortium (GRCh38). An extra 5k flanking region were attached to both ends of a gene when group variants in a testing unit. Despite the added flanking region, some unit contains no genomic variant, and they were excluded from further operations. At the end, there are $40,039$ primary and alternative gene assembles eligible for the subsequent study.

\subsection{Generalized Multivariate Similarity U Statistic}
The goal of our method is to jointly test possible association existing among the genomic, cortex and the phenotype profiles using the generalized similarity U statistic (GMSU) \cite{UST1, UST2}. To derive the GMSU, three kernel functions are chosen for each profile accordingly. A kernel function measure the similarity between a pair of samples with respect to one of the profiles. Depending on the characteristics of that profile, the exact form of the kernel functions are flexible, as long as they are symmetric and have finite second moment. That is, the similarity measurement $f$ is a valid U kernel function, if $f(x_i,x_j)=f(x_j,x_i)$ and $E(f^2(X_1, X_2))<+\infty$ are satisfied.
\newcommand{\vg}{\boldsymbol{g}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\vy}{\boldsymbol{y}}
\newcommand{\vG}{\boldsymbol{G}}
\newcommand{\vV}{\boldsymbol{V}}
\newcommand{\vY}{\boldsymbol{Y}}
\newcommand{\vq}{\boldsymbol{q}}
For genomic variants taking values from discrete minor allele count ${0, 1 \textrm{ and } 2}$, a fairly common choice of similarity measurement is the identical by state (IBS) kernel function
\label{eq:wSG}
\[ f_G(\vg_{i.}, \vg_{j.}) = \frac{\sum_{m=1}^{|vG|}{w_m(2 - |g_{im} - g_{jm}|)}} {2\sum_{m=1}^{|vG|}{w_m}}, \]
where $g_{im}$ and $g_{jm}$ is the value of $m$ th. variant in the testing unit (e.g. a gene) taken from the $i$ th. and $j$ th. samples, respectively, and $|vG|$ is the dimensionality of that testing unit (e.g. number of polymorphism in that gene). $w_m$ is the weight assigned to the $m th.$ variant according to \textit{a prior} hypothesis, an example is the minor allele frequency (MAF) based $w_m=\frac{1}{\sqrt{MAF(g_{.m})(1-MAF(g_{.m}))}}$ which emphasize more on rare variants. Without prior knowledge though, the IBS kernel is simplified to $\frac{\sum_{m = 1}^{|G|}{(2-|g_{im} - g_{jm}|)}}{2|G|}$ by setting $w_m \equiv 1$.

For cortex profiles whose variants (the vertices) taking continuous values in $[0,1]$, the euclidean distance based kernel function
\label{eq:wSV}
\[ f_V(v_{i.},v_{j.}) = \exp{ [-\frac{\sum_{m=1}^{|V|}{w_m(u_{im}-u_{jm})^2}} {\sum_{m=1}^{|V|}{w_m}}] } \]
is used to measure the similarity between sample $i$ and $j$, which is also called a Gaussian kernel function. Here $v_{im}$ and $v_{jm}$ are values of the $m$ th. vertex in the cortical testing unit of the $i$ th. and $j$ th. sample, respectively, and $|V|$ denotes the number of vertices in the testing unit. The vertices can also be weighted by the vector $\boldsymbol{w}.$, but for now we have no prior knowledge of the relative importance of the vertices, the Gaussian kernel function is thus simplified to $\exp{[-\frac{\sum_{m=1}^{|V|}{(v_{im}-v_{jm})^2}} {|V|}]}$.

Lastly, for a multivariate phenotype profile whose elements may be drawn from a variety of unknown distributions, we first normalize its elements with the rank normal quantile function
\[ \vq_{.m} = \frac{\Phi^{-1}[rank(\vy_{.m}) - 0.5)]}{N}, \quad m = 1 \dots |Y| \]
where $\vy_{.m}$ is the $m$ th. element of the phenotype profile, $|Y|$ is the dimensionality of the phenotype (i.e. number of elements), and $N$ is the number of samples. Doing so not only corrects skewed elements, but also bypass the complication of admixed distribution types introduced by such a multivariate phenotype. As a result, the pairwise similarity with regard to phenotype can also be measured by a Gaussian kernel function
\[ f_Y(\vq_{i.}, \vq_{j.}) = \exp{[-\frac{\sum_{m=1}^{|Y|}{w_m(q_{im}-q_{jm})^2}} {\sum_{m=1}^{|Y|}{w_m}}]} \]
where $q_{im}$ is the values of the $m$ th. element of the normalized phenotype profile of the $i$ th. sample, with weight $w_m$ denoting the relative importance of the $m$ th. phenotype element. For a phenotype with only one dimension, that is, $|Y|=1$, the measurement simplifies to $\exp{[-(q_i - q_j)^2]}$.

All three kernel functions must be centralized, which is done by subtracting the function value at each pair $(i,j)$ with the two marginal mean of all pairs involving $i$ and $j$, respectively, then adding the mean of all pairs to it \cite{UST1}. Taking the kernel function of genomic profile as an example, the centralized similarity measurement is
\begin{align*}
  \tilde{f}_G(\vg_{i.}, \vg_{j.})
  &= f_G(\vg_{i.}, \vg_{j.})-\frac{1}{N} \sum_{k=1}^N{f_G(\vg_{i.}, \vg_{k.})}-\frac{1}{N}\sum_{l=1}^N{f_G(\vg_{l.}, \vg_{j.})} \\ 
  &+ \frac{1}{N^2} \sum_{k=1}^{N} \sum_{l=1}^{N} {f_G(\vg_{l.}, \vg_{k.})}
\end{align*}
where $N$ is number of samples. 

Finally, the generalized multivariate similarity U statistics is the mean product of three similarity measurement of all pairs except the self-pairs, which is
\[ U_J = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_G(\vg_{i.}, \vg_{j.}) \tilde{f}_V(\vv_{i.}, \vv_{j.}) \tilde{f}_Y(y_i, y_j). \]
Under the null hypothesis that no correlation exists among all three profiles, the mean product of all pairs of similarity measurement should be $0$ since all three kernel functions are centralized. If the value of $U_J$ significantly deviates from $0$, it means the similarity regarding one profile is related to the similarity regarding one or more other profiles, which implies the presence of association. Under the null, $U_J$ follows a mixture of $\chi_1^2$ distribution, the $p$ value can be calculated using Davis method \cite{UST1, UST2}.

The joint U statistic $U_J$ is the core of proposed method, to test its robustness under model misspecification, two parsimonious statistics are also calculated. By dropping the kernel function of cortex vertices $f_V(\vv_{i.}, \vv_{j.})$, the simplified similarity U statistic
\[ U_G = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_G(\vg_{i.}, \vg_{j.}) \tilde{f}_Y(y_i, y_j) \]
tests the more specific null hypothesis that no association exists between the phenotype and genomic profiles. Likewise, by dropping the kernel function $f_G(\vg_{i.}, \vg_{j.})$, 
\[ U^V = \frac{2}{N(N-1)} \sum_{1 \leq i < j \leq N}^{} \tilde{f}_V(\vv_{i.}, \vv_{j.}) \tilde{f}_Y(y_i, y_j) \]
test the null hypothesis that no association exists between the phenotype and the cortex profiles.

The method also applies grouping and signal aggregation on cortex profile. For comparison purpose, we also implement the vertex-wise analysis (VWA). Briefly speaking, it first smooth the testing unit with a Gaussian filter of standard deviation 2, which reduces the noise by grinding away trivial details in the cortical surface. Next, it treats $|V|$ vertices in the original testing unit as $|V|$ single dimensional testing unit, based on which $|V|$ $U_J$(or $U_V$) statistics are calculated, and $|V|$ FDR (false discovery rate) adjusted p-values are derived. Should any of the adjusted p-values below 0.05 threshold, the entire testing unit is significantly associated with some other profiles (phenotype, genomic, or both).

\subsection{Stacked Autoencoder}
The stacked autoencoder is an artificial neural network mimicking sentimental visual processing which abstracts high order features from the raw image input. The high order feature not only has lower dimensionality, but is also more relevant to decision making. Taking our data as an example, being able to see the approximate location and size of the laceration sites in the cortex, is far more important than knowing the exact thickness, curvature and coordinates of every vertex in the raw profile. Thus, besides dimension reduction, we also anticipate a power boost for any similarity U statistics involving cortex profile if the raw vertex data is replaced with abstracted feature.

An SA is formed by layers of autoencoders stacking on top of each other, hence the name "Stacked". What an autoencoder layer does is first linearly recombining the input elements, then applying a non-linear transformation to every element of the recombination. Under most circumstances, the recombination is made lower in dimensionality than the input to ensure feature abstraction and dimension reduction actually happen. The autoencoder at the $i$ th. layer of the stack is has the form
\begin{equation} \label{eq:AE}
  \zEC_i^{d_i} = s(\WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}),
\end{equation}
where $\zEI{i}{i}$ is the layer output and $\zEI{i-1}{i-1}$ is the layer input. They layer input is also the output of the autoencoder from down below, that is, the $i-1$ th. layer in the stack. The linear recombination of input elements is achieved by the cross product between the input vector $\zEI{i}{i}$ and the weight matrix $\WEI{i}{i}{i-1}$ plus the offset $\bEI{i}{i}$. The superscript $d_{i-1}$ and $d_i$ denote the dimensionality of data and structure parameters of the autoencoder layer. As mentioned, to ensure feature abstraction and dimension reduction actually happens, $d_i$ is made smaller than $d_{i-1}$. For our method, a autoencoder layer always halve the dimension of its input, that is, $d_i$ = $d_{i-1}/2$. Lastly, inverse logit function $logit^{-1}(x) = \frac{1}{1+exp(-x)}$ is chosen to non-linearly transform every element in the recombination, that is,
\begin{equation} \label{eq:InvLgt}
    s(\etaEC_i^{d_i})     = [\SGM{\eta_{i1}}, \SGM{\eta_{i2}}, \dots, \SGM{\eta_{d_{id_i}}}]^{\prime},
\end{equation}
where $\etaEC_i^{d_i} = \WEI{i}{i}{i-1} \zEI{i-1}{i-1} + \bEI{i}{i}$ is the linear recombination of the input $\zEI{i-1}{i-1}$; the super script $d_i$ denotes its dimensionality and $k = 1, \dots, d_i$ indexes its $d_i$ elements. Upon close look, each element in the output $\zEI{i}{i}$ is calculated as
\[ z_{ik} = \frac{1}{1 + e^{\wEI{ik}{i-1} \zEI{i-1}{i-1}}}, k = 1, \dots, d_i .\]
The $k$ th. elements $z_{ik}$ is an ``S'' shaped function of all elements of the input, which resembles the biological activation of the $k$ th. neuron in the $i$ th. layer of visual cortex when the weighted sum of stimuli from the $d_{i-1}$ neurons in the previous layer, $\wEI{ik}{i-1} \zEI{i-1}{i-1}$, exceeds a threshold $-b_{ik}$. The weight $\wEI{ik}{i-1}$ is the $k$ th. row vector of the weight matrix $\WEI{i}{i}{i-1}$, and the threshold $-b_{ik}$ is the negation of $k$ th. elements of the offset $\bEI{i}{i}$.

An SA of $M$ layers, $P$ dimensional input $\xEC^P$ and $Q$ dimensional output $\yHT^Q$, is assembled by recursively taking the output of the lower autoencoder layer and, as the input, piping it to one layer above, and ensuring the dimension of the topmost output is $Q$.
\begin{equation} \label{eq:ES}
  \begin{split}
    \yHT^Q &= \zEI{M}{M} \\
    \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M  }{M  }) \\
    \zEI{M-1}{M-1} &= s(\WEI{M-1}{M-1}{M-2} \zEI{M-2}{M-2} + \bEI{M-1}{M-1}) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{i  }{i  } &= s(\WEI{i  }{i  }{i-1} \zEI{i-1}{i-1} + \bEI{i  }{i  }) \\
    & \quad \quad \quad \quad \vdots \\
    \zEI{2  }{2  } &= s(\WEI{2  }{2  }{1  } \zEI{1  }{1  } + \bEI{2  }{2  }) \\
    \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1  }{1  }) \\
    \zEI{0  }{0  } &= \xEC^P,
  \end{split}
\end{equation}
where $\xEC^P$ is the $P$ dimensional input of one sample, which is seen as the output of the non-existing $0$ th. autoencoder with $d_0=P$. Reading from bottom to top, the SA gradually abstracts higher order features from the $P$ dimensional raw input $\xEC^P$, until the dimensionality of the output is as low as $d_M=Q$.

The SA thus constructed is useless without calibration, a process that finds a set of structure parameters $\pEC=\{\WEC_1, \bEC_1, \WEC_2, \bEC_2, \dots, \WEC_M, \bEC_M\}$ that best represents the body of knowledge regarding the data, which, in our case, is the knowledge of human cortex. Only then the SA is truly capable of abstracting meaningful features out of the input instead of haphazardly reducing it into a small but irrelevant output (e.g. a vector of $Q$ random numbers). The calibration must be guided by a ``badness of abstraction'' gauge. We first reconstruct the input from the abstracted high order feature $\yHT^Q$, and compare it with true input $\zEC^P$. The disagreement between the reconstructed $\zDC^P$ and the true input $\zEC^P$ tells us how poorly the encoder had performed. The rationale is, a superior abstraction should be easier to recover the input from. Therefore, a set of parameter that lower the difference between $\xEC^P$ and $\xDC^P$ is considered a better configuration of the SA. The calibration guided by such criteria is called unsupervised training, or unsupervised machine learning. The term ``unsupervised'' states the fact that no external knowledge other than the raw input $\xEC^P$ is needed. The unsupervised training encourage the SA to manifest into an encrypted knowledge of the concerned data. Not requiring labeled data is the greatest strength of unsupervised learning, which allows a much larger pool of samples to contribute to its calibration, and, as new samples keep popping up, the SA can be continuously refined, mimicking a sentient being's ability of accepting new knowledge. In particular to our method, unsupervised learning ensure all 806 samples could contribute their cortex profiles to construct the SA, even if 427 of them cannot enter the read data analysis due to uncertainties in disease diagnosis.

The new issue on the table is how to reconstruct the input, that is, a decoder counterpart of the stacked autoencoder is needed. The most nature way to build a decoder is to mirror the encoder structure, thus the decoder will also be a stack of $M$ layers, each layer also performs linear recombination of its input through a weight matrix and a offset vector, followed by a element-wise inverse logit transformation, but, the dimensionality change is in exactly reverse order of the encoder stack. By mirroring the $i$ th. encoder in the SA, the $i$ th. layer in the decoder stack is
\begin{equation*}
  \zDI{i-1}{i-1} = s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}).
\end{equation*}
With the above layer definition, the decoder stack can be assembled in the same way the encoder stack was done. Continue with the $M$ layered example SA in \ref{eq:ES}, its decoder counterpart is
\begin{equation} \label{eq:DS}
\begin{split}
  \xDC^P &= \zDI{0}{0} \\
  \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1  }{0  }) \\
  \zDI{1  }{1  } &= s(\WDI{2  }{1  }{2  } \zDI{2  }{2  } + \bDI{2  }{1  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{i-1}{i-1} &= s(\WDI{i  }{i-1}{i  } \zDI{i  }{i  } + \bDI{i  }{i-1}) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{M-2}{M-2} &= s(\WDI{M-1}{M-2}{M-1} \zDI{M-1}{M-1} + \bDI{M-1}{M-2}) \\
  \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M  }{M-1}) \\
  \zDI{M  }{M  } &= \yHT^Q .
\end{split}
\end{equation}
Reading from bottom to top, the decoder gradually adds details back to the abstracted feature $\yHT^Q$, and eventually produce a restored state of the input on its top, denoted by $\xDC^P$. The restoration process is driven by the dimensionality change from $d_M = Q$ to $d_0 = P$, which is in exact reversed order of the SA. Now with both encoder and decoder stacks ready, the complete cycle of encoding and reconstruction is done by redirecting the top output of the SA (\ref{eq:ES}), that is, the abstracted code $\yHT^P$, as the input, to the lowest layer of the decoder stack. The combined the structure is
\begin{equation} \label{eq:ED}
\begin{split}
  \xDC^P &= \zDI{0}{0} \\
  \zDI{0  }{0  } &= s(\WDI{1  }{0  }{1  } \zDI{1  }{1  } + \bDI{1}{0  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zDI{M-1}{M-1} &= s(\WDI{M  }{M-1}{M  } \zDI{M  }{M  } + \bDI{M}{M-1}) \\
  \zDI{M  }{M  } &= \yHT = \zEI{M}{M} \\
  \zEI{M  }{M  } &= s(\WEI{M  }{M  }{M-1} \zEI{M-1}{M-1} + \bEI{M}{M  }) \\
  & \quad \quad \quad \quad \vdots \\
  \zEI{1  }{1  } &= s(\WEI{1  }{1  }{0  } \zEI{0  }{0  } + \bEI{1}{1  }) \\
  \zEI{0  }{0  } &= \xEC^P
\end{split}.
\end{equation}
In addition to structure mirroring, a common strategy to train an SA is to constrain the weight matrix in a decoder layer to be the transpose of its encoder counterpart, that is, by forcing $\WDI{i}{i-1}{i} \equiv \WEIt{i}{i}{i-1}$, the $i$ th. decoder layer become
\begin{equation} \label{eq:CW}
  \zDI{i-1}{i-1} = s(\WEIt{i}{i}{i-1} \zDI{i  }{i  } + \bDI{i  }{i-1}).
\end{equation}
We adopt this strategy, which not only halve the number of parameter to be calibrated but also follow the common sense that encoding and decoding are essentially symmetric concept. More importantly, this constraint encourages calibration of an optimal SA, not an inferior SA coupled with a superior decoder stack on its top. After all, our best interest is the high order features $\yHT$, not the reconstructed input $\xDC$.

The next thing to do is to measure the total disagreement between original profiles and reconstructed profiles for all the samples, which is called the reconstruction loss $L$. For now, the most popular form is cross entropy
\newcommand{\XECD}{\XEC^{N \times P}}   % encoder input, with dimensions
\newcommand{\XDCD}{\XDC^{N \times P}}   % decoder output, with dimensions
\newcommand{\YHTD}{\YHT^{N \times P}}   % encoder output, with dimensions
\begin{equation} \label{eq:CE}
  L(\XECD, \XDCD) = -\sum_{j=1}^{N}{\sum_{k=1}^P[x_{jk}\log{\tilde{x}_{jk}}+(1-x_{jk})\log(1-\tilde{x}_{jk})]}.
\end{equation}
The two $N \times P$ matrices $\XECD$ and $\XDCD$ store the original and the reconstructed profiles of all $N$ samples, respectively, with each individual sample indexed by $j=(1, \dots, N)$, and the elements in each sample profile indexed by $k = (1, \dots, P)$. One could view the restoration of $\XEC$ from $\YHT$ as an array of binary classification problems, with the true probabilities being $\XEC$, the predicted probability being $\XDC$. The reconstruction loss $L$ closely resembles the deviance of a logistic regression analysis which measures of how badly the fitted model reflects the observed reality. With the reconstruction loss $L$ defined, the calibration of SA become a numerical optimization problem
\[ \Par = \min_{\Par} L(\XDCD, \XECD). \]
With the constraint (\ref{eq:CW}) on the weight matrices in the decoder stack, the set of parameters to be tuned is
\[ \Par = \{\WEI{1}{1}{0}, \bEI{1}{1}, \bDI{1}{0}\} \cup \{\WEI{2}{2}{1}, \bEI{2}{2}, \bDI{2}{1}\} \dots \cup \{\WEI{M}{M}{M-1}, \bEI{M}{M}, \bDI{M}{M-1}\}, \]
whose size is $|\Par| = \sum_{i=1}^M{d_i d_{i-1}} + \sum_{j=1}^M{(d_j + d_{j-1})}$, which is $\sum_{i=1}^M{d_i d_{i-1}}$ parameters less then the non-constrained decoder. The optimization procedure is is done by gradient guided iterative algorithm, which is covered in the appendix section.

The optimization is computational intense, because the number of parameters $|\Par|$ is usually large. When the number of layers in the stack is also large, the computation used to be inhibiting, because with a fixed number of parameters, the complexity of the function represented by the network grows exponentially with the number of layers. A a deeper SA has more local minimum in the parameter space for the reconstruction loss $L$ to fall into. Yet, deep networks are enormously intriguing, since a exponentially richer function space means a much better chance to further reduce $L$ while at the same time produce even more compact abstraction (i.e. smaller $\yHT$). Deep artificial neural networks have revived its popularity in recent years, thanks to the break through in its training procedure, which is now popularly dubbed ``deep learning''. For our method, we implement the layer-wise greedy pre-training procedure \cite{DL:DBN1, DL:SDA1}. The idea is to first train each encoder layer separately, then fine tune the entire structure afterwards. To perform the layer-wise pre-training, the output of $i$ th. autoencoder $\zEC_i$ is not sent to the $i+1$ th. autoencoder above it, but instead is redirected to its decoder counterpart immediately to produce an intermediate reconstruction $\zDC_i$, 
\begin{equation*}
  \begin{split}
    \zDC_{i-1} &= s(\WDC_i \zDC_i + \bDC_i) \\
    \zDC_{i  } &= \yHT_i = \zEC_i \quad \quad \quad \quad (i = 0 \dots M) \\
    \zEC_{i  } &= s(\WEC_i\zEC_{i-1} + \bEC_i). \\
  \end{split}
\end{equation*}
The $i$ th. encoder--decoder tuple can then be pre-trained by minimizing the local reconstruction loss $L_i=L(\zEC_{i-1}, \zDC_{i-1})$, which is fairly easy thanks to the small number of parameters in $\Par_i=\{\WEI{i}{i}{i-1}, \bEI{i}{i}, \bDI{i}{i-1}\}$. A total of $M$ tuples is assembled and pre-trained separately. What the greedy layer-wise pre-training has achieved is non-randomly initialize the entire structure to a state closer to optimum. After pre-training, all the encoders and the decoders are wired back together like \ref{eq:ED} and fine tuned together. The comprehensive fine tuning will reach convergence much faster and less likely to ``climb the wrong mountain'' then a direct training scheme without the pre-training.

\subsection{Implementation}
The proposed method is based on the joint GMSU statistic $U_J$, formulated with three kernel functions corresponding to the three profiles. To test its robustness and versatility, we simulated continuous and binary phenotypes from the effect of purely genomic base, purely vertex base and mixed. Under these scenarios, the statistical power of $U_J$ is compared with two parsimonious statistics $U_G$ and $U_V$ whose null hypothesis is more specific but less flexible.
The method also intents to replace the original testing unit of the cortex profile with high order features abstracted from it, by training a 4 layered stacked autoencoder with all 806 sample profiles. The newly derived U statistics are compared with the one relying on the original vertices.

Lastly, we pick out 327 individual whose real diagnosis status is certain (either healthy control or Alzheimer's disease/Dementia case) and apply the proposed method to screen for associations among diagnosis, genes and cortex regions.