\section{Method}

To simoteniously detect association among genomics, cortical surface and phenotype profiles, a generalized multivariate similarity U statistic was used, postulated by Changshuai et. al. \cite{HWU}. Of all three types profile, the cortical surface has the most numerous variants, with 68 anatomical regions comprised of few hundreds to more than ten thousands of vertices. To deal with the dimensionality issue, a 4 layered Stacked Autoencoder(SA) was trained with a greed layer by layer manner, as suggested by the deep learning literatures. The SA is capable of abstracting high order features from the vertices in the cortical surface, and at the same time reducing its dimenstionality by 16 fold. 
The vertex similarity can then be calculated with the code instead of the orignial. Supposingly, any U statistic involving a vertex similarity weight should enjoy a power boost if the raw verties were to be replaced with the code, because high order features are less suscptable to random noise. to calculate will be cal by piping the encoded vertices can then by derived by summerizing the product of pairwise similarity between subjects, with respect to the aforementioned genetic and phenotipic profiles, and the encoded vertices. 

\subsection{Similarity U Test}
To derive the proposed U statistic, one U-kernel and one or more U-weights must be constructed. A kernal/weight measures the similarity of each pair of samples, with respect to one of the profiles. The measurement funciton can be flexable depending on the distribution of the profile and the hypothesis in mind, as long as it is symmetric and has finite second moment. Thus, function $f$ is a valid U kernal/weight if $f(x_i,x_j)=f(x_j,x_i)$ and $E(f^2(X_1, X_2))<+\infty$ are satisfied. For the three types of profile, three pairwise similarity measurement were chosen according to common practice.

For biallelic genetic variants whose value was taken from minor allele count ${0, 1, 2}$, a common choice is the weighted complement of Manhattan Distance (wMD):
\begin{equation} \label{eq:wSG}
\begin{split}
  S_{ij}^G &= wMD(g_{i.},g_{j.}) \\
  &= \frac{\sum_{m=1}^{|G|}{w_m(2-|g_{im}-g_{jm}|)}} {2\sum_{m=1}^{|G|}{w_m}},
\end{split}
\end{equation}
, which is also called identical by state (IBS), measurement. Here $g_{im}$($g_{jm}$) is the value of the $m th.$ variant (e.g. a SNP, indel, or deletion) in a testing unit (e.g. a gene) for the $i th.$($j th.$) sample; $|G|$ is the dimensionality of the genomic testing unit (e.g. total number of SNPs, indels and deletions in a gene). $w_m$ is the weight assigned to the $m th.$ variant depending on \textit{a prior} hypothesis, one such example is the reversed square root of minor allele frequency which places more emphasize on rarer variants:
\begin{displaymath}w_m=\frac{1}{\sqrt{MAF(g_m)(1-MAF(g_m))}}.\end{displaymath}
Without any \textit{a prior} hypothesis of the relative importance of genomic variants, an unweighted complement of Manhatten Distance (uMD) can be used by forcing $w_m \equiv 1$:
\begin{equation*} \label{eq:uSG}
\begin{split}
  S_{ij}^G &= uMD(g_{i.}, g_{j.}) \\
  &= \frac{\sum_{m = 1}^{|G|}{(2-|g_{im} - g_{jm}|)}}  {2|G|}.
\end{split}
\end{equation*}

For vertices in the reconstructed cortical surface, their values, for now, is taken from  rescaled $[0,1]$ white matter thickness, one could base the similarity measurement on weighted Euclidian Distance (wED):
\begin{equation} \label{eq:wSV}
\begin{split}
  S_{ij}^V &= wED(v_{i.},v_{j.}) \\
  &= \exp
  {
    \Big[-\frac{\sum_{m=1}^{|V|}{w_m(u_{im}-u_{jm})^2}} {\sum_{m=1}^{|V|}{w_m}}\Big]
  },
\end{split}
\end{equation}
, which is also called a Gaussian measurement. Here $v_{im}$($v_{jm}$) is the value of the $m$ th. vertex of the cortical surface testing unit in the $i$($j$) th. sample, while $|V|$ is the number of vertices in it that testing unit, that is, the dimensionality. $w_m$ is the weight assigned to the $m$ th. vertex in the testing unit. Without pre-knowledge regarding the relative importance of the vertices, the similarity measurement is simplifed by equal weighting ($w_m \equiv 1$):
\begin{equation*} \label{eq_uSV}
\begin{split}
  S_{ij}^V &= uED(v_{i.},v_{j.}) \\
  &= \exp
  {
    \Big[-\frac{\sum_{m=1}^{|V|}{(u_{im}-u_{jm})^2}} {|V|}\Big]
  }.
\end{split}
\end{equation*}
Lastly, for a multivariate phenotype profile, first a rank normal quantile normalization were a
pplied to all the components according to \cite{HWU}:
\begin{displaymath}
  q_{im}=\Phi^{-1}[(rank(y_{im})-0.5)/|Y|],
\end{displaymath} 
where $y_{im}$ is the value of the $m th.$ component of the phenotype for the $i th.$ sample, and $|Y|$ is the dimensionality of the phenotype (i.e. number of components). Doing so not only correct skewed components, but also remove the complication of admixed distribution type introduced by a multivariate phenotype. As a result, the phenotype based similarity can be measured in a manner similar to that of cortical surface:
\begin{equation} \label{eq_wSY}
\begin{split}
  S_{ij}^Y &= wED(q_{i.},q_{j.}) \\
  &=\exp
  {
    \Big[-\frac{\sum_{m=1}^{|Y|}{w_m(q_{im}-q_{jm})^2}} {\sum_{m=1}^{|Y|}{w_m}}\Big]
  },
\end{split}
\end{equation}
where $q_{im}$ is the values of the $m th.$ component of the normalized phenotype; the weight $w_m$ is the ralative importance of that dimension. For a case control study of one dimensional phenotype, that is, $|Y|=1$, the similarity measurement simplifys to:
\begin{displaymath}
  S_{ij}^{y}=wED(q_i,q_j)=\exp{[-(q_i-q_j)^2]},
\end{displaymath}
All three types of the similarity are centralized by substracting the raw measurement from every pair $(i,j)$ with two sample means of all measurements involving $i$ and $j$, respectively, then adding the sample mean of all the pairs to it \cite{HWU}. Takeing the weighted genetic similarity as an example, the centralized similarity measurement is:
\begin{equation} \label{eq_wSY}
\begin{split}
  \tilde{S}_{ij}^{G} &= S_{ij}^{G}-E(S_{i.}^{G})-E(S_{.j}^{G})+E(S_{..}^{G}) \\
  &= S_{ij}^{G} - \frac{1}{N}\sum_{k=1}^N{S_{ik}} - \frac{1}{N}\sum_{l=1}^N{S_{lj}} + \frac{1}{N^2}\sum_{k=1,l=1}^{N,N}{S_{kl}},
\end{split}
\end{equation}
where $N$ is number of samples. The same centralization scheme is applied to the other two similarity measurements. The U statistics is the mean of the product of three centralized similarity measurement across all but mirrored pairs:
\begin{displaymath}
  U^{GVY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{G} \tilde{S}_{ij}^{V} \tilde{S}_{ij}^{Y}.
\end{displaymath}
The actual implementation calculated three symmetric $N \times N$ similarity matrices to facilitate later manipulation. The U statistic follows a mixture of $\chi_1^2$ distrubtion, a $p$ value can then be calculated using Davis method \cite{HWU}.

For now, the genomic and vertex similarity are treated U-weight terms, while the phenotype similarity acts as the U-kernel term. In general, the assignment of U-kernal and U-weight terms does not alter the limiting distribution of the final U statistics. More specific association test can be perform by dropping one of the U-weight terms. To see if only the phenotype and genomic profiles are associated, construct the U statistics without vertex similarity term:
\begin{displaymath}
  U^{GY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{G} \tilde{S}_{ij}^{Y}.
\end{displaymath}
Likewise, dropping the genomic similarity to test if only the phenotype and genomic profiles are uncorrelated:
\begin{displaymath}
  U^{VY}=\frac{1}{N(N-1)}\sum_{1 \leq i < j \leq N} \tilde{S}_{ij}^{V} \tilde{S}_{ij}^{Y}.
\end{displaymath}

\subsection{Stacked Autoencoder}
The purpose of an encoder is to abstract high order features from the raw input which is meaningful for decision making. A real life example is vewing a painting: what intrests the viewer is the concept of object or event the painting captures, not the exact colouring at every inch of the canvas. As for the cortical surface profile, being able to see the overall shrinkage and laceration sites in a aged brain, even without the exact volumn, location, and boundery of these sites, is far more important than knowning the exact thickness, curvature and coordinates at every vertex. An extreamly accurate and objective measurement at every unit of the cortical surface is, though highly informative, remotely helpful for diagnostic purpose without abstraction of high order feature from it. The Stacked Autoencoder (SA) to be implemented here is the result of recent development of deep artificial neuro-network, originally inspired by the sentinental processing of visual information. By definition, an abstraction must reduce the complexity of the raw image  considerablly so the host could possibly process and make a sound accessment. The SA will take the high dimensional cortical surface and output abstracted features of much lower dimensionality.

An SA is formed by layers of autoencoders stacked from bottom to top, hence "Stacked". An autoencoder represents a linear recombination of the input elements, followed by an element-wise non-linear transformatio. Usually, the output has a lower dimensionality then the input to ensure abstraction and dimension reduction. In symbolic form, an autoencoder at the $i$ th. layer of an SA can be written as:
\begin{equation} \label{eq:AE}
  \begin{split}
    \zEC_i^{d_i}   &= s(\WEC_i^{d_i \times d_{i+1}} \zEC_{i+1}^{d_{i+1}} + \bEC_i^{d_i}) \\
  \end{split}
\end{equation},
where $\zEC_{i-1}^{d_{i-1}}$ is the input, which is also the output from the layer down below -- the output of the $i-1$ th. autoencoder in the stack; $\zEC_i^{d_i}$ is the output. The linear recombination is achieved by the corss product between the input and the weight matrix $\boldsymbol{W_i^{d_i\times d_{i-1}}}$, followed by an offest of $\boldsymbol{b^{d_i}}$. The superscript $d_{i-1}$ and $d_i$ denote the dimensionality. As mentioned before, to ensure feature abstraction and dimension reduction, in most cases, $d_i$ is chosen to be smaller than $d_{i-1}$. The elementwise non-linear transformation is usually chosen to be inverse logit:
\begin{equation} \label{eq:AE}
  \begin{split}
    s(\etaEC^{d_i})     &= [logit^{-1}(\eta_1), logit^{-1}(\eta_2), \dots, logit^{-1}(\eta_{d_i})]^{\prime} \\
    logit^{-1}(\eta_k)  &= \frac{1}{1+e^{-\eta_k}}
  \end{split}
\end{equation},
where $\etaEC^{d_i}$ is the recombined input, the super script $d_i$ is the dimensionality; $k = 1, \dots, d_i$ indices the $d_i$ elements in $\etaEC^{d_i}$. The "S" shaped inverse logit curve resembles the biological activation of the $k$ th. neuron when the sum of input signals from all $d_{i-1}$ neurons in the previous layer is above a threshold. The sum of input is weighted by is the $k$ th. row of weight matrix $WEC_i^{d_i \times d{i+1}}$, the threshold is the $k$ th. element of the offset vector $\bEC_i^{d_i}$. The actual biological activation of a neuron is a step function of the input which is steeper than the inverse logit, but being smooth and differentiabe throughout the real line is a desirable feature facilitating the later, gradient gudided learning procedure.

An SA of $M$ layers, $P$ dimensional raw input $\boldsymbol{x}$ and $Q$ dimensional output, is constucted by recursively pluging the output of an autoencoder into the one above, and ensuring the output of the top layer is $Q$ dimensinal:
\begin{equation} \label{eq:SA}
  \begin{split}
    \boldsymbol{\hat{y}}^Q        =
    \zEC_{M  }^{d_{M  }}         &= s(\WEC_{M  }^{d_{M  } \times d_{M-1}} \zEC_{M-1}^{d_{M-1}} + \bEC_{M  }^{d_{M  }}) \\
    \zEC_{M-1}^{d_{M-1}}         &= s(\WEC_{M-1}^{d_{M-1} \times d_{M-2}} \zEC_{M-2}^{d_{M-2}} + \bEC_{M-1}^{d_{M-1}}) \\
    ... &= ... \\
    \zEC_{i  }^{d_{i  }}         &= s(\WEC_{i  }^{d_{i  } \times d_{i-1}} \zEC_{i-1}^{d_{i-1}} + \bEC_{i  }^{d_{i  }}) \\
    ... &= ... \\
    \zEC_{2  }^{d_{2  }}         &= s(\WEC_{2  }^{d_{2  } \times d_{1  }} \zEC_{1  }^{d_{1  }} + \bEC_{2  }^{d_{2  }}) \\
    \zEC_{1  }^{d_{1  }}         &= s(\WEC_{1  }^{d_{1  } \times d_{0  }} \zEC_{0  }^{d_{0  }} + \bEC_{1  }^{d_{1  }}) \\
    \zEC_{0  }^{d_{0  }}         &= \xEC^P,
  \end{split}
\end{equation}
where $\xEC^P$ and $\boldsymbol{\hat{y}}^Q$ denote the $P$ dimensional raw input, and the $Q$ dimensional output, repectively. The vector $\zEC_i^{d_i}$ is the intermidiate output of the $i$ th. autoencoder. Here we use superscripts to display dimensionalities. The $P$ dimensional input $\xEC^P$ is viewed as output of the non-existing $0$ th. autoencoder, denoted by $\zEC_0^{d_0}$, where $d_0=P$. From the bottom to up, the SA will gradually abstract higher order features from the $P$ dimensional input $\xEC^P$, until the dimensionality of the output is as low as $d_M=Q$.

An SA just constructed is useless without calibration. We must find a set of parameters $\pEC=\{\WEC_1, \bEC_1, \WEC_2, \bEC_2, \dots, \WEC_M, \bEC_M\} = \pEC_1 \cup \pEC_2, \dots, \pEC_M$ that truely represents the body of the knowlege of the observed data $\xEC$, only then the SA would be capable of abstracting meaningful features instead of haphazardly reducing a high dimensional input into a small but irrelevent output (e.g. a vector of $Q$ random numbers). The "capability" of the SA can be gauged by two different criteria. For one, it is reasonable to say an abstraction should help to better predict the outcome of interest. Turning strategies based on this criteria belong to the so called supervised machine learning. Another criteria, saying if an abstraction is indeed done better, we should be able to be reconstructed the original input from it more accurately, will resulting in the so called unsupervised machine learning. Here ``supervised'' means the ``goodness of abstraction'' is judged by asking how accurate the predictions were made using features extracted by the SA from the raw input, in comparison with the known external fact from a specific domain of problems. The SA thus learned will try to extract features better suited for problems from that domain (e.g. tell the future risk of neuralogical disorder by examing the cortical surface). The ``unsupervised'' learning, on the other hand, judges the ``goodness of abstraction'' by having the raw input comparing with its reconstructed self from the extracted features. An SA tuned in this manner will try to extract more general features not necessarily specific to a certain problem domain. In this work, the later, unsupervised learning is to be used to explore generic knowledge regarding the human cortical surface, not just for the purpose of diagnosing a certain type of neuralogical disorder (e.g. Alzheimer's Disease). As a matter of fact, all available T1 MRI samples contrubute to the ``machines's understanding of human brain'', even if more than half of them cannot enter the association analysis because of non-definite diagnosis.

Prior to unsupervised learning however, one has to come up with a reconstruction scheme -- a decoder counterpart of the SA. The most nature way is to mirror the structure of the SA, that is, the number of layers in the decoder is the same with the SA, each layer is also a linear recombination of its input followed by a element-wise non-linear transformation, but the dimensionality changes are exactly in reverse order of the SA. In symbolic form, the decoder can be structured in a similar manner:
\begin{equation} \label{eq:DC}
\begin{split}
  \boldsymbol{\tilde{x}}=
  \zDC_{0  }^{d_{0  }} &= s(\WDC_{1  }^{d_{0  } \times d_{1  }} \zDC_{1  }^{d_{1  }}+  \bDC_{1  }^{d_{0  }}) \\
  \zDC_{1  }^{d_{1  }} &= s(\WDC_{2  }^{d_{1  } \times d_{2  }} \zDC_{2  }^{d_{2  }} + \bDC_{2  }^{d_{1  }}) \\
  ... &= ... \\
  \zDC_{i  }^{d_{i  }} &= s(\WDC_{i+1}^{d_{i  } \times d_{i+1}} \zDC_{i+1}^{d_{i+1}} + \bDC_{i+1}^{d_{i  }}) \\
  ... &= ... \\
  \zDC_{M-2}^{d_{M-1}} &= s(\WDC_{M-1}^{d_{M-2} \times d_{M-1}} \zDC_{M-1}^{d_{M-1}} + \bDC_{M-1}^{d_{M-2}}) \\
  \zDC_{M-1}^{d_{M-1}} &= s(\WDC_{M  }^{d_{M-1} \times d_{M  }} \zDC_{M  }^{d_{M  }} + \bDC_{M  }^{d_{M-1}}) \\
  \zDC_{M  }^{d_{M  }} &= \yHT^Q .
\end{split}
\end{equation}
From bottom to up, the decoder gradually restores details from the abstracted features $\yHT^Q$, and eventually reaches a reconstruction of the raw input $\xDC^P$. The encoder \ref{eq:SA} is linked from below by plugin its output into the bottom of the decoder through ${\boldsymbol{\hat{y}}}$. The entire encoding -- decoding structure can be written as:
\begin{equation} \label{eq:ED}
\begin{split}
  \boldsymbol{\tilde{x}}                 &= \boldsymbol{\tilde{z}_{0  }^{d_{0  }}} \\
  \boldsymbol{\tilde{z}_{0  }^{d_{0  }}} &= s(\boldsymbol{\tilde{W}_{1  }^{d_{0  } \times d_{1  }} \tilde{z}_{1  }^{d_{1  }}}+\boldsymbol{\tilde{b}_{0  }^{d_{0  }}}) \\
  ... &= ... \\
  \boldsymbol{\tilde{z}_{M-1}^{d_{M-1}}} &= s(\boldsymbol{\tilde{W}_{M  }^{d_{M-1} \times d_{M  }} \tilde{z}_{M  }^{d_{M  }}}+\boldsymbol{\tilde{b}_{M-1}^{d_{M-1}}}) \\
  \boldsymbol{\tilde{z}_{M  }^{d_{M  }}} &= \boldsymbol{\hat{y}} = \boldsymbol{z_{M  }^{d_{M  }}} \\
  \boldsymbol{z_{M  }^{d_{M  }}}         &= s(\boldsymbol{W_{M  }^{d_{M  } \times d_{M-1}} z_{M-1}^{d_{M-1}}}+\boldsymbol{b_{M  }^{d_{M  }}}) \\
  ... &= ... \\
  \boldsymbol{z_{1  }^{d_{1  }}}         &= s(\boldsymbol{W_{1  }^{d_{1  } \times d_{0  }} z_{0  }^{d_{0  }}}+\boldsymbol{b_{1  }^{d_{1  }}}) \\
  \boldsymbol{z_{0  }^{d_{0  }}}         &= \boldsymbol{x}.
\end{split}
\end{equation}
The difference between $\boldsymbol{\tilde{x}}$ and $\boldsymbol{x}$ is a rough measure of how badly the reconstructed input resemble the original, which is also telling us how badly the encoder had performed, because a truely meaningful abstraction should not obstruct the recovery of details. Therefore, the learning of the SA is the process of minimizing the disagreement between $\boldsymbol{\tilde{x}}$ and $\boldsymbol{x}$ by fine tuning the structuring parameters in the entire autoencoder and decoder stack, which is:
\begin{equation} \label{eq:PAR}
\begin{split}
  \Par &= \pEC \cup \pDC = \{\WEC_{1:M}, \bEC_{1:M}\} \cup \{\WDC_{M:1}, \bDC_{M:1}\}
\end{split}
\end{equation}
Two common measurement of the disagreement between $\xEC^P$ and $\xDC^P$ are least square error and corss entrophy, the former is more of a tradition, the later has gained popularity recently. In symbolic form, they can be written as:
\begin{equation} \label{eq:SE}
\begin{split}
  L^{SE}(\boldsymbol{\tilde{x}},\boldsymbol{x}) &= \sum_{i=1}^N\sum_{k=1}^Q\frac{1}{Q}(\tilde{x}_{ik}-x_{ik})^2,
\end{split}
\end{equation},
\begin{equation} \label{eq:CE}
\begin{split}
  L^{CE}(\boldsymbol{\tilde{x}},\boldsymbol{x}) &= -\sum_{i=1}^N\sum_{k=1}^Q[x_{ik}\log{\tilde{x}_{ik}}+(1-x_{ik})\log(1-\tilde{x}_{ik})],
\end{split}
\end{equation}
where $i$ indexes the samples, $k$ indexes the elements of the orignal/reconstruction input, both $L^{SE}$ and $L^{CE}$ are functions of $\boldsymbol{\Theta}$. In this study, the reconstruction loss takes the cross entrophy ($L=L^{CE}$) form, so the search of the best setup of parameters is expressed as a numerical optimization problem:
\[ \boldsymbol{\Theta} = \min_{\boldsymbol{\Theta}} L(\boldsymbol{\tilde{x}},\boldsymbol{x}). \]
Taking the $M$ layered SA and its decoder counterpart together, the total number of parameters is $|\boldsymbol{\Theta}| = |\boldsymbol{\theta}| + |\boldsymbol{\tilde{\theta}}| = \sum_{i=1}^{M}{(d_i + 1)d_{i-1}} + \sum_{j=M}^{1}{(d_{j-1} + 1)d_j}$. The optimization can be computationally intense because this number is usually huge.

One practical strategy to learn a SA is to constrain the linear reconbination in the decoder layers to be the transpose of their corresponding layers in the SA, that is, forcing $\boldsymbol{\tilde{W}_i^{d_{i-1} \times d_i}} \equiv (\boldsymbol{W_i^{d_i \times d_{i-1}})^{\prime}}$ for $i=1 \ldots M$. Doing so alost halve the number of parameters to $\sum_{i=1}^{M}{(d_{i-1}) \times (d_i - 1)} + \sum_{i=M}^{1}{d_i}$, and better fits the intuition that encoding and decoding are conceptually symmetric. More importantly, it also encourages the learning of an optimal SA instead of a sub-optimal SA coupled with a powerful decoder on its top, afterall, it is the best abstraction of the raw input being mostly wanted, not its reconstruction. 
The optimization is done by gradient decent. Starting with a randomly initialized assignment of $\Theta^t$ at $t=0$, and compute the next assignment $\Theta^{t+1}$ by substracting from the current assignment $\Theta^t$ a small fraction of the gradient of reconstruction loss $L$ with respect to the current assignment ($\frac{\partial L}{\partial \boldsymbol{\Theta^{t}}}$). The small fraction is called a learning step, if the step is reasonablly small, the reconstruction loss $L$ will keep dropping. The learning will repeat until $L$ cease to drop. The final assignment is considered the optimal $\Theta$. 

The arduous calculation of gradient $\PDV{L}{\boldsymbol{\Theta}^t}$ is done by a process called back propagation [?], which relies on heavily on the chain rule of derivatives. First, realize the reconstruction loss $L=L^{CE}(\xVO, \xVR)$ is a function of $\xVR$ while $\xVR$ is a function of $\Par$, using the chain rule, one gets:
\begin{equation*}
\begin{split}
  \PDV{L}{\Par} &= (\PDV{L}{\xVR})^{\prime} \PDV{\xVR}{\Par} \\
  \PDV{L}{\xVR} &= \xVO \oslash \xVR -  (\one{} - \xVO) \oslash (\one{} - \xVR) \\
  \PDV{\xVR}{\Par} &= \PDV{\zDC_0}{\Par}.
\end{split}
\end{equation*}
Here we use $\oslash$ to denote element-wise division. 
Recall the $i$ th. layer in the decoder stack \ref{eq:DC}, by expanding elements in the output, one have the following form:
\begin{equation*}
\begin{split}
  \zDC_i^{d_i} &= s(\WDC_i^{d_i \times d_{i+1}} \zDC_{i+1}^{d_{i+1}} + \bDC_i^{d_i})
  = \left[ \begin{array}{c}
    logit^{-1}(\wDC_{i1}^{d_{i+1}} \zDC_{i+1}^{d_{i+1}} + \tilde{b}_{i1}) \\
    \vdots \\
    logit^{-1}(\wDC_{id_i}^{d_{i+1}} \zDC_{i+1}^{d_{i+1}} + \tilde{b}_{id_i}) \\
  \end{array} \right] \\
\end{split},
\end{equation*}
where $\wDC_{ij}^{d_{i+1}}$ is the $j$ th. row vector of linear recombination matrix $\WDC_i^{d_i \times d_{i+1}}$, and $\tilde{b}_{ij}$ is the $j$ th. element of offset vector $\bDC_i^{d_i}$, with $j=1 \dots d_i$. The output of the $i$ th. decoder layer, $\oDi$, is a function of its structure parameters: $\pDC_i = \{\WDC_i, \bDC_{i+1}\} \subset \Par$. The gradient of $\oDi$ with respect to these parameters can be directly calculated:
\begin{equation*}
\begin{split}
  \PDV{\oDi}{\pDC_i} &= \PDV{\oDi}{[\WDi, \bDi]} \\
  &= \left[\begin{array}{c}
      \PDV{logit^{-1}(\wDC_{i1  }^{d_{i+1}} \zDC_{i+1}^{d_{i+1}} + \tilde{b}_{i1  })} {(\wDC_{i1  }^{d_{i+1}} \zDC_{i+1}^{d_{i+1}} + \tilde{b}_{i1  })}
      \PDV{(\wDC_{i1}^{d_{i+1}} \zDC_{i+1}^{d_{i+1}} + \tilde{b}_{i1})}{[\wDC_{i1}, \tilde{b}_{i1}]} \\
      \vdots \\
      \PDV{logit^{-1}(\wDC_{id_i}^{d_{i+1}} \zDC_{i+1}^{d_{i+1}} + \tilde{b}_{id_i})} {(\wDC_{id_i}^{d_{i+1}} \zDC_{i+1}^{d_{i+1}} + \tilde{b}_{id_i})}
      \PDV{(\wDC_{id_i}^{d_{i+1}} \zDC_{i+1}^{d_{i+1}} + \tilde{b}_{id_i})}{[\wDC_{id_i}, \tilde{b}_{id_i}]} \\
    \end{array} \right] \\
  &= \left[\begin{array}{c}
      \tilde{z}_{i1  }(1-\tilde{z}_{i1  }) [\zDC_{i+1}^{d_{i+1}\prime}, 1] \\
      \vdots \\
      \tilde{z}_{id_i}(1-\tilde{z}_{id_i}) [\zDC_{i+1}^{d_{i+1}\prime}, 1] \\
    \end{array} \right] \\
  &= \oDi \odot (\one{} - \oDi) [\zDC_{i+1}^{d_{i+1}\prime}, 1].
\end{split}
\end{equation*}
Here $\odot$ means element-wise product. 

Besides the gradient of the $i$ th. decoder ouput with respect to  its own paremeters, notice the input of the $i$ th. decoder layer, $\iDi$, is in fact a function of every structure parameter in the layers below, decoder and encoder regardless. Let's deal with the decoders first. Using the chain rule, the gradient of $\oDi$ with respect to the parameters in any of the lower decoder layer $i$ is:
\begin{equation*}
\begin{split}
  \PDV{\oDi}{\pDC_j}
  &= \PDV{\oDi}{\iDi} \PDV{\iDi}{\pDC_j} \\
  &= [\oDi \odot (\one{} - \oDi)]^\prime \WDi \PDV{\iDi}{\pDC_j}, 
\end{split}
\end{equation*}
where $\pDC_j$ are parameters in the lower, $j$ th. decoder layer ($i < j$). Puting together the above calculations, we see for the out of $i$ th. decoder layer, $\oDi$, its gradient with respect to any structure parameters within itself or the other decoders down below can be calculated by recursively invoking the chain rule:
\begin{equation*}
\begin{split}
  \PDV{\oDi}{\pDC_j} =
  \begin{cases}
    [\oDi \odot (\one{} - \oDi)]^\prime \WDi \PDV{\zDC_{i+1}^{d_{i+1}}}{\pDC_j} & 0 \le i < j < M \\
    \oDi \odot (\one{} - \oDi) [\zDC_{i+1}^{d_{i+1}\prime}, 1]                  & 0 \le i = j < M
  \end{cases}
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
  \zDC_i &= s(\WDC_{i+1} \zDC_{i+1} + \bDC_i)
\end{split}
\end{equation*}

\begin{equation} \label{eq:unit encoder-decoder}
\begin{split}
  \boldsymbol{\tilde{z}_{i-1}^{d_{i-1}}} &= s(\boldsymbol{\tilde{W}_{i  }^{d_{i-1} \times d_{i  }} z_{i  }^{d_{i  }}}+\boldsymbol{\tilde{b}_{i-1}^{d_{i-1}}}) \\
  \boldsymbol{z_{i  }^{d_{i  }}}         &= s(\boldsymbol{W_{i  }^{d_{i  } \times d_{i-1}} z_{i-1}^{d_{i-1}}}+\boldsymbol{b_{i  }^{d_{i  }}}), \\
\end{split}
\end{equation}
where $i=(1 \ldots M)$. Thus, instead of pipping the output of the $i th$ encoder layer ($\boldsymbol{z_{i  }^{d_{i  }}}$) to the one above, it is pipped to the cooresponding $M-i th.$ layer in the docoder counterpart for an immediate reconstruction of the input $\boldsymbol{z_{i-1}}$. The unit encoder--decoder tuple can then be trained by minimizeing the reconstruction loss $L(\boldsymbol{z_{i-1}},\boldsymbol{\tilde{z}_{i-1}})$. A total of $M$ units can be formed and trained seperatetly. Although the pre-training cannot be done in parallel since the higher unit have to wait the lower for input, the process is very fast because the number of parameters is rather small in each unit compared with the whole network. The pre-training serves as a non-random parameter initilization, after which the whole network is already near convergence. Therefore, the subsequent fine-tuning of entire network would required much less steps to reach convergence.

The SA thus trained may suffer the overfitting problem, that is, the network abstracted well on the facts so far presented to it, showing low reconstruction loss, but may deals poorly for the future, unknown data. To better balance the internal and external validy, one way is to add a regulator term to the loss funciton $L$, usually the $L_1$ or $L_2$ norm of the parameter elements. Doing so discourages large parameters which causes instability of the prediction/reconstruction. For encoder training, another way to ensure generalizability is to randomly corrupt the input $\xEC^P$ by setting some elements to zero [2008 Vincent]. The rationel is, the more rigours a feature is, the more like it will survive the random corruption. More training epoch is needed to ensure the randomness of corruption, but doing so encourages the network to seek major features of the input while neglecting the minor changes between facts. And since major feature are more likely to show up in the future, the external validty of the encoder is thus preserved.

For the current study howver, the purpose of deep-learning is not to construct an encoder encrypted with universal knowledge regarding white matter surface, but to produced the abstration the surface vertices at hand for the up-coming analysis. In other words, the code $\boldsymbol{\hat{y}}$ is the only concern here, and the external validty will be implictly handled by the subsequent U-statistical analysis using such code. For now, an ad-hoc encoder trained without regulator term or ramdom corruption would surfice. The current implementation is a five layered encoder, each halves the dimensionality of its input. Thus the final encoding will be 32 times smaller then the raw input.
