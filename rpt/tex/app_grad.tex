\subsection{Gradient Descent}
\newcommand{\oi}[2][]{\boldsymbol{o}_{#2}^{#1}}
\newcommand{\si}{\boldsymbol{s}}
\newcommand{\ei}[2][]{\boldsymbol{\eta}_{#2}^{#1}}
\newcommand{\bi}[2][]{\boldsymbol{\theta}_{#2}^{#1}}
The optimization is done by gradient decent. Starting with a randomly initialized assignment of $\Par^t$ at $t=0$, and update it by substracting a small fraction $\gamma$ of the gradient of reconstruction loss $d(\xDC, \xEC)$ with respect to the current assignment ($\PDV{d}{\Par^t}$). 
\[ \Par^{t+1} = \Par^t - \gamma \PDV{d}{\Par^t} \]
The small fraction $\gamma$ is called learning rate, if $\gamma$ is small enough, the loss $d$ will keep dropping. The updating process repeats itself until $d$ cease to drop and the final assignment $\Par^*$ is considered optimal.

The high dimensional gradient $\PDV{d(\xDC, \xEC)}{\Par}$ is calculated through backward propagation (BP) which heavily relies on the chain rule. The loss $d(\xDC, \xEC)$ is a function of $\xEC$ which in our case is the corss-entropy, and $\xDC$ in turn is a function of $\Par$, thus we have:
\begin{equation*}
  \begin{split}
    \PDV{d(\xDC, \xEC)}{\Par} &= \PDV{d(\xDC, \xEC)}{\xDC} \PDV{\xDC}{\Par}\\
    &= [\xEC \oslash \xDC - (\one - \xEC) \oslash (\one - \xDC)] \PDV{\xDC}{\Par} \\
    \PDV{\xDC_j^P}{\Par} &= \PDV{\zDC_{0j}^{d_0}}{\Par},
  \end{split}
\end{equation*}
where $\oslash$ means entry-wise division. 

Recall the symbolic form of any layer in an SA (see \ref{eq:SE}, \ref{eq:SD} and \ref{eq:SA}), regardless of being an encoder or a decoder, the layer output $\oi{l}$ is always a function its own parameters $\bi{l} = \{\WEC_l, \bEC_l\}$ and the output from the layer below, $\oi{l-1}$, that is,
\[ \oi{l} = \si(\ei{l}), \quad \ei{l} = \oi{l-1} \WEC_l + \bEC_l. \]
where $\ei{l} = \oi{l-1} \WEC_l + \bEC_l$ is the linear recombination and offsetting, while $\oi{l} = \si(\ei{l})$ being the non-linear entry-wise transformation which is the inverse logit in most cases. The gradient of a layer's output w.r.t its own parameters and immediate input, $\PDV{\oi{l}}{\bi{l}}$ and $\PDV{\oi{l}}{\oi{l-1}}$, are relatively easy to implement, 
\begin{equation*}
  \PDV{\oi{l}}{\ei{l}} = 
  \left[\begin{array}{cccc}
          \oi{l1}(1-\oi{l1}) & 0                  & \dots  & 0 \\
          0                  & \oi{l2}(1-\oi{l2}) & \dots  & 0 \\
          \vdots             & \vdots             & \ddots & \vdots \\
          0                  & \dots              & \dots  & \oi{lq}(1-\oi{lq})
    \end{array}\right]
\end{equation*}
and with them avaiable, the gradient of the highest layer's output $\oi{2M}$, that is, the reconstructed input $\xDC$, w.r.t parameters and output from any lower layer are recursively derived from top to bottom:
\newcommand{\SEP}{\qquad}
\newcommand{\PDT}[3]{\PDV{#1}{#2}\PDV{#2}{#3}}
\newcommand{\BPI}[4][\zDC_0]{\PDV{#1}{#2} &= \PDC{#1}{#4}{#2} & \qquad \PDV{#1}{#3} &= \PDC{#1}{#4}{#3}}
\newcommand{\DOT}            {             &  \vdots           &                     &  \vdots}
\begin{align*}
  \BPI{\pDC_2}{\zDC_2}{\zDC_1} \\
  \BPI{\pDC_3}{\zDC_3}{\zDC_2} \\
  \DOT \\
  \BPI{\pDC_M}{\zDC_M}{\zDC_{M-1}} \\
  \BPI{\bi{ M  }}{\oi{ M-1}}{\oi{ M  }} \numeq \label{eq:BP} \\
  \BPI{\bi{ M-1}}{\oi{ M-2}}{\oi{ M-1}} \\
  \DOT \\
  \BPI{\bi{ 2  }}{\oi{ 1  }}{\oi{ 2  }} \\
  \BPI{\bi{ 1  }}{\oi{ 0  }}{\oi{ 1  }} \\
\end{align*}

\begin{equation*}
  \PDV{\oi{l}}{\Par_m} = \PDC{\oi{l}}{\oi{l-1}}{\Par_m}
\end{equation*}

 par simple if only one layer is considered. $\zDI{i-1}{i-1}$, $\pDC_{i}=[\WDI{i}{i-1}{i}, \bDI{i}{i-1}]$ and $\zDI{i}{i}$, respectively (see \ref{eq:DS}). The calculation for an encoder layer will be similar, since the structure of an encoder layer is identical to its decoder counterpart except the dimensionality change (see \ref{eq:ES} and \ref{eq:ED}). The derivatives will be calculated separately for each output element $\tilde{z}_{ik} (k=1,\dots,d_{i-1})$, because the inverse logit transformation is applied element-wise. To ease the thought process, the $i$ th. decoder layer is rewritten in the per-element manner,
\begin{equation*}
  \zDI{i-1}{i-1} = 
  \left[\begin{array}{c}
      \tilde{z}_{i-1,1} \\ 
      \tilde{z}_{i-1,2} \\ \vdots \\
      \tilde{z}_{i-1,k} \\ \vdots \\
      \tilde{z}_{i-1,d_{i-1}}
    \end{array}\right]
  = s(\WDI{i}{i-1}{i} \zDI{i}{i} + \bDI{i}{i-1})
  = \left[ \begin{array}{c}
      logit^{-1}(\wDI{i1}{i} \zDI{i}{i} + \tilde{b}_{i1}) \\
      logit^{-1}(\wDI{i2}{i} \zDI{i}{i} + \tilde{b}_{i2}) \\ \vdots \\
      logit^{-1}(\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{ik}) \\ \vdots \\
      logit^{-1}(\wDI{id_{i-1}}{i} \zDI{i}{i} + \tilde{b}_{id_{i-1}}) \\
    \end{array} \right],
\end{equation*}
where $\wDI{ik}{i}$ is the $k$ th. row vector of weight matrix $\WDI{i}{i-1}{i}$, and $\tilde{b}_{ik}$ is the $k$ th. element of threshold vector $\bDI{i}{i-1}$, with $k=1 \dots d_{i-1}$. The gradients of the $k$ th. element in the output, $\tilde{z}_{i-1,k}$, with respect to its contributing parameters $\pDC_{ik}=[\wDI{ik}{i}, \tilde{b}_{ik}]$ and the input $\zDI{i}{i}$ are
\begin{equation*}
  \begin{split}
    \PDV{\tilde{z}_{i-1,k}}{[\wDI{ik}{i}, \tilde{b}_{ik}]}
    &= \PDC{\tilde{z}_{ik}}{\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{i2}}{[\wDI{ik}{i}, \tilde{b}_{ik}]} \\
    &= \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k})[\zDIt{i}{i},1] \\
    \text{and,} & \\
    \PDV{\tilde{z}_{i-1,k}}{\zDI{i}{i}}
    &= \PDC{\tilde{z}_{ik}}{\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{ik}}{\zDI{i}{i}} \\
    &= \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k})\wDI{ik}{i},
  \end{split}
\end{equation*}
respectively. Notice that, the derivative of $y=logit^{-1}(x)=\SGM{x}$ has a more compact expression using the dependent variable $y$ instead of $x$, since
\begin{equation*}
  \DRV{\SGM{x}}{x} = \frac{1}{1+e^{-x}}\frac{e^{-x}}{1+e^{-x}} = \SGM{x}(1-\SGM{x}),
\end{equation*}
thus, during the invocation of chain rule, we have
\begin{equation*}
  \DRV{\tilde{z}_{i-1,k}}{\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{ik}}=\tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k}).
\end{equation*}
Packing up the gradient for each element in the $i$ th. decoder's output into a vector, we have
\begin{equation*}
  \begin{split}
    \arraycolsep=1.4pt\def\arraystretch{1.5}
    \PDV{\zDI{i-1}{i-1}}{\pDC_{i}}
    &= \left[\begin{array}{c}
        \PDV{\tilde{z}_{i-1,1}}{[\wDI{i1}{i}, \tilde{b}_{i1}]} \\
        \PDV{\tilde{z}_{i-1,2}}{[\wDI{i2}{i}, \tilde{b}_{i2}]} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,k}}{[\wDI{ik}{i}, \tilde{b}_{ik}]} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,d_{i-1}}}{[\wDI{id_{i-1}}{i}, \tilde{b}_{id_{i-1}}]} \\
      \end{array} \right]
    = \left[\begin{array}{c}
        \tilde{z}_{i-1,1}(1-\tilde{z}_{i-1,1}) [\zDIt{i}{i}, 1] \\
        \tilde{z}_{i-1,2}(1-\tilde{z}_{i-1,2}) [\zDIt{i}{i}, 1] \\ \vdots \\
        \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k}) [\zDIt{i}{i}, 1] \\ \vdots \\
        \tilde{z}_{i-1,d_{i-1}}(1-\tilde{z}_{i-1,d_{i-1}}) [\zDIt{i}{i}, 1] \\
      \end{array} \right] \\
    &= \diag{\zDI{i-1}{i-1}} \diag{\one - \zDI{i-1}{i-1}} [\zDIt{i}{i},1], \\ \text{in a similarly way,} \\
    \PDV{\zDI{i-1}{i-1}}{\zDI{i}{i}}
    &= \left[\begin{array}{c}
        \PDV{\tilde{z}_{i-1,1}}{\zDI{i}{i}} \\
        \PDV{\tilde{z}_{i-1,2}}{\zDI{i}{i}} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,k}}{\zDI{i}{i}} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,d_{i-1}}}{\zDI{i}{i}} \\
      \end{array} \right]
    = \left[\begin{array}{c}
        \tilde{z}_{i-1,1}(1-\tilde{z}_{i-1,1})\wDI{i1}{i} \\
        \tilde{z}_{i-1,2}(1-\tilde{z}_{i-1,2})\wDI{i2}{i} \\ \vdots \\
        \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k})\wDI{ik}{i} \\ \vdots \\
        \tilde{z}_{i-1,d_{i-1}}(1-\tilde{z}_{i-1,d_{i-1}})\wDI{id_{i-1}}{i} \\
      \end{array} \right] \\
    &= \diag{\zDI{i-1}{i-1}} \diag{\one - \zDI{i-1}{i-1}} \WDI{i}{i-1}{i}
  \end{split}
\end{equation*}

\begin{equation}\label{eq:GD}
  \begin{split}\
    \begin{array}{rl}
      \textrm{the i th. decoder:} & \begin{array}{rcl}
        \PDV{\zDI{i-1}{i-1}}{\pDC_i} & = & \diag{\zDI{i-1}{i-1}} \diag{\one{} - \zDI{i-1}{i-1}} [\zDIt{i}{i}, 1] \\
        \PDV{\zDI{i-1}{i-1}}{\zDI{i}{i}} & = & \diag{\zDI{i-1}{i-1}} \diag{\one{} - \zDI{i-1}{i-1}} \WDI{i}{i-1}{i} \\
      \end{array} \\ \\
      \textrm{the i th. encoder:} & \begin{array}{rcl}
        \PDV{\zEI{i}{i}}{\pEC_i} & = & \diag{\zEI{i}{i}}\diag{\one - \zEI{i}{i}} [\zEIt{i-1}{i-1}, 1] \\
        \PDV{\zEI{i}{i}}{\zEI{i-1}{i-1}} & = &  \diag{\zEI{i}{i}} \diag{\one - \zEI{i}{i}} \WEI{i}{i}{i-1}\\
      \end{array}
    \end{array}
  \end{split}
\end{equation}
Here $\diag{v}$ means creating a $0$ matrix and asign the vector $v$ to its diagonal. Since the input of a layer is essencially the output of the layer down below, who also has its own structure paremeters and input from the layer even lower, the gradient of the top output, $\zDC_0$, with respect to any lower layers' parameters, can be calculated by recursively invoking the chain rule, that is,

In reverse to the "bottom to top" encoding and decoding procedure, the gradient is propagated from top to bottom, hince the name "backward propagation". Taking the $M$ layered SA and its decoder counterpart together, the total number of parameters to be calibrated is $|\Par| = |\pEC| + |\pDC| = \sum_{i=1}^{M}{(d_i + 1)d_{i-1}} + \sum_{j=M}^{1}{(d_{j-1} + 1)d_j}$. The optimization can be computationally intense because this number is usually huge. One commonly applied strategy for learning an SA is to constrain the weight matrix in the decoder layers to be the transpose of their counterpart in the SA, that is, forcing $\WDI{i}{i-1}{i} \equiv \WEIt{i}{i}{i-1}$ for $i=1 \ldots M$, and on top of the assignment of gradient layed out in \ref{eq:GD} and \ref{eq:BP}, the gradient of the top output $\zDI{0}{0}$ with respect to the weight matrix of any decoder will be absorded by the one in its encoder counterpart, that is, 
\begin{equation*}
  \begin{split}\
    \PDV{\zDC_0}{\WEC_i}^* &= \PDV{\zDC_0}{\WEC_i} + (\PDV{\zDC_0}{\WEC_i})^\prime.
  \end{split}
\end{equation*}
Doing so introduces slightly more computation for each learning step, but at the same time almost halve the number of tuning parameters to $\sum_{i=1}^{M}{(d_{i-1}) \times (d_i - 1)} + \sum_{i=M}^{1}{d_i}$, greatly speed up the convergence of $L(\XDC, \XEC)$.  Beside, the whole structure fits the common sense that, encoding and decoding are essentially symmetric operations. More importantly, the constraint encourages learning of an optimal SA instead of a sub-optimal SA coupled with a powerful decoder on its top, afterall, our best interest is the high order feature abstracted from the raw input, not its reconstruction.