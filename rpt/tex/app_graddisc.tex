\subsection{Gradient Descent}
The optimization is done by gradient decent. Starting with a randomly initialized assignment of $\Theta^t$ at $t=0$, and compute the next assignment $\Theta^{t+1}$ by substracting from the current assignment $\Theta^t$ a small fraction of the gradient of reconstruction loss $L$ with respect to the current assignment ($\frac{\partial L}{\partial \boldsymbol{\Theta^{t}}}$). The small fraction is called a learning step, if the step is reasonablly small, the reconstruction loss $L$ will keep dropping. The learning will repeat until $L$ cease to drop. The final assignment is considered the optimal $\Theta$. 

The gradient $\PDV{L}{\Par^t}$ is calculated by a process called backward propagation [?], which relies heavily on the chain rule of derivatives. 
First, realize the total loss $L(\XDC, \XEC)$ is a summation of  of individual loss $l_j(\xDC_j^P, \xEC_j^P)$ for $j=1, \dots, N$, and $l_j(\xDC_j^P, \xEC_j^P)$ in turn is a function of $\xDC_j^P$ (the observed $\xEC_j^P$ is a constant vector), at last, $\xDC_j^P$ is a function of all structure parameters $\Par$, using the additive rule and chain rule we get:
\begin{equation*}
\begin{split}
  \PDV{L(\XDC, \XEC)}{\Par} &= \sum_{j=1}^{N}\PDV{l_j(\xDC_j^P, \xEC_j^P)}{\Par} \\
  \PDV{l_j(\xDC_j^P, \xEC_j^P)}{\Par} &= \PDV{l_j(\xDC_j^P, \xEC_j^P)}{\xDC_j^P} \PDV{\xDC_j^P}{\Par} \\
  \PDV{l_j(\xDC_j^P, \xEC_j^P)}{\xDC_j^P} &= \xEC_j^P \oslash \xDC_j^P - (\one - \xEC_j^P) \oslash (\one - \xDC_j^P) \\
  \PDV{\xDC_j^P}{\Par} &= \PDV{\zDC_{0j}^{d_0}}{\Par}.
\end{split}
\end{equation*}
Here we use $\oslash$ to denote element-wise division. From now on we will focus on the $j$ th. sample and ommit $j$ from the subscript, since the output $\xDC_j^P = \zDI{0j}{0}$ on top is constructed in exactly the same manner across all $N$ individuals. Recall the symbolic form of SA and the decoders on its top (see \ref{eq:ES}, \ref{eq:DS} and \ref{eq:ED}), the output of any layer, either from an encoder or a decoder, is a function of its own structure parameters and the direct input come from the lower layer. The gradient of the layer output with respect to its own paremeters and input can be calculated directly. Let us take the $i$ th. decoder layer as an representative, whose own ouput, parameters, and input are $\zDI{i-1}{i-1}$, $\pDC_{i}=[\WDI{i}{i-1}{i}, \bDI{i}{i-1}]$ and $\zDI{i}{i}$, respectively (see \ref{eq:DS}). The calculation for an encoder layer will be similar, since the structure of an encoder layer is identical to its decoder counterpart except the dimensionality change (see \ref{eq:ES} and \ref{eq:ED}). The derivatives will be calculated separately for each output element $\tilde{z}_{ik} (k=1,\dots,d_{i-1})$, because the inverse logit transformation is applied element-wise. To ease the thought process, the $i$ th. decoder layer is rewritten in the per-element manner,
\begin{equation*}
  \zDI{i-1}{i-1} = 
  \left[\begin{array}{c}
      \tilde{z}_{i-1,1} \\ 
      \tilde{z}_{i-1,2} \\ \vdots \\
      \tilde{z}_{i-1,k} \\ \vdots \\
      \tilde{z}_{i-1,d_{i-1}}
    \end{array}\right]
  = s(\WDI{i}{i-1}{i} \zDI{i}{i} + \bDI{i}{i-1})
  = \left[ \begin{array}{c}
      logit^{-1}(\wDI{i1}{i} \zDI{i}{i} + \tilde{b}_{i1}) \\
      logit^{-1}(\wDI{i2}{i} \zDI{i}{i} + \tilde{b}_{i2}) \\ \vdots \\
      logit^{-1}(\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{ik}) \\ \vdots \\
      logit^{-1}(\wDI{id_{i-1}}{i} \zDI{i}{i} + \tilde{b}_{id_{i-1}}) \\
    \end{array} \right],
\end{equation*}
where $\wDI{ik}{i}$ is the $k$ th. row vector of weight matrix $\WDI{i}{i-1}{i}$, and $\tilde{b}_{ik}$ is the $k$ th. element of threshold vector $\bDI{i}{i-1}$, with $k=1 \dots d_{i-1}$. The gradients of the $k$ th. element in the output, $\tilde{z}_{i-1,k}$, with respect to its contributing parameters $\pDC_{ik}=[\wDI{ik}{i}, \tilde{b}_{ik}]$ and the input $\zDI{i}{i}$ are
\begin{equation*}
  \begin{split}
    \PDV{\tilde{z}_{i-1,k}}{[\wDI{ik}{i}, \tilde{b}_{ik}]}
    &= \PDC{\tilde{z}_{ik}}{\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{i2}}{[\wDI{ik}{i}, \tilde{b}_{ik}]} \\
    &= \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k})[\zDIt{i}{i},1] \\
    \text{and,} & \\
    \PDV{\tilde{z}_{i-1,k}}{\zDI{i}{i}}
    &= \PDC{\tilde{z}_{ik}}{\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{ik}}{\zDI{i}{i}} \\
    &= \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k})\wDI{ik}{i},
  \end{split}
\end{equation*}
respectively. Notice that, the derivative of $y=logit^{-1}(x)=\SGM{x}$ has a more compact expression using the dependent variable $y$ instead of $x$, since
\begin{equation*}
  \DRV{\SGM{x}}{x} = \frac{1}{1+e^{-x}}\frac{e^{-x}}{1+e^{-x}} = \SGM{x}(1-\SGM{x}),
\end{equation*}
thus, during the invocation of chain rule, we have
\begin{equation*}
  \DRV{\tilde{z}_{i-1,k}}{\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{ik}}=\tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k}).
\end{equation*}
Packing up the gradient for each element in the $i$ th. decoder's output into a vector, we have
\begin{equation*}
  \begin{split}
    \arraycolsep=1.4pt\def\arraystretch{1.5}
    \PDV{\zDI{i-1}{i-1}}{\pDC_{i}}
    &= \left[\begin{array}{c}
        \PDV{\tilde{z}_{i-1,1}}{[\wDI{i1}{i}, \tilde{b}_{i1}]} \\
        \PDV{\tilde{z}_{i-1,2}}{[\wDI{i2}{i}, \tilde{b}_{i2}]} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,k}}{[\wDI{ik}{i}, \tilde{b}_{ik}]} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,d_{i-1}}}{[\wDI{id_{i-1}}{i}, \tilde{b}_{id_{i-1}}]} \\
      \end{array} \right]
    = \left[\begin{array}{c}
        \tilde{z}_{i-1,1}(1-\tilde{z}_{i-1,1}) [\zDIt{i}{i}, 1] \\
        \tilde{z}_{i-1,2}(1-\tilde{z}_{i-1,2}) [\zDIt{i}{i}, 1] \\ \vdots \\
        \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k}) [\zDIt{i}{i}, 1] \\ \vdots \\
        \tilde{z}_{i-1,d_{i-1}}(1-\tilde{z}_{i-1,d_{i-1}}) [\zDIt{i}{i}, 1] \\
      \end{array} \right] \\
    &= \diag{\zDI{i-1}{i-1}} \diag{\one - \zDI{i-1}{i-1}} [\zDIt{i}{i},1], \\ \text{in a similarly way,} \\
    \PDV{\zDI{i-1}{i-1}}{\zDI{i}{i}}
    &= \left[\begin{array}{c}
        \PDV{\tilde{z}_{i-1,1}}{\zDI{i}{i}} \\
        \PDV{\tilde{z}_{i-1,2}}{\zDI{i}{i}} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,k}}{\zDI{i}{i}} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,d_{i-1}}}{\zDI{i}{i}} \\
      \end{array} \right]
    = \left[\begin{array}{c}
        \tilde{z}_{i-1,1}(1-\tilde{z}_{i-1,1})\wDI{i1}{i} \\
        \tilde{z}_{i-1,2}(1-\tilde{z}_{i-1,2})\wDI{i2}{i} \\ \vdots \\
        \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k})\wDI{ik}{i} \\ \vdots \\
        \tilde{z}_{i-1,d_{i-1}}(1-\tilde{z}_{i-1,d_{i-1}})\wDI{id_{i-1}}{i} \\
      \end{array} \right] \\
    &= \diag{\zDI{i-1}{i-1}} \diag{\one - \zDI{i-1}{i-1}} \WDI{i}{i-1}{i}
  \end{split}
\end{equation*}

\begin{equation}\label{eq:GD}
  \begin{split}\
    \begin{array}{rl}
      \textrm{the i th. decoder:} & \begin{array}{rcl}
        \PDV{\zDI{i-1}{i-1}}{\pDC_i} & = & \diag{\zDI{i-1}{i-1}} \diag{\one{} - \zDI{i-1}{i-1}} [\zDIt{i}{i}, 1] \\
        \PDV{\zDI{i-1}{i-1}}{\zDI{i}{i}} & = & \diag{\zDI{i-1}{i-1}} \diag{\one{} - \zDI{i-1}{i-1}} \WDI{i}{i-1}{i} \\
      \end{array} \\ \\
      \textrm{the i th. encoder:} & \begin{array}{rcl}
        \PDV{\zEI{i}{i}}{\pEC_i} & = & \diag{\zEI{i}{i}}\diag{\one - \zEI{i}{i}} [\zEIt{i-1}{i-1}, 1] \\
        \PDV{\zEI{i}{i}}{\zEI{i-1}{i-1}} & = &  \diag{\zEI{i}{i}} \diag{\one - \zEI{i}{i}} \WEI{i}{i}{i-1}\\
      \end{array}
    \end{array}
  \end{split}
\end{equation}
Here $\diag{v}$ means creating a $0$ matrix and asign the vector $v$ to its diagonal. Since the input of a layer is essencially the output of the layer down below, who also has its own structure paremeters and input from the layer even lower, the gradient of the top output, $\zDC_0$, with respect to any lower layers' parameters, can be calculated by recursively invoking the chain rule, that is,
\begin{align*}\label{eq:BP}
  \newcommand{\PDT}[3]{\PDV{#1}{#2}\PDV{#2}{#3}}
  \newcommand{\CHN}[3]{\PDV{#1}{#3} & = & \PDT{#1}{#2}{#3}}
  \newcommand{\SEP}{\quad \quad}
%  \arraycolsep=1.4pt\def\arraystretch{1.4}
%  \begin{array}{rclcrcl}
    \CHN{\zDC_0}{\zDC_{1  }}{\pDC_{2  }} & \SEP & \CHN{\zDC_0}{\zDC_{1  }}{\zDC_{2  }} \\
    \CHN{\zDC_0}{\zDC_{2  }}{\pDC_{3  }} & \SEP & \CHN{\zDC_0}{\zDC_{2  }}{\zDC_{3  }} \\
    & \vdots & & \SEP & & \vdots & \\
    \CHN{\zDC_0}{\zDC_{M-2}}{\pDC_{M-1}} & \SEP & \CHN{\zDC_0}{\zDC_{M-2}}{\zDC_{M-1}} \\
    \CHN{\zDC_0}{\zDC_{M-1}}{\pDC_{M  }} & \SEP & \CHN{\zDC_0}{\zDC_{M-1}}{\zDC_{M  }} \\
    & & & & \PDV{\zDC_0}{\zEC_{M  }} & = & \PDV{\zDC_0}{\yHT} = \PDV{\zDC_0}{\zDC_M} \\
    \CHN{\zDC_0}{\zEC_{M  }}{\pEC_{M  }} & \SEP & \CHN{\zDC_0}{\zEC_{M  }}{\zEC_{M-1}} \\
    \CHN{\zDC_0}{\zEC_{M-1}}{\pEC_{M-1}} & \SEP & \CHN{\zDC_0}{\zEC_{M-1}}{\zEC_{M-2}} \\
    & \vdots & & \SEP & & \vdots & \\
    \CHN{\zDC_0}{\zEC_{2  }}{\pEC_{2  }} & \SEP & \CHN{\zDC_0}{\zEC_{2  }}{\zEC_{1  }} \\
    \CHN{\zDC_0}{\zEC_{1  }}{\pEC_{1  }} & \SEP & & &
%  \end{array}
\end{align*}
In reverse to the "bottom to top" encoding and decoding procedure, the gradient is propagated from top to bottom, hince the name "backward propagation". Taking the $M$ layered SA and its decoder counterpart together, the total number of parameters to be calibrated is $|\Par| = |\pEC| + |\pDC| = \sum_{i=1}^{M}{(d_i + 1)d_{i-1}} + \sum_{j=M}^{1}{(d_{j-1} + 1)d_j}$. The optimization can be computationally intense because this number is usually huge. One commonly applied strategy for learning an SA is to constrain the weight matrix in the decoder layers to be the transpose of their counterpart in the SA, that is, forcing $\WDI{i}{i-1}{i} \equiv \WEIt{i}{i}{i-1}$ for $i=1 \ldots M$, and on top of the assignment of gradient layed out in \ref{eq:GD} and \ref{eq:BP}, the gradient of the top output $\zDI{0}{0}$ with respect to the weight matrix of any decoder will be absorded by the one in its encoder counterpart, that is, 
\begin{equation*}
  \begin{split}\
    \PDV{\zDC_0}{\WEC_i}^* &= \PDV{\zDC_0}{\WEC_i} + (\PDV{\zDC_0}{\WEC_i})^\prime.
  \end{split}
\end{equation*}
Doing so introduces slightly more computation for each learning step, but at the same time almost halve the number of tuning parameters to $\sum_{i=1}^{M}{(d_{i-1}) \times (d_i - 1)} + \sum_{i=M}^{1}{d_i}$, greatly speed up the convergence of $L(\XDC, \XEC)$.  Beside, the whole structure fits the common sense that, encoding and decoding are essentially symmetric operations. More importantly, the constraint encourages learning of an optimal SA instead of a sub-optimal SA coupled with a powerful decoder on its top, afterall, our best interest is the high order feature abstracted from the raw input, not its reconstruction.