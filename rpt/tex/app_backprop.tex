\subsection{Backward Propagation}
First, realize the reconstruction loss $L=L^{CE}(\xVO, \xVR)$ is a function of $\xVR$ while $\xVR$ is a function of $\Par$, using the chain rule, one gets:
\begin{equation*}
\begin{split}
  \PDV{L}{\Par} &= (\PDV{L}{\xVR})^{\prime} \PDV{\xVR}{\Par} \\
  \PDV{L}{\xVR} &= \xVO \oslash \xVR -  (\one{} - \xVO) \oslash (\one{} - \xVR) \\
  \PDV{\xVR}{\Par} &= \PDV{\zDC_0}{\Par}.
\end{split}
\end{equation*}
Here we use $\oslash$ to denote element-wise division.

In the main text, derivation of $\PDV{\zDC_0}{\Par}$ requires explict form of the gradient of a layer's output with respect to its own paremeters and direct input. Here, we take the $i$ th. decoder layer as an example, whose ouput, domestic parameters, and direct input are $\zDI{i-1}{i-1}$, $\pDC_{i}=[\WDI{i}{i-1}{i}, \bDI{i}{i-1}]$ and $\zDI{i}{i}$, respectively (see \ref{eq:DS}). The calculation for an encoder layer is similar, since the structure of an encoder layer is identical to its decoder counterpart except the dimensionality change (see \ref{eq:ES} and \ref{eq:ED}). The derivatives will be calculated separately for each output element $\tilde{z}_{ik} (k=1,\dots,d_{i-1})$, because the inverse logit transformation is applied element-wise. To ease the thought process, the $i$ th. decoder layer is rewritten in a per-element manner, which is
\begin{equation*}
  \zDI{i-1}{i-1} = 
  \left[\begin{array}{c}
      \tilde{z}_{i-1,1} \\ 
      \tilde{z}_{i-1,2} \\ \vdots \\
      \tilde{z}_{i-1,k} \\ \vdots \\
      \tilde{z}_{i-1,d_{i-1}}
    \end{array}\right]
  = s(\WDI{i}{i-1}{i} \zDI{i}{i} + \bDI{i}{i-1})
  = \left[ \begin{array}{c}
      logit^{-1}(\wDI{i1}{i} \zDI{i}{i} + \tilde{b}_{i1}) \\
      logit^{-1}(\wDI{i2}{i} \zDI{i}{i} + \tilde{b}_{i2}) \\ \vdots \\
      logit^{-1}(\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{ik}) \\ \vdots \\
      logit^{-1}(\wDI{id_{i-1}}{i} \zDI{i}{i} + \tilde{b}_{id_{i-1}}) \\
    \end{array} \right],
\end{equation*}
where $\wDI{ik}{i}$ is the $k$ th. row vector of weight matrix $\WDI{i}{i-1}{i}$, and $\tilde{b}_{ik}$ is the $k$ th. element of threshold vector $\bDI{i}{i-1}$, with $k=1 \dots d_{i-1}$. The gradients of the $k$ th. element in the output, $\tilde{z}_{i-1,k}$, with respect to its contributing parameters $\pDC_{ik}=[\wDI{ik}{i}, \tilde{b}_{ik}]$ and the input $\zDI{i}{i}$ are
\begin{equation*}
  \begin{split}
    \PDV{\tilde{z}_{i-1,k}}{[\wDI{ik}{i}, \tilde{b}_{ik}]}
    &= \PDC{\tilde{z}_{ik}}{\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{i2}}{[\wDI{ik}{i}, \tilde{b}_{ik}]} \\
    &= \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k})[\zDIt{i}{i},1] \\
    \text{and,} & \\
    \PDV{\tilde{z}_{i-1,k}}{\zDI{i}{i}}
    &= \PDC{\tilde{z}_{ik}}{\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{ik}}{\zDI{i}{i}} \\
    &= \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k})\wDI{ik}{i},
  \end{split}
\end{equation*}
respectively. Notice that, the derivative of $y=logit^{-1}(x)=\SGM{x}$ has a more compact expression using the dependent variable $y$ instead of $x$, since
\begin{equation*}
  \DRV{\SGM{x}}{x} = \frac{1}{1+e^{-x}}\frac{e^{-x}}{1+e^{-x}} = \SGM{x}(1-\SGM{x}),
\end{equation*}
thus, during the invocation of chain rule, we have
\begin{equation*}
  \DRV{\tilde{z}_{i-1,k}}{\wDI{ik}{i} \zDI{i}{i} + \tilde{b}_{ik}}=\tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k}).
\end{equation*}
Packing up the gradient for each element in the $i$ th. decoder's output into a vector, we have
\begin{equation*}
  \begin{split}
    \arraycolsep=1.4pt\def\arraystretch{1.5}
    \PDV{\zDI{i-1}{i-1}}{\pDC_{i}}
    &= \left[\begin{array}{c}
        \PDV{\tilde{z}_{i-1,1}}{[\wDI{i1}{i}, \tilde{b}_{i1}]} \\
        \PDV{\tilde{z}_{i-1,2}}{[\wDI{i2}{i}, \tilde{b}_{i2}]} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,k}}{[\wDI{ik}{i}, \tilde{b}_{ik}]} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,d_{i-1}}}{[\wDI{id_{i-1}}{i}, \tilde{b}_{id_{i-1}}]} \\
      \end{array} \right]
    = \left[\begin{array}{c}
        \tilde{z}_{i-1,1}(1-\tilde{z}_{i-1,1}) [\zDIt{i}{i}, 1] \\
        \tilde{z}_{i-1,2}(1-\tilde{z}_{i-1,2}) [\zDIt{i}{i}, 1] \\ \vdots \\
        \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k}) [\zDIt{i}{i}, 1] \\ \vdots \\
        \tilde{z}_{i-1,d_{i-1}}(1-\tilde{z}_{i-1,d_{i-1}}) [\zDIt{i}{i}, 1] \\
      \end{array} \right] \\
    &= \diag{\zDI{i-1}{i-1}} \diag{\one - \zDI{i-1}{i-1}} [\zDIt{i}{i},1], \\ \text{in a similarly way,} \\
    \PDV{\zDI{i-1}{i-1}}{\zDI{i}{i}}
    &= \left[\begin{array}{c}
        \PDV{\tilde{z}_{i-1,1}}{\zDI{i}{i}} \\
        \PDV{\tilde{z}_{i-1,2}}{\zDI{i}{i}} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,k}}{\zDI{i}{i}} \\ \vdots \\
        \PDV{\tilde{z}_{i-1,d_{i-1}}}{\zDI{i}{i}} \\
      \end{array} \right]
    = \left[\begin{array}{c}
        \tilde{z}_{i-1,1}(1-\tilde{z}_{i-1,1})\wDI{i1}{i} \\
        \tilde{z}_{i-1,2}(1-\tilde{z}_{i-1,2})\wDI{i2}{i} \\ \vdots \\
        \tilde{z}_{i-1,k}(1-\tilde{z}_{i-1,k})\wDI{ik}{i} \\ \vdots \\
        \tilde{z}_{i-1,d_{i-1}}(1-\tilde{z}_{i-1,d_{i-1}})\wDI{id_{i-1}}{i} \\
      \end{array} \right] \\
    &= \diag{\zDI{i-1}{i-1}} \diag{\one - \zDI{i-1}{i-1}} \WDI{i}{i-1}{i}
  \end{split}
\end{equation*}

Besides the gradient of the $i$ th. decoder ouput with respect to  its own paremeters, notice the input of the $i$ th. decoder layer, $\iDi$, is in fact a function of every structure parameter in the layers below, decoder and encoder regardless. Let's deal with the decoders first. Using the chain rule, the gradient of $\oDi$ with respect to the parameters in any of the lower decoder layer $i$ is:
\begin{equation*}
\begin{split}
  \PDV{\oDi}{\pDC_j}
  &= \PDV{\oDi}{\iDi} \PDV{\iDi}{\pDC_j} \\
  &= [\oDi \odot (\one{} - \oDi)]^\prime \WDi \PDV{\iDi}{\pDC_j}, 
\end{split}
\end{equation*}
where $\pDC_j$ are parameters in the lower, $j$ th. decoder layer ($i < j$). Puting together the above calculations, we see for the out of $i$ th. decoder layer, $\oDi$, its gradient with respect to any structure parameters within itself or the other decoders down below can be calculated by recursively invoking the chain rule:
\begin{equation*}
\begin{split}
  \PDV{\oDi}{\pDC_j} =
  \begin{cases}
    [\oDi \odot (\one{} - \oDi)]^\prime \WDi \PDV{\zDC_{i+1}^{d_{i+1}}}{\pDC_j} & 0 \le i < j < M \\
    \oDi \odot (\one{} - \oDi) [\zDC_{i+1}^{d_{i+1}\prime}, 1]                  & 0 \le i = j < M
  \end{cases}
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
  \zDC_i &= s(\WDC_{i+1} \zDC_{i+1} + \bDC_i)
\end{split}
\end{equation*}

\begin{equation} \label{eq:unit encoder-decoder}
\begin{split}
  \boldsymbol{\tilde{z}_{i-1}^{d_{i-1}}} &= s(\boldsymbol{\tilde{W}_{i  }^{d_{i-1} \times d_{i  }} z_{i  }^{d_{i  }}}+\boldsymbol{\tilde{b}_{i-1}^{d_{i-1}}}) \\
  \boldsymbol{z_{i  }^{d_{i  }}}         &= s(\boldsymbol{W_{i  }^{d_{i  } \times d_{i-1}} z_{i-1}^{d_{i-1}}}+\boldsymbol{b_{i  }^{d_{i  }}}), \\
\end{split}
\end{equation}
where $i=(1 \ldots M)$. Thus, instead of pipping the output of the $i th$ encoder layer ($\boldsymbol{z_{i  }^{d_{i  }}}$) to the one above, it is pipped to the cooresponding $M-i th.$ layer in the docoder counterpart for an immediate reconstruction of the input $\boldsymbol{z_{i-1}}$. The unit encoder--decoder tuple can then be trained by minimizeing the reconstruction loss $L(\boldsymbol{z_{i-1}},\boldsymbol{\tilde{z}_{i-1}})$. A total of $M$ units can be formed and trained seperatetly. Although the pre-training cannot be done in parallel since the higher unit have to wait the lower for input, the process is very fast because the number of parameters is rather small in each unit compared with the whole network. The pre-training serves as a non-random parameter initilization, after which the whole network is already near convergence. Therefore, the subsequent fine-tuning of entire network would required much less steps to reach convergence.

The SA thus trained may suffer the overfitting problem, that is, the network abstracted well on the facts so far presented to it, showing low reconstruction loss, but may deals poorly for the future, unknown data. To better balance the internal and external validy, one way is to add a regulator term to the loss funciton $L$, usually the $L_1$ or $L_2$ norm of the parameter elements. Doing so discourages large parameters which causes instability of the prediction/reconstruction. For encoder training, another way to ensure generalizability is to randomly corrupt the input $\xEC^P$ by setting some elements to zero [2008 Vincent]. The rationel is, the more rigours a feature is, the more like it will survive the random corruption. More training epoch is needed to ensure the randomness of corruption, but doing so encourages the network to seek major features of the input while neglecting the minor changes between facts. And since major feature are more likely to show up in the future, the external validty of the encoder is thus preserved.

For the current study howver, the purpose of deep-learning is not to construct an encoder encrypted with universal knowledge regarding white matter surface, but to produced the abstration the surface vertices at hand for the up-coming analysis. In other words, the code $\boldsymbol{\hat{y}}$ is the only concern here, and the external validty will be implictly handled by the subsequent U-statistical analysis using such code. For now, an ad-hoc encoder trained without regulator term or ramdom corruption would surfice. The current implementation is a five layered encoder, each halves the dimensionality of its input. Thus the final encoding will be 32 times smaller then the raw input.
