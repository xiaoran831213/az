\section{Introduction}
The decade long search of casual variant by genome wide associatio analysis (GWA) hasn't been satisfying. So far GWA hardly find any single nucleotite variant (SNV) with an large enough effect to act as a stand along necessary cause of any complex diseases. Although a large number of statistically significant common variants were indeed identified by GWA, only a moderate fraction of heritability have been explained by the totality of these finding\cite{GWA1, GWA2}. Despite the setback, human genome is still an intriging source of curiosity owning to its intrisic advantages. When viewed as an exposure, genetic polymophisim is consistant throughout an individual's life course and all types of organism, saving the complication of study design. Also, as one of the fundermental causes of all biological processes, genomic polymophisim is not suseptable to reverse causality. From a population perspective, the occurance of genetic variation mimic a random assignment of treatment in an quasi-experiment, which in turn can be exploited to infer non-genetic effect through an intrumental variable approach, such as Mandilian Randomization \cite{MR1, MR2}. These features keeps genomic analysis a promising tool for casual inference.

\subsection{Analysis of Rare Genomic Variants}
The "rare variant, common disease (RVCD)" hypothesis aims to to explain of the "missing heritability" which GWA failed to capture. RVCD states the gap could be attributed to rare variants of moderate to large effect not covered by GWA \cite{RVCD1}. The Next Generation Sequencing (NGS) projects, growing in both number and scale over the last decade, offered numerous data sources for the analysis of rare variants. However, the stockpiling data also raise a number of methodological challenges. For one, the variants in a NGS profile is much denser than a GWA profile, which poses intense computation and multiple testing should the traditional per-variant based screening procedures is applied. Also, as the name suggested, the newly detected rare variants came with their minor allele frequencies (MAF) close to 0. As a consequence, the lack of heterogeneity in genotype threatens the statistical power for studies of moderate and small sample size. So far the most popular remedy is signal aggregation, that is, instead of screening the whole profile one variant after another, we first group the variants accroding to certain criteria, and subsequently all the variants in one group are tested together as an unit, during which the signal of the members are aggregated in certain way. The aggregation can be achieved by either collapsing the grouped variants into a single variant \cite{Burden1} before the statistical test, or by testing all the variants together with a multivariant approach \cite{HWU, SKT}. The aggregation can also be done after a per-variant screening, by summarizing the test statistics (e.g. p-values) of group members into one statistics \cite{Dai:2015, plink1}. Grouping and aggregation drastically reduce the number of hypothesis to be tested, and a grouped testing unit have a much improved heterogeniety over any of its member variant. A difficulty comes alongside this approach though, is the choice of grouping criteria. The most common criteria refer prior knowledge of biological function, resulting in gene or pathway based grouping. One could also group the variants by every few kb[?] or by a threshold of linkage disequilibrium (LD) \cite{plink1}. Since the focus of this study is hypothesis testing procedure, for now, we adopt a gene based grouping scheme for both simulation and real data analysis.

\subsection{Incorporating Imaging Data}
An important factor contributing to the unsatisfactory performance of GWA is the genetic effect being intrinsically weak over a complex disease, due to the large ``black box'' between the upstream genomic variants and the health outcome at the downstream far end. It is desirable to probe the ``black box'' by incorporating other intermidiate biological profiles. The added information should increase the chance of detecting strong association, especially when the new profiles is in the casual pathway from the genomic profile to the disease. Our method borrows additional information from neuroimaging data, knowning that the brain structure is a powerful predictor of neurological disorder, and its association with the genomic profile can be captured with proper statistics. Much like the genomic analyis, similar procedures can be applied to image profiles given if proper definition of a variant and its value is given. Taking the structured MRI as an example, it is natural to view a voxel in the pile of slices as a variant, and the normalized brightness of that voxel as its value. Following the definition of image variant and value, as an analogy of GWA, a per-voxel screening procedure can then be applied to the entire profile to detect signifiant loci in the brain, which is usually called voxel-wise analysis (VWA) \cite{VWA1, VWA2, VWA3, VWA4}. For our study though, the neuroimaging profiles are first processed by \FS, a freely distributed neuroimaging processing pipeline. Thus, instead of the orignal structure MRI slices, the 3D cortex reconstructed from these slices is treated as our image profile, and corespondingly, the image units are vertices spanning the cortical surface instead of voxels in the original MRI slices. The software package \FS also came with its own package for vertex-wise analysis (also abbreviated to ``VWA''), which is essentially applying generalized mixed linear model for each vertex \cite{FS:Anl1, FS:Anl2}. The per-variant screening of image profile may work better than the same procedure applied to genomic profile since the continuous value of a image variant (e.g., brightness of a voxel or thickness at a vertex) do not suffer the low variability issue like a rare genomic variant does. However, the multiple testing might be severer than the genomic profile because the variants in a image based profile are densely placed in the 3D space, so the neighboering variants cover tightly connected cerebral tissue, which means their values are highly coorelated. Thus, the grouping and aggregation strategy is also practical for any analysis involving image profiles, which is also adapted by our method. As an analogy of gene based grouping and signal aggregation, it is most nature to segregate the cortex into 68 well recognized functional anatomial regions (34 for each hemisphere) and treat each of them as an analytical unit. In this study, we will compare the power performance of aggregated unit with the per-variant based VWA.

\subsection{Dimension reduction using Deep Artifical Neural Network}
The typical testing unit of the image profile is also high dimensional, oftentimes even higher then most of the genomic testing units. In our current study data, after the cortex profile being partitioned into 68 functional anatomy regions by \FS, the chance of a randomly picked cortex region holding more variants than a randomly picked gene is as high as 96.8\%. Although the multivariate algorithms that works with genomic testing unit can be applied to the cortex with little to no rework, it would be desirable to first reduce the dimensionality of a image based test unit before the analysis. One popular trend is to ultilize deep artificial neural network to extract high order features from the raw profile, which not only lowers the dimensionality but also raises the signal to noise ratio \cite{DL:Intro1}. An important potent of deep network lies in its capability to accumulatively encrypt incoming knowledge with unsupervised training technique. In other words, as long as the future collection of data profile follows protocols compatible to the current, the existing deep network can be incrementally refined to achieve feature extraction that is more informative and more concise. For now, our method included a stacked autoencoder (SA) to process the cortex profile, which is a ralatively light weighted deep network whose feature extraction capability is comparable to the more powerful but much slower deep belief network \cite{DL:SDA1, DL:Intro2}. The construction and calibration of the stacked autoencoder is detailed in the method section and the appendix. If the feature abstraction of the cortex profile is proven to be helpful, the deep neural network could also be used for the futuristic genomic analysis, because the dimensionality of genomic profile is growing alongside with the advancing genotyping technology and sample size, which could benifit from high order feature extraction as well.

\subsection{Association Analysis of multiple high-dim Component}
The introduction of medical image also complicated the constitution of association. In most cases, an investigator does not know the effect composition in advance, that is, the variation of phenotypes can be attributed to either genomic polymorphism or cortical vertex alone, or both, either with or without some unknown type of interaction, mediation or even feedback loops among genomic, cortex and phenotype profiles. Thus, the method in mind must be sensitive to the association but at the same time robust enough to maintain statistical power when the putative mode is unavoidably mis-representing the reality. Also, the value of genomic and cortical variants could come from distinct, non-normal distinct distriubtions, while the phenotype profile could also be multivariate (e.g. disease diagnosis with additional demographics and known risk factors), with each element following unknown distribution. Taking these uncertainty into consideration, the test statistic should also be versatile enough to counter an admixture of possibally skewed, non-normally distributed data component. More ever, even with the grouping and aggregation technique, the method has to be reasonablly fast and powerful, because the combination of two or more profile types can be a huge number (in our case, there are approximately 21,000 genes and 68 cortex regions), which is a computation and multiple testing challange. , and the number  with the high dimensional profiles comprised of up to tens of thousands genomic polymophisms or vertices (before being replaced with high order features).

In general, we are facing an increasingly denser genomic profile with a growing number of rare variants, and the oppertunity to incroporate additional high dimensional biological profiles in to the genomic analysis. These large data however, while offering new possiblities for the exploration of missing heritability, also poses serious methodology challanges in terms of lower power and high dimensionality. We propose use of a similarity U statistics for the association analysis involving genomic and cortex variants, and compare the power performance with the the per-variants based vertex-wise analysis. In addition, we will train an stacked autoencoder with the cortex profile of all available samples, and use it to extract high order features from the profile, after which another U statistical test will be derived by replacing the cortex vertices with the extracted features, and be compared with the original statistic.