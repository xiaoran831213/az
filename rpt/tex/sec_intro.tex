\section{Introduction}
The decade long search of casual variant by genome wide associatio analysis (GWA) hasn't been satisfying. So far GWA hardly find any single nucleotite variant (SNV) with an large enough effect to act as a stand along necessary cause of any complex diseases. Although a large number of statistically significant common variants were indeed identified by GWA, only a moderate fraction of heritability have been explained by the totality of these finding\cite{GWA1, GWA2}. Despite the setback, human genome is still an intriging source of curiosity owning to its intrisic advantages. When viewed as an exposure, genetic polymophisim is consistant throughout an individual's life course and most types of organism, saving the complication of statistical modeling. Also, as one of the fundermental causes of all biological processes, genomic polymophisim as an exposure, is not suseptable to reverse causality that troubles most non-experimantal designs. From a population perspective, the occurance of genetic variation is random, mimicing a random assignment of exposure in an quasi-experiment, which in turn can be exploited to infer non-genetic effect through an intrumental variable approach, embodied by a Mandilian Randomization design\cite{MR1, MR2}. These features keeps genomic analysis a promising tool for weak effect inference in a complex casual netowrk.

The "common variant, common disease (CVCD)" notion led to the GWA era, as its alternative, the "rare variant, common disease (RVCD)" hypothesis states that the "missing heritability" inexplanable by GWA findings could be attributed to unobserved rare variants of moderate to large effect \cite{RVCD1}. In other words, the so called "whole genome" in GWA was not yet up to its name. The Next Generation Sequencing (NGS) projects, growing in both number and scale over the last decade, facilitated the trial of rare variants. However, as the NGS data keep stockpiling, it also poses a number of new methodological challange. For one, the variants in a NGS profile is much more densely located in the genome, than a typical GWA profile typed by microarrays, which poses serious computational and multiple testing issue should the traditional per-variant screening procedures were applied. Also, as expected, most of the newly called rare variants come with minor allele frequencies (MAF) close to 0. As a direct consequence, such lack of heterogeneity in genotype poses a heavy toll on the statistical power for studies of moderate and smaller sample size. In the epidemiology sence, the number of exposed subjects (either cases or controls) is too small to draw any meaningful inference. So far the most widely accpeted solution is signal aggregation, that is, instead of screening the whole profile one variant after another, one first assign the variants to groups accroding to certain criteria, and subsequently all the variants in one group are test as one unit, by aggregating their signal in a certain way. Commanly, the aggregation can be achieved by collapsing the group into a single variable \cite{Burden1}, or by jointly testing all the variants together \cite{HWU, SKT}. Signal aggregation could be post-screening as well, done by first perform a traditional per-variant GWA screening, then the test statistics (e.g. odds ratio) or its derivation (e.g. the p-values) of a group are combined into a single statistics \cite{Dai:2015, plink1}. Through grouping and aggregation, the number of hypothesis testing reduces to the number of groups, and the heterogeniety of genotype is enchanced to the probability of any variation of the group member. A difficulty comes along with the approach however, is the choice of groupingcriteria. The most popular grouping schemes refer prior knowledge of biological function, resulting in gene or pathway based grouping. Some more debatable schemes rely on the physics of the genome, such as binning the genome by every few kb[?] or by a threshold of linkage disequilibrium (LD) \cite{plink1}. The function based grouping is less subjective and better suited for later interpretation, but for the same reason it is also somewhat a ad-hoc approach, since the ultimate goal of grouping these variants by function, is itself the inference of certain function. Another downside of functional grouping is that it can not exhaustively cover the whole genome, because some variants do not fall into any protein coding gene or known pathway. The physical grouping, on the other hand, could comprehensively cover the entire genome, but is somewhat arbitrary, because the the optimal bin size, LD threshold, and starting position of segmentation, are not pre-known by the investegators. The analytical outcome could also be drastcially different if a different setting was choosen. Aside from signal aggregation, another trend is to form huge, multi-site cohort for large sample, or to perform meta-analysis for a large number of published result \cite{META1}, so the sheer number of the observations could overcome the issue of multip testing and low MAF. Now it is not uncommon to apply a brutle per-variant scan for a cohort of more than a thousand subjects. However, the curse of weak effect and low heritability is still lingering, despite frequently reported rare variants reaching statistical significance. The general picture, is still a lack of methodology of moderate prediction power for complexe health outcomes.

The reasons of genetic effects being weak, is in many way like any other fundermantal cause in the framework of epidemiology, such as social economy status, race/ethnicity and birth place. While being persistant and free from feedback loops, this kind of causes have to reach the final outcomes through a huge "blackbox". As a result, sololy altering one dimenstion of the exposure while leaving the other causes and modifiers untouched won't change the outcome by large. Naturely one would seek amending forces in an attempt to unfold the "blackbox", which can be approached from both upstream and downstream ends of the casual network. From the downstream end, the bottom-up approach revoles around the central dogma, that is, DNA transcript to mRNA, and mRNA translate to Protein, and thus the profiles of interest are expanded into transcriptome and proteome, respectively. The most prominent practises baring such priciple are expression quantitative trait loci (eQTL)\cite{eQTL1, eQTL2} and gene network analysis. As for the upstream end, an investegator could collect a variety of strong indicators proximate to the health outcome. Typical examples are biomarkers like inflammatory cytokines, c-reactive protein, serum cholesterol and anti-body titers \cite{cytokine1, CRP1}, and non-biomarkers, such as the neuroimage profiles \cite{VWA1, VWA2, VWA3, VWA4}, which is also included into this study. To be noted however, unfolding the "blackbox" is no easy task since any intermidiate factor in the casual network loses the simplicity of a fundermental cause. Namely, an indicators is susceptable to feedback loops and confounders, thus a strong biomarker does not equate to a strong cause, hince the separation of the concept of cause and indicator. Also, unlike genomic variants, intermidicate indicators are no longer static throughout time and space, for example, the transcriptome of the same subject are quite distince across different types of cell and life cycle. As an matter of fact, most eQTL and gene network analysis are largely seen with, and limited to cancer researches, while the downstream strong biomarkers are usually eligible for short term diagnose and confirmation, but rarely suitable for long term risk prediction. Another difficulty this work trying to address is the joint analysis of both fundermental casuse and intermidiate factors, either from upstream or downstream. Traditional Epidemiology relies on regression to seperate the direct effect of the fundermental cause and the effect mediated throught the intermidiate factors. This is not suitable when the potential causes in question are groups of tens of hundrend genomic variants, and more ever, when the proximate disease indicators, in our case, the neuroimage profile, are even higher in dimensionality compared to an average genomic testing unit.

Alongsided with the NGS data, the neuroimaging profiles have also been actively incroperated in the casual inference and predition models, constructed by procedures similar to those applied to genomic profiles. The association strengh between neuroimaging profile and neurological disorder are much higher than from the genome to the same disease per se. Intuitively, the deteriation of cortial and subcortical tissue is an event proximate to the CNS dysorder, thus a small sample is suffice for the detection of the association does exists. Similar to GWA, one could also define a variant for the image profile, usually goes with an image unit. For the most abundant structure MRI, a variant is usually taken as a cubic voxel in the 3D volumn spanned by a series of MRI slices [pic?]; here a voxel can be seen as the expansion of a square pixel in a 2D plane to the 3D space. The value of such a variant is taken from the intensity of the voxel in the MRI scan, that is, the darkest one has a value of 0, the brightest one is 1. For our study though, the neuroimg profile are not the original MRI and its voxels, but instead we use the 3D cortical surface reconstructed from the structure MRI, performed with the \FS image analysis suite, which is documented and freely available for download online (\url{http://surfer.nmr.mgh.harvard.edu}) \cite{FS:Intro}. Every subject has two cortical surfaces reconstructed, corresponding to the left and right hemispheres, each cortical surface is spanned by 163842 connected vertices, each of which is teated as a variant. The value of a variant(vertex) is no longer the MRI scan brightness, but the gray matter thickness \cite{FS:Tck1, FS:Tck2}, white matter area, gray matter volumn, located around the close proximity of that variant, which were calculated by \FS during the cortical surface reconstruction. The neuroimaging profile, either the brightness of the voxels or the genometry of the vertices, do not suffer poor signal like the rare genomic variants of very low MAF do, because the values of either type of image units are continuous. Therefore, much like GWA, a per-variant screening can be applied to neuroimage profiles to search for signifiant loci. For analysis using the brightness of voxels from the original MRI slices, this is called voxel-wise analysis (VWA) \cite{VWA1, VWA2, VWA3, VWA4}. As for vertices in the reconstructed surface, \FS comes with its own surface analysis tool which is essentially applying generalized mixed linear regression for each vertex \cite{FS:Anl1, FS:Anl2}, the small area around each vertex is then color coated with the cooresponding $-\log{P}$. A clinical association between neuroanatomical region of interest (ROI) and the health outcome is readly checked by comparing the color pattern of that ROI (Figure \ref{fig:FS1}). In such sense, the \FS surface analysis tool can be called vertex-wise analysis, also abbrevated to VWA. However, the \FS tool is not meat for cohort analysis involving hundreads to thousands participants, because the investigation of color coated surface has to be done by human labor. 
\begin{figure}[h]
\includegraphics[width=\textwidth]{FS_GLM}
\caption[\FS's Vertex-Wise Analysis]{\FS's Vertex-Wise Analysis}
\label{fig:FS1}
\textbf{top:} 40 sample cortical surfaces color coated by gray matter thickness;
\textsf{bottom left:} fitted $thickness - age$ slop at one of the 327684 vertices;
\textsf{bottom right:} cortical surface color coated by $-log{P}$ of the $thickness - age$ slop at each vertex.
\end{figure}

Recognizing that the closely located variants are clearly highly correlated, the grouping and aggregation approach can also be used on them to boost statistical power[Zhu et. al.].
Except per-variant tests, before the aggregation of the variants within a testing unit, a number of procedures could be applied to the unit to enhance the later statistical power, which usually involves dimension reduction and noise supression. For genomic profiles, one could fit a 1D curve through the dosage values within a testing unit for each subject [Olga V.]. With a balanced smoothness and goodness of fit, one could reduce the profile differences among the cases as well as the controls and ralatively enlarge the overall difference between the two groups in a small expense of profile accuray. An overall power boost can be thus achieved because the within group difference are most likely contrubited by noise unexplainable by the group membership, and grinding them away with a smooth function makes it easier to distinguish profiles of cases and controls. For image profile, throught a gaussian blur filter one could burn away the trivial details but maintain major features to achieve higher statistical power. The same retionel can be seen that the irregular jumps of the values in an image unit is mainly caused by noise not attributed to the case control membership. 

Up until now there are numerious methods dealing with the genome - disease, image - disease, or even genome - image association. In this study we would like to simoteniously incoperate both genomic and image profile and the disease for the association analysis. To cope with low power and multiple testing, we applied regional aggregation for both profiles.