\section{Introduction}
The decade long search of casual variant by genome wide associatio analysis (GWA) hasn't been satisfying. So far GWA hardly find any single nucleotite variant (SNV) with an large enough effect to act as a stand along necessary cause of any complex diseases. Although a large number of statistically significant common variants were indeed identified by GWA, only a moderate fraction of heritability have been explained by the totality of these finding\cite{GWA1, GWA2}. Despite the setback, human genome is still an intriging source of curiosity owning to its intrisic advantages. When viewed as an exposure, genetic polymophisim is consistant throughout an individual's life course and all types of organism, saving the complication of study design. Also, as one of the fundermental causes of all biological processes, genomic polymophisim is not suseptable to reverse causality. From a population perspective, the occurance of genetic variation mimic a random assignment of treatment in an quasi-experiment, which in turn can be exploited to infer non-genetic effect through an intrumental variable approach, such as Mandilian Randomization \cite{MR1, MR2}. These features keeps genomic analysis a promising tool for casual inference.

\subsection{Next Generation Sequencing data}
The "rare variant, common disease (RVCD)" hypothesis aims to to explain of the "missing heritability" which GWA failed to capture. RVCD states the gap could be attributed to rare variants of moderate to large effect not covered by GWA \cite{RVCD1}. The Next Generation Sequencing (NGS) projects, growing in both number and scale over the last decade, offered numerous data sources for the analysis of rare variants. However, the stockpiling data also raise a number of methodological challenges. For one, the variants in a NGS profile is much denser than a GWA profile, which poses intense computation and multiple testing should the traditional per-variant based screening procedures is applied. Also, as the name suggested, the newly detected rare variants came with their minor allele frequencies (MAF) close to 0. As a consequence, the lack of heterogeneity in genotype threatens the statistical power for studies of moderate and small sample size. So far the most popular remedy is signal aggregation, that is, instead of screening the whole profile one variant after another, we first group the variants accroding to certain criteria, and subsequently all the variants in one group are tested together as an unit, during which the signal of the members are aggregated in certain way. The aggregation can be achieved by either collapsing the grouped variants into a single variant \cite{Burden1} before the statistical test, or by testing all the variants together with a multivariant approach \cite{HWU, SKT}. The aggregation can also be done after a per-variant screening, by summarizing the test statistics (e.g. p-values) of group members into one statistics \cite{Dai:2015, plink1}. Grouping and aggregation drastically reduce the number of hypothesis to be tested, and a grouped testing unit have a much improved heterogeniety over any of its member variant. A difficulty comes alongside this approach though, is the choice of grouping criteria. The most common criteria refer prior knowledge of biological function, resulting in gene or pathway based grouping. One could also group the variants by every few kb[?] or by a threshold of linkage disequilibrium (LD) \cite{plink1}. Since the focus of this study is hypothesis testing procedure, for now, we adopt a gene based grouping scheme for both simulation and real data analysis.

\subsection{Imaging Data Analysis}
An important factor contributing to the unsatisfactory performance of GWA is the genetic effect being intrinsically weak over a complex disease, due to the large ``black box'' between the upstream genomic variants and the health outcome at the downstream far end. It is desirable to probe the ``black box'' by incorporating other intermidiate biological profiles. The added information should increase the chance of detecting strong association, especially when the new profiles is in the casual pathway from the genomic profile to the disease. Our method borrows additional information from neuroimaging profiles, knowning that the brain structure is a powerful predictor of neurological disorder, and its association with the genomic profile can be captured with proper statistics. Much like the genomic analyis, similar procedures can be applied to image profiles given if proper definition of a variant and its value is given. Taking the structured MRI as an example, it is natural to view a voxel in the pile of slices as a variant, and the normalized brightness of that voxel as its value. Following the definition of image variant and value, as an analogy of GWA, a per-voxel screening procedure can then be applied to the entire profile to detect signifiant loci in the brain, which is usually called voxel-wise analysis (VWA) \cite{VWA1, VWA2, VWA3, VWA4}. For our study though, the neuroimaging profiles are first processed by \FS, a freely distributed neuroimaging processing pipeline. Thus, instead of the orignal structure MRI slices, the 3D cortical surface reconstructed from these slices is treated as our image profile, and corespondingly, the image units are vertices spanning the cortical surface, instead of the voxel spanning the MRI slices. Also abbreviated The to ``VWA'', the software package \FS also came with its own vertex-wise analysis component, which is essentially applying generalized mixed linear model for each vertex \cite{FS:Anl1, FS:Anl2}.

\subsection{Process high dimensional profile with Deep Artifical Neural Network}
The typical testing unit of the image profile is also high dimensional, oftentimes even higher then the genomic testing units. As for our study data, after the cortical surface profile being partitioned into 68 anatomical regions using \FS, some larger regions hold more than ten thousands vertices -- greater than the number of variants in all genes. Although the multivariant techniques that worked with genomic testing unit can be readily applied to the cortical surface vertices as well, it is desirable to first reduce both the dimensionality and noise of a large unit before the statistical analysis. One popular trend is to ultilize deep artificial neural network to extract high order features from the raw profiles, which has low dimensionality and low noise -- information ratio \cite{DL:Intro1}. An important potent of deep network lies in its capability to accumulatively encrypt incoming knowledge when coupled with unsupervised training technic. In other words, as long as the future collection of image profiles follow protocols compatible to the current, the existing deep network can be continuously refined to achieve feature extraction that is more informative but yet more concise. Another potent of deep network is relevent to the future genomic analysis, because the dimensionality of genomic testing unit is growing with the ever deepening genotyping technology, should the feature extraction proven to be useful for the image profiles, it could be generalized to the genomic profile as well. Included in our method is an stacked autoencoder (SA), which is a ralatively light weighted deep network whose feature extraction capability is comparable to the more powerful but much slower deep belief network \cite{DL:SDA1, DL:Intro2}. The construction and calibration of the stacked autoencoder will be detailed in the method section and the appendix.

In general, we are facing an increasingly denser genomic profile with a growing number of rare variants, and the oppertunity to incroporate other high dimensional biological profiles in to the genomic analysis. These large data however, while bringing new possibility to explore the missing heritability, also poses serious methodology challanges in terms of lower power and high dimensionality. We propose the use of a similarity U statistics for the association analysis involving genomic variants and cortical surface vertices, both of which are high demensional. We will than use the per-variants based vertex-wise analysis as a reference to compare the power of the proposed U statistics. In addition, we will train an stacked autoencoder with the raw cortical surface vertex data and utilize it to extract high order feature from the raw profile, after which another comparison of performance will happen between the similarity U statistics using the raw cortical surface vertices, versus the one using the extracted features.