\documentclass[twocolumn]{article}

\usepackage{natbib}

\usepackage{syntonly}
%\syntaxonly
\usepackage{amsmath}

\pagestyle{headings}

\author{Xiaoran Tong}

\begin{document}
\title{A non-parametric method for joint association analysis of sequencing and imaging data}
\maketitle

\begin{abstract}
The next generation genome sequencing and neroimaging technology give rise to large, mult-site cohort with growing wealth of whole genome sequencing (WGS) data and magnetic resonance imaging (MRI) data, which mandates the corresponding analytical methodology capable of utilizing both type of information to identify predictive biomarkers associated with complex disorders. Such attemp, however, are met with "the curse of dimensionality", due to the large number of variants in the genome and images. In this work, we tackled the dimensionality issue of 3D cortical surface vertices by training a deep encoder with the deep learning algorithm and generating a compact, abstracted representation of the original surface with the encoder per se. An U statistic with profile similarity based weight term were then adopted to evaluate the joint association of encoded surface and genome data with the phenotype. We showed by simulation that the method maintains the correct type 1 error rate, and achieved a statistical power higher then using either genome or image data alone, or using the original surface, or methods relying on large number of per-variant test. To illustrate our approach, we apply the proposed method to the genomic sequencing and neuroimage data from the Alzheimer's disease Neuroimaging Initiative (ADNI).
\end{abstract}

\section{Introduction}
The decade long search of common casual variant by the genome wide associatio study (GWAS) hasn't been very successful, so far GWAS hardly find any single nucleotite variant (SNV) with an effect large enough to act as a stand along necessary cause of any chronic diseases. Although a large number of statistically significant common variants were inditified by GWAS, only a small fraction of the heritability can be explained by these finding\cite{GWAS1, GWAS2}. Despite the setback, the zeal for genomic study never wanned, on the countraroy, the human genome is an ever intriging attraction for curiosity because of design advantages peculiar to a geneomic profile. When viewed as an exposure, genetic polymophisim is consistant throughout a individual's life, saving the complication of organism specific sampling and life course based modeling. Also, as one of the fundermental causes of all biological processes, such an exposure is not suseptable to reverse causality that plague most observational studies. From a population perspective, the occurance of overall genetic variation is at random, therefore the gentic polymophism mimics a random assignment of exposure in an quasi-experiment, which in turn can be exploited to infer the effect of non-genetic variables through an intrumental variable design, such as the Mandilian randomization \cite{MR1}. These features makes genomic analysis a promising tool for inferencing the weak effects in a complex casual netowrk.

As an alternative of the "common variant, common disease (CVCD)" notion that led us to the GWAS era, the "rare variant, common disease (RVCD)" hypothesis states that the "missing heritability" inexplanable by GWAS findings could be attributed to untyped rare variants \cite{RVCD1}. In other words, the so called "whole genome" in GWAS was not yet up to its title. The Next Generation Sequencing (NGS) projects, growing in both number and scale over the last decade thanks to the ever cheapening genotyping cost, facilitated the study of rare variants. Similar to the energence of GWAS however, the WGS era is not without methodological challange, driven by the stockpiling of enormous new information. The variants in a NGS profile is much denser, with more than 100 fold increase in the number of variants compared to the GWAS typing. Therefore, the WGS profile poses serious computational and multiple testing issue if the traditional per-variant screening procedure was left unchanges. To make the inference even harder, most of the newly called variants are rare ones, who have minor allele frequencies (MAF) close to 0, resulting in a lack of heterogeneity that is crucial for statistical inference. In an epidemiology sence, the number of exposed subjects (either diseased or not) is too small to draw any meaningful inference by seeing the rare variants one at a time. So far the most widely accpeted solution is signal aggregation, that is, instead of screening the dense genome one variant after another, a group of variants are tested as one unit. The aggregation can be achieved by collapsing the group into a single variable \cite{Burden1}, or by jointly testing all the variants together \cite{Wei:HWU, Wu:SKAT}. One could also first perform a traditional per-variant screening, then combinded the test statistics (e.g. the p-values) of the group into a single statistics \cite{Dai:2015, plink1}. Through aggregation, the number of hypothesis testing shinkes to the number of groups, and the heterogeniety of exposure is enchanced to the probability of any variation among grouped variants. A trouble comes with the signal aggregation however, is the choice of grouping scheme. The grouping decision could refer prior knowledge regarding biological function, resulting in gene or pathway based grouping. It could also use the physics of the genome, such as binning the genome by every few kb[?] or by a threshold of linkage disequilibrium (LD) \cite{plink1}. The function based grouping is less subjective and better suited for later interpretation of biological function, but for the same reason it is also somewhat a ad-hoc circulating argument, since the ultimate goal of grouping the variants by function is itself the inference of function. Another drawback of function based grouping is non-exhaustiveness, because not all variants fall into a protein coding genes or an known pathway. The physiccal grouping, on the other hand, could comprehensively covering the entire densely typed genome, but is somewhat arbitrary due to the lack of optimal bin size, LD threshold. The analytical outcome could also be drastcially different if a different starting starting position on a chromosome was choosen. Besides aggregative methods, another trend is to form huge, national-wide multi-site cohort before the analysis, or to perform meta-analysis of a large number of published reseach [?], so the sheer size of the pool of obervations could overcome the issue of multip testing and infrequent exposure (i.e. low MAF), thus a brutle per-variant scan become applicable. Screening with large sample has been fruitful in discovering statistically significant rare variants. Besides loosely hinting or confirming celluar level etiology pathway, however, these new casual variants did not significantly help the prediction of complex diseses, due to the low effect issue inherited from the GWAS findings.

Despite the desirable properties of the genomic profile, being a collection of fundermental causes of all disease also means being remote and highly non-specific, leaving a huge "blackbox" between the genome profile and the disease. The consequences is the aforementioned dilemma of weak effect, that is, new found variants being statistically significant but clinically meaningless. To unfold the blackbox, investigators would collect the intermediate profiles along the etiology pathway and incroperate them into the functional inference. On could do so in a bottom-up or top-down fassion. A bottom-up approach revoles around the central dogma, that is, DNA transcript to mRNA, and mRNA translate to Protein, and the corresponding profiles of interest are called transcriptome and proteom, respectively. A common practice in cancer researches is to first perform a expression quantitative trait loci (eQTL) \cite{eQTL1} screening to select pairs of significantly associated genomic variant and transcriptomic variant in the cancer cells, followed by another screening for significant associations between the selected pairs and the clinical profile \cite{eQTL2}. For a pair of genomic variant and transciptomic variant, linear regresstion are usually adopted to roughly untangle the direct effect from the genomic variant to the outcome and the indirect effect mediated through the transcriptomic variant. To explore the blackbox in a top to bottom manner, the investegators would collect indicators close to the disease outcome on etiology pathway, such as inflammatory cytokines and c-reactive protein, etc \cite{cytokine1, CRP1}. One thing to be noted is that the profile of interest is not  not necessarily a direct biological measure, such as the neuroimaging data adapted by the current study.

Alongsided with the NGS projects, the neuroimaging profile has also been actively engaging in analytical procedures similar to those applied to genomic profiles. The strengh of association between neuroimaging profile and the neurological disorder are stronger than those between the genome and the disease per se, because the deteriation of cortial and subcortical tissue is a very proximate upstream event of central nerve system dysorder. One could define a variant for the image profile with image unit. For the most easily obtained structure MRI, a variant is a cubic voxel in the 3D volumn spanned by a series of MRI slices [pic?]; here a voxel is an analogy of a squared pixel in a 2D plane. The value of a variant is taken as the intensity of the voxel, that is, the darkest one has a value of 0, the brightest one is 1. For our study though, the image profile is a 3D cortical surface spaned by 327684 connecting vertices, reconstructed from the original structure MRI [Freesurfer]. Each vertex is teated as a variant. The neuroimaging data do not suffer the lack of heterogeneity like the NGS data, because the values of the image units are always continuous, in other words, there is no rare variants. Therefore, a GWAS like per-variant screening can be applied to image profiles to search for signifiant loci, which is called voxel-wise analysis (VWA) \cite{VWA}. A clinical association can then be readily established by replacing the significant loci with the the anatomy regions where they are located. The casual inference is somewhat complicated however, because the neuroimage is not immune to reverse casality, that is, the structure and the health status can mutually reshaple each other. From an analytical point of view, the neuroimaging also poses multiple testing issue because of the large number of voxels or vertices. Recognizing that the closely located variants are clearly highly correlated, the grouping and aggregation approach can also be used on them to boost statistical power[Zhu et. al.].

Except per-variant tests, before the aggregation of the variants within a testing unit, a number of procedures could be applied to the unit to enhance the later statistical power, which usually involves dimension reduction and noise supression. For genomic profiles, one could fit a 1D curve through the dosage values within a testing unit for each subject [Olga V.]. With a balanced smoothness and goodness of fit, one could reduce the profile differences among the cases as well as the controls and ralatively enlarge the overall difference between the two groups in a small expense of profile accuray. An overall power boost can be thus achieved because the within group difference are most likely contrubited by noise unexplainable by the group membership, and grinding them away with a smooth function makes it easier to distinguish profiles of cases and controls. For image profile, throught a gaussian blur filter one could burn away the trivial details but maintain major features to achieve higher statistical power. The same retionel can be seen that the irregular jumps of the values in an image unit is mainly caused by noise not attributed to the case control membership. 

Up until now there are numerious methods dealing with the genome - disease, image - disease, or even genome - image association. In this study we would like to simoteniously incoperate both genomic and image profile and the disease for the association analysis. To cope with low power and multiple testing, we applied regional aggregation for both profiles.

\section{Material}
Whole genome sequencing (WGS) and magnetic resonance imaging (MRI) data were obtained from Alzheimer’s Disease Neuroimaging Initiative (ADNI). A totol of 808 subjects at the screening and baseline of ADNI1 and ADNI2 study have both types of profiles sampled, alongside with Alzheimer Disease diagnosis, demographics, and the genotype of APOE $\epsilon$4.

The MRI data first went throught a series of preprocessing pipeline composed of special registration, skull stripping, cortical/subcortical segmentation, white/gray matter segregation, vexel intensity normalization, reconstruction of brain surface, surface registration and virtual anatomical paceration. The whole pipeline is supported by \emph{FreeSurfer} - a software initiated by [Dale and Fischl et.al.] and currently maintained by \textit{the Laboratory for Computational Neuroimaging (LCN)}  at \textit {the Athinoula A. Martinos Center for Biomedical Imaging}. The preprocessing produces a 3D brain surface, with each hamersphere expressed by $10\times2^16$ vertices connected by triangles, and vitually segemented into 34 anatomical regions [pic ?]. For each vertex, besides its location in the 3D space, statistics ascribing the tissure at its close vicility were also calculated, among which the most important properties are gray matter thickness and sulcity (positive means the vertex is in a gyrus, otherwise in a sulcus). For real data analysis, the 68 anatomical regions are treated as testing unit; for a single iteration in a simulation study, small ovals of $512=2^9$ vertices (mean diametter=28mm) were randomly picked from the whole surface.

Rigorous quality control had been done by ADNI during variant calling process, thus the WGS data from ADNI do not require intensive preprocessing. For genomic profile, the testing unit for both real data and simulation study are based on gene. The chromosome location of known genes were queried from the table of genomic features of human reference genome assemble version 38, maintained by Genome Reference Consortium (GRCh38). An extra 5k flanking basepairs were attached to both ends of a gene when searching for vairants in a testing unit. Despit the added flanking region, some unit contains no genomic variant, and they were excluded from further opertations.

\section{Method}

To simoteniously detect association among genetics, neuroimaging and phenotype profiles, we implemented a generalized multivariate similarity U statistic postulated by [changshuai et. al.]. Of all three profiles, the cortical surface reconstructed from MRI scan has the most numerous variants, such that the 68 anatomical region are comprised of few hundreds to more than ten thousands of vertices. The deep-learning algorithem is used to construct a Staked Denoising Autoencoder (SDA) capable of extraplating higher order features from the surface vertices, resulting in a 32 fold reduction of dimenstionality, and a reduction of noise. The U statistic is then derived by summerizing the product of pairwise similarity between subjects, with respect to the aforementioned genetic and phenotipic profiles, and the encoded vertices. 

\subsection{Deep Vertex Encoder}
The purpose of an encoder is to abstract high order features out of the raw information, resembling the visual processing of a sentient being. A real life example is vewing a painting: what intrests the viewer is the concept of object or event the painting captures, not the exact RGB color at every inch of the canvas. The former -- the concept object or event, is the high order feature abstracted from the the later -- the exact color at pixels. For neuroimaging, being able to see the overall shrinkage and laceration sites in a aged brain is far more important than knowning the exact brightness of the voxcels in a series of MRI slice. The voxcels was no doubt an objective recording of the brain with creditable accuracy, yet it is remotely helpful for diagnostic purpose before the extraploation of higher order features. The encoder to be implemented is a mechanical emulation of the sentinental visual contact of the reality. As the definition suggest, the abstraction must reduce the complexity of the raw visual contact considerablly so the host could possibly make an accessment, which, in the language of an encoder, the dimensionality of the input is reduced.  

The predecessor of the deep encoder is the Multiple Layer Perceptron (MLP), one of the earlist biologically inspired neuronetworks among Artifical Intellegence (AI) framework. An MLP allows one directional flow of data (e.g. a clique of clinical readings) through layers of "neurons" to reach a decision at the top (e.g. disease diagnosis). The neurons are fully connected between two adjecent layers, but not connected at all within a layer or non-adjecent ones. In symbolic form, using superscript $d_.$ to show the dimensionality of the data, the input for layer $\#i$ is the $d_{i-1}$ dimensional output of the layer below ($\#i-1$), denoted by vector $\boldsymbol{z_{i-1}^{d_{i-1}}}$.  The concept abstraction happens at layer $\#i$, is a linear recombination of $\boldsymbol{z_{i-1}^{d_{i-1}}}$'s elements performed by scaling matrix $\boldsymbol{W_i^{d_{i-1}\times d_i}}$, followed by the addtion of an offset $\boldsymbol{b^{d_i}}$, then followed by an element-wise non-linear transformation $s$. Thus, the output of layer $\#i$ is a $d_i$ dimensional vector:
\[ \boldsymbol{z_i^{d_i}} = s(\boldsymbol{W_i^{d_i\times d_{i-1}}z_{i-1}^{d_{i-1}}}+\boldsymbol{b^{d_i}})\].
The usual choice of non-linear transformation $s$ is a sigmoid curve, because their "S" shape resembles the biological activation of a neuron, and its continuality and differentiability over $R$ facilitates subsequent computation. Amoung the family of sigmoid curves, the most commonly used is the logistic fucntion: 
\[s^{logistic}(x)=\frac{L}{1+e^{-k(x-x_0)}}\],
where $x_0$ is the midpoint of the sigmoid curve, $L$ is the curve's maximum value, and $k$ controls steepness of the curve. Since a normalized calculation will fix the upper bound of $L$ at 1, and the midpint is replaced by the offset $\boldsymbol{b^{d_i}}$, only the steepness $k$ is flexible. Without pre-knowledge, one could set $k$ to 1 by default, simplifying the logistic curve to the reverse logit: 
\[s^{logit^{-1}}(x)=\frac{1}{1+e^{-x}}\].
The biological connection from neurons in layer $\#i-1$ to those in $\#i$ is emulated by the linear recombination $\boldsymbol{W_i^{d_{i-1} \times d_i}}$. The offset $\boldsymbol{b^{d_i}}$, together with a steep sigmoid curves $s$, emulate the activation threshold of the "neurons", that is, the aggregated input signal must be strong enough to switch a neuron from 0 to 1.
Although not mandatory, $d_i$ is usually chosen to be smaller then $d_{i-1}$, reflecting the fact that, higher concept abstracted from the lower, more chaotic input has reduced complexity. 

An MLP of $M$ layers, $P$ dimensional raw input $\boldsymbol{x}$, and $Q$ outcome classes is constucted by recursively pluging the output of $i-1 th.$ layer into the $i th.$ layer, and ensuring the output of the top layer is $Q$ dimensinal:
\begin{equation} \label{eq:MLP}
  \begin{split}
    \boldsymbol{\hat{y}}=
    \boldsymbol{z_{M  }^{d_{M  }}}         &= s(\boldsymbol{W_{M  }^{d_{M  } \times d_{M-1}}z_{M-1}^{d_{M-1}}}+\boldsymbol{b_{M  }^{d_{M  }}}) \\
    \boldsymbol{z_{M-1}^{d_{M-1}}}         &= s(\boldsymbol{W_{M-1}^{d_{M-1} \times d_{M-2}}z_{M-2}^{d_{M-2}}}+\boldsymbol{b_{M-1}^{d_{M-1}}}) \\
    ... &= ... \\
    \boldsymbol{z_{i  }^{d_{i  }}}         &= s(\boldsymbol{W_{i  }^{d_{i  } \times d_{i-1}}z_{i-1}^{d_{i-1}}}+\boldsymbol{b_{i  }^{d_{i  }}}) \\
    ... &= ... \\
    \boldsymbol{z_{2  }^{d_{2  }}}         &= s(\boldsymbol{W_{2  }^{d_{2  } \times d_{1  }}z_{1  }^{d_{1  }}}+\boldsymbol{b_{2  }^{d_{2  }}}) \\
    \boldsymbol{z_{1  }^{d_{1  }}}         &= s(\boldsymbol{W_{1  }^{d_{1  } \times d_{0  }}z_{0  }^{d_{0  }}}+\boldsymbol{b_{1  }^{d_{1  }}}) \\
    \boldsymbol{z_{0  }^{d_{0  }}}         &= \boldsymbol{x}
  \end{split}
\end{equation}
The initial $P$ dimensional input $\boldsymbol{x}$ is treated as the output of the non-existing layer $\#0$, denoted by vector $\boldsymbol{z_0^{d_0}}$ where $d0=P$. Reading from the bottom to the top, the MLP graduelly extrapolate higher order features from the $P$ dimensional raw input $x$, until the dimensionality of information reduces to $Q$ -- the number of outcome classes(thus $d_M=Q$). The output vector $\hat{y}$ at the top has its elements taking values between $[0,1]$, represent the confidence in each outcome category.

A MLP thus constructed must be tuned to be truely useful in outcome prediction, that is, to find a set of parameters $\boldsymbol{\theta}=\{\boldsymbol{W_{1:M}},\boldsymbol{b_{1:M}}\}$ that yield optimal sensitivity and specificity given the known facts, a procedure called machine learning or training. The "known facts" are tuples of $(x, y)$ representing the observed raw input and true outcome, for example, $x^q$ could be a $M$ item food checklist taken from a study subject, and $y^q$ is a $q$ dimensional binary coding of hypertension, cardiovascular disease, diabetes and non-disease (thus $q=4$ in this case). The hope is, after surficient training with known facts, the MLP could make creditable assessment of unknown future diseases given a new dietary checklist, and the parameters $\boldsymbol{\theta}$ so found will be an encrypted knowledge relating diet to chronic disease. Leaning is done by minimizing the value of an loss function $L$ who measures the wrongfulness of the prediction made by the MLP ($\boldsymbol{\hat{y}^q}$) relative to the actual fact observed ($\boldsymbol{y}$). 
One choice of the total loss of N obervations is the sum of Euclidian Distance (ED) between predictions and true outcomes:
\begin{equation*}
\begin{split}
  L^{ED}(\boldsymbol{\hat{y}},\boldsymbol{y}) = \sum_{i=1}^N\sum_{k=1}^Q\frac{1}{Q}(\hat{y}_{ik}-y_{ik})^2,
\end{split}
\end{equation*},
For now, the Cross-Entrophy (CE) based loss function is more frequently used:
\begin{equation*}
\begin{split}
\noindent L^{CE}(\boldsymbol{\hat{y}},\boldsymbol{y}) = \\
  -\sum_{i=1}^N\sum_{k=1}^Q[y_{ik}\log{\hat{y}_{ik}}+(1-y_{ik})\log(1-\hat{y}_{ik})].
\end{split}
\end{equation*}
Here $N$ is the number of observed facts, indexed by $i$. The search of the best $\boldsymbol{\theta}$ is a numerical optimiation problem:
\[\boldsymbol{\hat{\theta}}=\min_{\theta}L(\boldsymbol{\hat{y}},\boldsymbol{y})\]
The total number of parameters must be turned tune is $|\boldsymbol{\theta}|=\sum_{i=1}^{M}{(d_{i-1}+1) \times d_i}$. The optimization problem can be computationally intense if $|\boldsymbol{\theta}|$ is huge. For a small $\theta$, Newton–Raphson method can be used for fast convergence. Here we use the gradient decent method, starting with a randomly initialized assignment of $\theta^(0)$ at $t=0$, and compute the next assignment $\theta^{(t+1)}$ by substracting a tiny fraction (called a learning step) of the gradient of loss function $L$ with respect to the current assignment ($\frac{\partial L}{\partial \boldsymbol{\theta^{t}}}$). The process will repeat until the loss $L$ cease to drop marketable between two batch of assignments. The final assignment is considered the optimal $\theta$.

The structure of an encoder is identical to an MLP, what's different is how they are trained. Tuning a MLP required a duel of facts known $a priori$: the predictors $x$ characterizing a subject, and the outcomes $y$ predictable by those characteristics, which is called supervised training. For an SDA, the training can be and is usually unsupervised, that is, knowing the outcome $y$ is only optional. However, tunning an SDA requires a decoder counterpart whose purpose is to reconstruct the  high-dimensional raw input $x$ from the abstrated, low-dimensional code $y$. By piping the original feature $x$ through the encoder for the code $\hat{y}$, then piping the code $\hat{y}$ throught the decoder for a reconstructed $\tilde{x}$, the encoder can then be tuned by minimizing the loss between the true data $x$ and the reconstructed $x'$. The retionel is, a optimal encoder should extrapolate the most significant features of the raw data, so the reconstructed version could resemble the true original at best. After the training with known $x$, the SDA can serve as a dimensionality reduction tool.
A decoder is also a series of linear recombination and offseting, followed by non-linear elementwise transformation, recersively applied to the code $\hat{y}$. The most nature way to structure a decoder is to mirror the encoder. Treating the aforementioned MLP \ref{eq:MLP} as an encoder, one could build a corresponding counterpart by expanding the dimension in exact reverse order:
\begin{equation} \label{eq:DEC}
\begin{split}
  \boldsymbol{\tilde{x}}=
  \boldsymbol{\tilde{z}_{0  }^{d_{0  }}} &= s(\boldsymbol{\tilde{W}_{1  }^{d_{0  } \times d_{1  }} \tilde{z}_{1  }^{d_{1  }}}+\boldsymbol{\tilde{b}_{0  }^{d_{0  }}}) \\
  \boldsymbol{\tilde{z}_{1  }^{d_{1  }}} &= s(\boldsymbol{\tilde{W}_{2  }^{d_{1  } \times d_{2  }} \tilde{z}_{2  }^{d_{2  }}}+\boldsymbol{\tilde{b}_{1  }^{d_{1  }}}) \\
  ... &= ... \\
  \boldsymbol{\tilde{z}_{i  }^{d_{i  }}} &= s(\boldsymbol{\tilde{W}_{i+1}^{d_{i  } \times d_{i+1}} \tilde{z}_{i+1}^{d_{i+1}}}+\boldsymbol{\tilde{b}_{i  }^{d_{i  }}}) \\
  ... &= ... \\
  \boldsymbol{\tilde{z}_{M-2}^{d_{M-2}}} &= s(\boldsymbol{\tilde{W}_{M-1}^{d_{M-2} \times d_{M-1}} \tilde{z}_{M-1}^{d_{M-1}}}+\boldsymbol{\tilde{b}_{M-2}^{d_{M-2}}}) \\
  \boldsymbol{\tilde{z}_{M-1}^{d_{M-1}}} &= s(\boldsymbol{\tilde{W}_{M  }^{d_{M-1} \times d_{M  }} \tilde{z}_{M  }^{d_{M  }}}+\boldsymbol{\tilde{b}_{M-1}^{d_{M-1}}}) \\
  \boldsymbol{\tilde{z}_{M  }^{d_{M  }}} &= \boldsymbol{\hat{y}},
\end{split}
\end{equation}
where the superscript shows the dimensionality; the non-linear elementwise transformation $s$ can again be the reverse-logit. Reading bottom-up, the decoder expands details from the abstracted concept ${\boldsymbol{\hat{y}}}$ and eventually reach a reconstruction $\boldsymbol{\tilde{x}}$ of the raw data  $\boldsymbol{x}$. The encoder in (\ref{eq:MLP}) is linked from below by plugin the its output into the bottom of the decoder throught ${\boldsymbol{\hat{y}}}$. The entire encoding -- decoding process can then be written as:
\begin{equation} \label{eq:SDA}
\begin{split}
  \boldsymbol{\tilde{x}}=
  \boldsymbol{\tilde{z}_{0  }^{d_{0  }}} &= s(\boldsymbol{\tilde{W}_{1  }^{d_{0  } \times d_{1  }} \tilde{z}_{1  }^{d_{1  }}}+\boldsymbol{\tilde{b}_{0  }^{d_{0  }}}) \\
  ... &= ... \\
  \boldsymbol{\tilde{z}_{M-1}^{d_{M-1}}} &= s(\boldsymbol{\tilde{W}_{M  }^{d_{M-1} \times d_{M  }} \tilde{z}_{M  }^{d_{M  }}}+\boldsymbol{\tilde{b}_{M-1}^{d_{M-1}}}) \\
  \boldsymbol{\tilde{z}_{M  }^{d_{M  }}} &= \boldsymbol{\hat{y}} = \boldsymbol{z_{M  }^{d_{M  }}} \\
  \boldsymbol{z_{M  }^{d_{M  }}}         &= s(\boldsymbol{W_{M  }^{d_{M  } \times d_{M-1}} z_{M-1}^{d_{M-1}}}+\boldsymbol{b_{M  }^{d_{M  }}}) \\
  ... &= ... \\
  \boldsymbol{z_{1  }^{d_{1  }}}         &= s(\boldsymbol{W_{1  }^{d_{1  } \times d_{0  }} z_{0  }^{d_{0  }}}+\boldsymbol{b_{1  }^{d_{1  }}}) \\
  \boldsymbol{z_{0  }^{d_{0  }}}         &= \boldsymbol{x}
\end{split}
\end{equation}
To measure the reconstruction loss $L(x,\tilde{x})$, cross-entrophy (CE) can be employed again:
\begin{equation*}
\begin{split}
  L^{CE}(\boldsymbol{x},\boldsymbol{\tilde{x}}) = \\
  -\sum_{i=1}^N\sum_{k=1}^P[x_{ik}\log{\tilde{x}_{ik}}+(1-x_{ik})\log(1-\tilde{x}_{ik})].
\end{split}
\end{equation*}
The parameters to be tuned are $\boldsymbol{\theta} \cup \boldsymbol{\tilde{\theta}}=\{\boldsymbol{W_{1:M}},\boldsymbol{b_{1:M}}\} \cup \{\boldsymbol{\tilde{W}_{1:M}}, \boldsymbol{\tilde{b}_{1:M}}\}$, that is, the the decoder is also implicitly trained. One common practise is to constrain the liner reconbination in the decoder as the tanspose of their encoder counterpart, that is, by forcing $\boldsymbol{\tilde{W}_i}=\boldsymbol{W_i}^{\prime} (i=1 \ldots M)$, which almost halves the number of parameters and fits the intuition better since encoding and decoding are conceptually symmetric. Once the parameters and loss function is decided, training can be done by gradient decent.

A deep encoder is fundermentally an encoder, albeit with much larger number of layers and nodes, allowing input of very high dimension such as medical images and genetic sequences. The increase in depth and input causes some problems. The training tends to be unstable due to the large number of parameters, so one have to use smaller learning step to ensure the loss $L$ always decrease when the training advances. Slow convergence and large number of tuning paramter all adds to computation burden. The deep-learning method was postulated as a counter measure, which is a greedy, layer by layer pre-training process followed by overall fine-tuningby [2008 Vincent]. The idea of layer by layer pre-training is to form and train encoder--decoder by unit. Using similar symbolic form, the $M \time 2$ layered encoder--decoder network can be seperated into $M$ units, written as:
\begin{equation} \label{eq:unit encoder-decoder}
\begin{split}
  \boldsymbol{\tilde{z}_{i-1}^{d_{i-1}}} &= s(\boldsymbol{\tilde{W}_{i  }^{d_{i-1} \times d_{i  }} z_{i  }^{d_{i  }}}+\boldsymbol{\tilde{b}_{i-1}^{d_{i-1}}}) \\
  \boldsymbol{z_{i  }^{d_{i  }}}         &= s(\boldsymbol{W_{i  }^{d_{i  } \times d_{i-1}} z_{i-1}^{d_{i-1}}}+\boldsymbol{b_{i  }^{d_{i  }}}), \\
\end{split}
\end{equation}
where $i=(1 \ldots M)$. Thus, instead of pipping the output of the $i th$ encoder layer ($\boldsymbol{z_{i  }^{d_{i  }}}$) to the one above, it is pipped to the cooresponding $M-i th.$ layer in the docoder counterpart for an immediate reconstruction of the input $\boldsymbol{z_{i-1}}$. The unit encoder--decoder tuple can then be trained by minimizeing the reconstruction loss $L(\boldsymbol{z_{i-1}},\boldsymbol{\tilde{z}_{i-1}})$. A total of $M$ units can be formed and trained seperatetly. Although the pre-training cannot be done in parallel since the higher unit have to wait the lower for input, the process is very fast because the number of parameters is rather small in each unit compared with the whole network. The pre-training serves as a non-random parameter initilization, after which the whole network is already near convergence. Therefore, the subsequent fine-tuning of entire network would required much less steps to reach convergence.

The MLP or encoder thus trained may suffer the overfitting problem, that is, the network performs will on the known facts, producing low prediction/reconstruction loss, but deals poorly for the future, unknown data. To better balance the internal and external validy, one way is to add a regulator term to the loss funciton $L$, usually the $L1$ or $L2$ norm of the parameter elements. Doing so discourages large parameters which causes instability of the prediction/reconstruction. For encoder training, another way to ensure generalizability is to randomly corrupt the input $\boldsymbol{x}$ by setting some elements to zero [2008 Vincent]. The rationel is, the more rigours a feature is, the more like it will survive the random corruption. More training epoch is needed to ensure the randomness of corruption, but doing so encourages the network to seek major features of the input while neglecting the minor changes between facts. And since major feature are more likely to show up in the future, the external validty of the encoder is thus preserved.

For the current study howver, the purpose of deep-learning is not to construct an encoder encrypted with universal knowledge regarding white matter surface, but to produced the abstration the surface vertices at hand for the up-coming analysis. In other words, the code $\hat{y}$ is the only concern here, and the external validty will be implictly handled by the subsequent U-statistical analysis using such code. For now, an ad-hoc encoder trained without regulator term or ramdom corruption would surfice. The current implementation is a five layered encoder, each halves the dimensionality of its input. Thus the final encoding will be 32 times smaller then the raw input.

\subsection{Similarity U Test}
  To derive the proposed U statistic, one U-kernel function and one or more U-weight functions must be constructed. These functions are centered measurements of pairwise subject similarity with respect to their profiles. The measurement funciton can be flexable depending on the type of profile and the hypothesis in mind, as long as the function is symmetric and has finite second moment. Thus, function $f$ is a valid U-weight or U-kernel if $f(x_i,x_j)=f(x_j,x_i)$ and $E(f^2(X_1, X_2))<+\infty$ are satisfied. The current study has three types profile, namely the genetics, vertex coding, and the phenotype, for which three pairwise similarity measurement were chosen respectively for a pair of subject indexed by $(i,j)$, according to common practice.

For biallelic genetic variants whose value was taken from minor allele count ${0, 1, 2}$, a common choice is the weighted complement of Manhattan Distance (wMD):
\begin{equation} \label{eq:wSG}
\begin{split}
  S_{ij}^G &= wMD(g_{i.},g_{j.}) \\
  &= \frac{\sum_{m=1}^{|G|}{w_m(2-|g_{im}-g_{jm}|)}} {2\sum_{m=1}^{|G|}{w_m}},
\end{split}
\end{equation}
where $g.m$ is the genotype of the $m th.$ variant (e.g. a SNP, indel, or deletion, ect.) in a testing unit (e.g. a gene); $w_m$ is the weight assigned to the $m th.$ variant depending on \textit{a prior} hypothesis, one common example is the reversed square root of minor allele frequency which place more emphasize on rarer variants:
\begin{displaymath}w_m=\frac{1}{\sqrt{MAF(g_m)(1-MAF(g_m))}}.\end{displaymath}
Without any \textit{a prior} hypothesis of the relative importance of genomic variants, an unweighted complement of Manhatten Distance (uMD) can be used by forcing $w_m \equiv 1$:
\begin{equation*} \label{eq:uSG}
\begin{split}
  S_{ij}^G &= uMD(g_{i.}, g_{j.}) \\
  &= \frac{\sum_{m = 1}^{|G|}{(2-|g_{im} - g_{jm}|)}}  {2|G|}.
\end{split}
\end{equation*}

The encoded surface is another type of profile, since the code are real values between $[0,1]$, one could exploit the weighted Euclidian Distance (wED) to build a similarity measurement:
\begin{equation} \label{eq:wSV}
\begin{split}
  S_{ij}^V &= wED(v_{i.},v_{j.}) \\
  &= \exp
  {
    \Big[-\frac{\sum_{m=1}^{|V|}{w_m(u_{im}-u_{jm})^2}} {\sum_{m=1}^{|V|}{w_m}}\Big]
  },
\end{split}
\end{equation}
where $v_{.m}$ is the value of the $m th.$ component of the encoded surface vertices, $w_m$ is the weight assigned to that component, and $|V|$ is dimensionality (i.e. number of components) of the code. Because there is no knowledge regarding the relative importance of each component, the measurement is simplifed by equal weighting ($w_m \equiv 1$):
\begin{equation*} \label{eq_uSV}
\begin{split}
  S_{ij}^V &= uED(v_{i.},v_{j.}) \\
  &= \exp
  {
    \Big[-\frac{\sum_{m=1}^{|V|}{(u_{im}-u_{jm})^2}} {|V|}\Big]
  }.
\end{split}
\end{equation*}

For the phenotype profile, we first apply a rank normal quantile tansformation to each of its dimensions suggested by [changshuai et. al.]. 
\begin{displaymath}
  q_{im}=\Phi^{-1}[(rank(y_{im})-0.5)/|Y|],
\end{displaymath} 
where $y_{.m}$ is the value of the $m th.$ dimension of the phenotype, and $|Y|$ is the dimensionality of the phenotype. Doing so will remove the complication of admixed distribution type introduced by a high dimensional phenotype. As a result, the rank-normal-quantile tranformed phenotype can be compared in a manner similar to that of encoded vertices:
\begin{equation} \label{eq_wSY}
\begin{split}
  S_{ij}^Y &= wED(q_{i.},q_{j.}) \\
  &=\exp
  {
    \Big[-\frac{\sum_{m=1}^{|Y|}{w_m(q_{im}-q_{jm})^2}} {\sum_{m=1}^{|Y|}{w_m}}\Big]
  },
\end{split}
\end{equation}
where $q_{.m}$ is the values of the $m th.$ dimension of the rank-normal-quantile transformed phenotype; the weight $w_m$ is the ralative importance of that dimension. For a case control study with one dimensional phenotype, that is, $|Y|=|y|=1$, the above similarity measurement simplifys to:
\begin{displaymath}
  S_{ij}^{y}=ED(q_i,q_j)=\exp{[-(q_i-q_j)^2]},
\end{displaymath}

Centralization of a similarity function is done by substracting each raw measurement $S_{ij}$ with the two conditional mean on subject i and j, then add the mean of all measurement to it [changshuai et. al.]. Takeing the weighted genetic similarity as a example, the centered measurement is:
\begin{displaymath}
  \tilde{S}_{ij}^{G}=S_{ij}^{G}-E(S_{i.}^{G})-E(S_{.j}^{G})+E(S_{..}^{G}).
\end{displaymath}
The same centralization scheme is applied to the other two similarity measurements.

The U statistics is the mean of the product of three centralized similarity measurement across all subject pairs:
\begin{displaymath}
  U^{GVY}=\frac{1}{N(N-1)}\sum_{i \neq j} \tilde{S}_{ij}^{G} \tilde{S}_{ij}^{V} \tilde{S}_{ij}^{Y},
\end{displaymath}
where N is the number of subjects.



For the actually implementation, three N by N symmetric similarity matrices are first constructed, and elementwisely multipled to get an u scores matrix, and the lower (or upper) traiangle elements are then summed up for the U statistics.

For now, the genetic and image similarity gauges are treated as U-weight terms, and the phenotype gauge is treated as U-kernel. In general, the assignment of U-kernal and U-weight terms does not alter the limiting distribution of the final U statistics, but fixing the U-kernel on phenotype has the added benifit of testing simpler hypothesis by dropping one of the U-weight terms.

\bibliography{ref}
\bibliographystyle{plain}

\end{document}
