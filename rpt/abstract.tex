\documentclass{article}

\usepackage{syntonly}
\syntaxonly
\usepackage{amsmath}

\pagestyle{headings}

\author{Xiaoran Tong}
\title{}

\begin{document}
\maketitle
\section{Abstract}
The rapid accumulation of conprehensive whole genome sequence(WGS) and brain image data(MRI) mandates the development of analysical procedures capable of utilizing both genetic and image information to detect predicative biomarkers of both type for CNS diseases of interest such as Alzheimer. Both type of profiles however, poses "the curse of dimensionality" due to the intrisic colossal size inherited from the vast wealth of genomic variants and brain surface vertices. In this work we tackled the dimensionality issue on the image side by piping the MRI structure data of one anatomycal region through an stacked denosing autoencoder(SDA) constructed using deep learning approach, reducing the sample features from millions to 10; For the the whole genome data, an IBS kernel were used to aggregate rare variants by gene units, which reduces the total number of tests/variants and at the same time boost statistical power. With the condensed profiles of lower dimension, we were able to perform a joint association test against disease phenotype with a weighted U statistics which bypass the assumptions on distribution and mediation effect. Preliminary simulation showed correct type I error rate and moderate power boost incompression with tests using only genetic profile or non-surface vertices based image data(e.g. regional volumn)

\section{Introduction}
We know the voxels/vertices near by are closely related due to the fact that they represent physical connected brain tissues, and the location of the vertices and the gray matter thickness at that location are two distinct type of information.
To speed up learning and improve generalization we should build a priori knowledge into the network and let it use the information in the training data to discover structure that we do not already understand or difficault to modulate by human labor.

6.9. Self-supervised backpropagation
One drawback of the standard form of backpropagation is that it requires an external supervisor to specify the desired states of the output units (or a transformed "image" of the desired states). It can be converted into an unsupervised procedure by using the input itself to do the supervision, using a multi-layer "encoder" network [2] in which the desired output vector is identical with the input vector. The network must learn to compute an approximation to the identity mapping for all the input vectors in its training set, and if the middle layer of the network contains fewer units than the input
layer, the learning procedure must construct a compact, invertible code for each input vector. This code can then be used as the input to later stages of processing.
The use of self-supervised backpropagation to construct compact codes resembles the use of principal components analysis to perform dimensionality reduction, but it has the advantage that it allows the code to be a nonlinear transform of the input vector. This form of backpropagation has been used successfully to compress images [19] and to compress speech waves [25]. A variation of it has been used to extract the underlying degrees of freedom of simple shapes [83].

\section{Method}
To detect possible phenotype to genetic and phenotype to image association simoteniously, we adopted a joint U statistics test [changshuai et. al.].
To compute the U statistic, we first measure the pairwise similarity based on phenotype, genetic and image profiles of N observed subjects. There are two mandatory critera a similarity measurement kernel f must satisfy: 1) the measurement must be symmetric, that is, \begin{equation}f(i,j)=f(j,i)\end{equation} and 2) the measurement must have finite second moment. For current study, we choose the following kernels to measure phenotype, genetic and image similarities, respectively:

\begin{equation}
f_G(x_i,x_j){k=1}^{n} I_k = 0
\end{equation}

\begin{equation}
f_G(x_i,x_j){k=1}^{n} I_k = 0
\end{equation}

\begin{equation}
f_G(x_i,x_j){k=1}^{n} I_k = 0
\end{equation}

 the for each subject pair, calcuate a u score by the product of 3 aforementioned similarity measurement; the final U statistic is the summation of u scores accorss all subject pairs. In actually implementation, 3 pairwise similarity matrix are constructed and multipled elementwise to get the pairwise u score matrix, all elements are added up to get the U statistics.

\end{document}
