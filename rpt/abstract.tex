
\documentclass{article}
\begin{document}
\textbf{Abstract}
The rapid accumulation of conprehensive whole genome sequence(WGS) and brain image data(MRI) mandates the development of analysical procedures capable of utilizing both genetic and image information to detect predicative biomarkers of both type for CNS diseases of interest such as Alzheimer. Both type of profiles however, poses "the curse of dimensionality" due to the intrisic colossal size inherited from the vast wealth of genomic variants and brain surface vertices. In this work we tackled the dimensionality issue on the image side by piping the MRI structure data of one anatomycal region through an stacked denosing autoencoder(SDA) constructed using deep learning approach, reducing the sample features from millions to 10; For the the whole genome data, an IBS kernel were used to aggregate rare variants by gene units, which reduces the total number of tests/variants and at the same time boost statistical power. With the condensed profiles of lower dimension, we were able to perform a joint association test against disease phenotype with a weighted U statistics which bypass the assumptions on distribution and mediation effect. Preliminary simulation showed correct type I error rate and moderate power boost incompression with tests using only genetic profile or non-surface vertices based image data(e.g. regional volumn)
\end

\textbf{Introduction}
We know the voxels/vertices near by are closely related due to the fact that they represent physical connected brain tissues, and the location of the vertices and the gray matter thickness at that location are generally distinct type of information.
To speed learning and improve generalization we should build a priori knowledge into the network and let it use the information in the training data to discover structure that we do not already understand or difficault to modulate by human labor.

6.9. Self-supervised backpropagation
One drawback of the standard form of backpropagation is that it requires an external supervisor to specify the desired states of the output units (or a transformed "image" of the desired states). It can be converted into an unsupervised procedure by using the input itself to do the supervision, using a multi-layer "encoder" network [2] in which the desired output vector is identical with the input vector. The network must learn to compute an approximation to the identity mapping for all the input vectors in its training set, and if the middle layer of the network contains fewer units than the input
layer, the learning procedure must construct a compact, invertible code for each input vector. This code can then be used as the input to later stages of processing.
The use of self-supervised backpropagation to construct compact codes resembles the use of principal components analysis to perform dimensionality reduction, but it has the advantage that it allows the code to be a nonlinear transform of the input vector. This form of backpropagation has been used successfully to compress images [19] and to compress speech waves [25]. A variation of it has been used to extract the underlying degrees of freedom of simple shapes [83].

There is another promising method that reduces the time required to compute the equilibrium distribution and eliminates the noise caused by the sampling errors in (s_i, s_j)+ and (S_i, S_j)-. Instead of directly simulating the stochastic network it is possible to estimate its mean behavior using "mean field theory" which replaces each stochastic binary variable by a deterministic real value that represents the expected value of the stochastic variable. Simulated annealing can then be replaced by a deterministic relaxation procedure that operates on the real-valued parameters [51] and settles to a single state that gives a crude representation of the whole equilibrium distribution. The product of the "activity levels" of two units in this settled state can be used as an approximation of (sis j) so a version of the Boltzmann machine learning procedure can be applied. Peterson and Anderson [74] have shown that this works quite well.

12.1. Generalization, may not be a major considertion for dimension reduction problems
A major goal of connectionist learning is to produce networks that generalize correctly to new cases after training on a sufficiently large set of typical cases from some domain. In much of the research, there is no formal definition of what it means to generalize correctly. The network is trained on examples from a domain that the experimenter understands (like the family relationships domain described in Section 6) and it is judged to generalize correctly if its generalizations agree with those of the experimenter. This is sufficient as an informal demonstration that the network can indeed perform nontrivial generalization, but it gives little insight into the reasons why the generalizations of the network and the experimenter agree, and so it does not allow predictions to be made about when networks will generalize correctly and when they will fail.
    Another useful method is to impose equality constraints between weights that encode symmetries in the task. In solving any practical problem, it is wasteful to make the network learn information that is known in advance. If possible, this information should be encoded by the architecture or the initial weights so that the training data can be used to learn aspects of the task that we do not already know how to model.
\end
{document}
