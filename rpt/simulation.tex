\documentclass[twocolumn]{article}

\usepackage{syntonly}
%\syntaxonly
\usepackage{amsmath}

\pagestyle{headings}

\author{Xiaoran Tong}

\begin{document}

\section{Simulation}
Simulations are based on 806 participants of ADNI study with both WGS and MRI data avaliable. Each iteration run choose a pair of testing units for genomc and image profiles. The genomic testing unit is a randomly gene randomly picked from the genomic features list supplied by GRC38. For neuroimage profile, the testing unit is regions of 512 vertices randomly located in the white matter (WM) surface reconstructed from MRI data. The genomic effect $\beta^G$ and vertex effect $\beta^G$ are then generated by randomly selecting 5\% of the variants (e.g. SNPs in a gene, vertices in a WM region) in both testing units and assigning a value drawn from $N(0,1)$. Given the row major genomic profile $X^G$ and image profile $X^V$ of all subjects, two basic continuous response $Y^G_C$ and $Y^V_C$ were additively calculated across variants in the corresponding testing unit; two additional continuous responses $Y^A_C$ and $Y^X_C$ were then created by adding up the the basics with or without an extra product term. Four binary responds $Y^G_b, Y^V_b, Y^A_b, Y^X_b$ were also generated by putting the four continuous ones through inverse logit and draw cases from the resulting probability. The 8 responses can be written as:
\begin{equation*} \label{eq:SIM}
\begin{split}
  \beta^G_k &= N(0,1) \times Bernoulli(0.05), k=1, \dots, |G| \\
  \beta^V_l &= N(0,1) \times Bernoulli(0.05), l=1, \dots, |V| \\
  \boldsymbol{Y^G_c} &= \boldsymbol{X^G \beta^G} \\
  \boldsymbol{Y^V_c} &= \boldsymbol{X^V \beta^V} \\
  \boldsymbol{Y^+_c} &= \boldsymbol{Y^G_c} + \boldsymbol{Y^V_c} \\
  \boldsymbol{Y^*_c} &= \boldsymbol{Y^G_c} + \boldsymbol{Y^V_c} + \boldsymbol{Y^G_c Y^V_c} \\
  \boldsymbol{Y^G_b} &= Bernolli(logit^{-1}(\boldsymbol{Y^G_c})) \\
  \boldsymbol{Y^V_b} &= Bernolli(logit^{-1}(\boldsymbol{Y^V_c})) \\
  \boldsymbol{Y^+_b} &= Bernolli(logit^{-1}(\boldsymbol{Y^A_c})) \\
  \boldsymbol{Y^*_b} &= Bernolli(logit^{-1}(\boldsymbol{Y^X_c}))
\end{split}
\end{equation*}
where $k$ and $l$ indices variants within the choson genomic and image units, and $|G|$ and $|V|$ is the variant count. For now $|V|$ is fixed to 512 -- the number of vertices in a WM sample region.

To see the power performance of the joint U-statistics and the potential benifit of vertex encoder, two factors are considered simutaniously in the simulation study. First is the construct of U statistics which may result in correct, partially or completely mis-specification when being faced with responses of various underlying effect (G, V, +, *). The available consitution of the U statistcis are genomic only (G), image only (V), or joint (G+V), corresponding to the 3 aforementioned hypothesis. The second factor to be considered is the construct of image kernel (V), which can be built upon both the original and the encoded vertices. Alternatively, as an reference algorithm, the widely used vertex-wise analysis (VWA) can also be applied, which is an analogy of GWAS by treating a vertices as a SNP. In brief, VWA first smoothes the vertices data via a gaussian blur process to attenuate the noises within the image vertices, then builds a similarity weight kernel for each vertex and performs a large number of tests, after which the most significant one is chosen as the representative of the whole testing unit. All scensiable combinations of the two factors were tested against all 8 responses across a sample sized spectrium from 100 to 800.

\begin{tabular}{|c|c|c|c|}
  \hline
  *   & Encoded Vertex, regional & Raw Vertex, regional & Raw vertex, VWA  \\ \hline
  V   & Y              & Y          & Y            \\ \hline
  G   & N              & N          & N            \\ \hline
  G+V & Y              & Y          & Y            \\ \hline
\end{tabular}

 (e.g. gaussian blur is only applicable to original vertices)         
For continuous outcomes, as expected, the U statistics composition that exactly matches the response composition asertained the heighest power across all sample sizes, while a complete mis-specification yields no power at all. Partially mis-specified U statistics maintained a performance close to that of the perfect match, thus the joint U-kernel ($G+V$) displayed an overall better performance [figure ?]. By limiting the comparesion among tests involving raw vertices, it is observed that regional U test could outperform VWA under all possible scenarios. By looking at all regional similarity U tests involving vertices, those employing encoded vertices start with slightly lower at smallest sample sizes (100) but enjoyed higher power boost when sample size increase. When the U-kernel consitution is completely correct, the statistical power is always higher with encoded vertices. The only exception happends when a partial mis-specification was inccured by testing a pure vertex effect ($Y^V_c$) with a joint U statistics ($G+V$), where the encoded vertices out performed the raw vertices only at a higher sample size (>500), suggesting that the benefit brought by feature abstraction could "back fire" when the statistical model is partially or fully wrong.
For binary responses, the statistical power shared similar patten under every combination of U-statistic and vertex kernel constuct, albeit universally lower then its continuous counterpart as expected. Another noteworthy fact is that the U statistics are indifferent to the preprocessing of the binary response thanks to the rank normal quantile standardization. The same U score are obtained regardless the basis of U-kernel: the deviance residual, the least squre resisual, or  the binary response itself.
Nonetheless, the simulation demonstrated that, the joint U statistics offered an omnibus test which is more robust and safe under most circumstances, especially when the prior knowledge of effect composition is unknown to the investigator. As for the feature abstration and dimension reduction, the resulting vertex encode offered an accelerated power boost when sample sizes is linearly increased.

\section{real data analysis}
The same 806 subject were used for the analysis. Exlcuding participants with uncertain diagnosis, 399 remained for the analysis, with ? definite Alzheimer' disease (AD) cases and ? healthy controls. The dichotomous outcome was first regressed on 7 well known risk factors of AD, namely age, gender, race, years of education, APOE $\epsilon$4, the regression residuals were then taken as an continuous phenotype to construct the U-kernel term. The genomic testing units are still genes and the calculation of the corresponding weight term (G) is the same with the simulation study; the image testing units are 68 standard anatomy regions in the white matter surface, and only the encoded vertices are employed for the real data analysis. The comprehensive resuls of all 3 types of U statistics formulation are show in Figure [?], the top 10 most significant testing units are listed in table [?]. Results showed that the vertex based test could detect significant units more frequently then the genetic based test, which follows the expectation that the neuron loss and the consequential thinning of gray matter is an approxmate indicator of AD while genomic profile is a remote predictor of a weighty uncertainty. 

%\begin{bibliography}
%\end{bibliography}

\end{document}
